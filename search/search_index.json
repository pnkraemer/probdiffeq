{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"probdiffeq","text":""},{"location":"#probabilistic-ode-solvers-in-jax","title":"Probabilistic ODE solvers in JAX","text":"<p>Probdiffeq implements adaptive probabilistic numerical solvers for ordinary differential equations (ODEs). It builds on JAX, thus inheriting automatic differentiation, vectorisation, and GPU acceleration.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\u26a1 Calibration and step-size adaptation  </li> <li>\u26a1 Stable implementations of filtering, smoothing, and other estimation strategies  </li> <li>\u26a1 Custom information operators, dense output, and posterior sampling  </li> <li>\u26a1 State-space model factorisations  </li> <li>\u26a1 Parameter estimation</li> <li>\u26a1 Taylor-series estimation with and without Jets  </li> <li>\u26a1 Seamless interoperability with Optax, BlackJAX, and other JAX-based libraries  </li> <li>\u26a1 Numerous tutorials (basic and advanced) -- see the documentation </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest release from PyPI:</p> <pre><code>pip install probdiffeq\n</code></pre> <p>This assumes JAX is already installed.  </p> <p>To install with JAX (CPU backend):  </p> <pre><code>pip install probdiffeq[cpu]\n</code></pre> <p>\u26a0\ufe0f Note: This is an active research project. Expect rough edges and breaking API changes.</p>"},{"location":"#benchmarks","title":"Benchmarks","text":"<p>We maintain benchmarks comparing Probdiffeq against other solvers and libraries, including SciPy, JAX, and Diffrax.</p> <p>Run benchmarks locally:</p> <pre><code>pip install .[example,test]\nmake benchmarks-run\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome! - Browse open issues (look for \u201cgood first issue\u201d). - Check the developer documentation. - Open an issue for feature requests or ideas.  </p>"},{"location":"#citing","title":"Citing","text":"<p>If you use Probdiffeq in your research, please cite:</p> <p><pre><code>@phdthesis{kramer2024implementing,\n  title={Implementing probabilistic numerical solvers for differential equations},\n  author={Kr{\"a}mer, Peter Nicholas},\n  year={2024},\n  school={Universit{\"a}t T{\"u}bingen}\n}\n</code></pre> The PDF explains the mathematics and algorithms behind this library.  </p> <p>For the solve-and-save-at functionality, cite:</p> <p><pre><code>@InProceedings{kramer2024adaptive,\n  title     = {Adaptive Probabilistic ODE Solvers Without Adaptive Memory Requirements},\n  author    = {Kr\"{a}mer, Nicholas},\n  booktitle = {Proceedings of the First International Conference on Probabilistic Numerics},\n  pages     = {12--24},\n  year      = {2025},\n  editor    = {Kanagawa, Motonobu and Cockayne, Jon and Gessner, Alexandra and Hennig, Philipp},\n  volume    = {271},\n  series    = {Proceedings of Machine Learning Research},\n  publisher = {PMLR},\n  url       = {https://proceedings.mlr.press/v271/kramer25a.html}\n}\n</code></pre> Link to the paper: PDF.</p> <p>Link to the experiments:  Code for experiments.  </p> <p>\ud83d\udccc Algorithms in Probdiffeq are based on multiple research papers. If you\u2019re unsure which to cite, feel free to reach out.  </p>"},{"location":"#versioning","title":"Versioning","text":"<p>Probdiffeq follows 0.MINOR.PATCH until its first stable release: - PATCH \u2192 bugfixes &amp; new features - MINOR \u2192 breaking changes  </p> <p>See semantic versioning.</p>"},{"location":"#related-projects","title":"Related projects","text":"<ul> <li>Tornadox </li> <li>ProbNumDiffEq.jl </li> <li>ProbNum </li> </ul> <p>The docs include guidance on migrating from these packages. Missing something? Open an issue or pull request!</p>"},{"location":"#you-might-also-like","title":"You might also like","text":"<ul> <li>diffeqzoo \u2014 reference implementations of differential equations in NumPy and JAX  </li> <li>probfindiff \u2014 probabilistic finite-difference methods in JAX  </li> </ul>"},{"location":"choosing_a_solver/","title":"Choosing a solver","text":"<p>Good solvers are problem-dependent. Nevertheless, some guidelines exist:</p>"},{"location":"choosing_a_solver/#state-space-model-factorisation","title":"State-space model factorisation","text":"<ul> <li>If your problem is scalar-valued (<code>shape=()</code>), use a <code>scalar</code> implementation. Of course, you are always welcome to transform your problem into one with shape <code>(1,)</code> and use a vector-valued solver (not all features are implemented for scalar models).</li> <li>If your problem is vector-valued, be aware that different implementation choices imply different modelling choices.</li> </ul> <p>If you don't care about modelling choices:</p> <ul> <li>If your problem is high-dimensional, use a <code>blockdiag</code> or <code>isotropic</code> implementation.</li> <li>If your problem is medium-dimensional, use any implementations.    <code>isotropic</code> factorisations tend to be the fastest with the worst UQ and worst stability,    <code>dense</code> factorisations tend to be the slowest with the best UQ and best stability,    <code>blockdiag</code> factorisations are somewhere in between.</li> </ul>"},{"location":"choosing_a_solver/#stiffness","title":"Stiffness","text":"<p>If your problem is stiff, use a a <code>dense</code> implementation in combination with a correction scheme that employs first-order linearisation;  for instance, <code>ts1</code> or <code>slr1</code>. Zeroth-order approximation and too-aggressive state-space model factorisation  will likely fail.</p> <p>If your problem is stiff and high-dimensional: try first-order linearisation with a block-diagonal factorisation.  If that does not work: let me know what you come up with...</p>"},{"location":"choosing_a_solver/#filters-vs-smoothers","title":"Filters vs smoothers","text":"<p>Almost always, use a <code>ivpsolvers.strategy_filter</code> strategy for <code>simulate_terminal_values</code>,  a <code>ivpsolvers.strategy_smoother</code> strategy for <code>solve_adaptive_save_every_step</code>, and a <code>ivpsolvers.strategy_fixedpoint</code> strategy for <code>solve_adaptive_save_at</code>. Use either a filter (if you must) or a smoother (recommended) for <code>solve_fixed_step</code>. Other combinations are possible, but rather rare  (and require some understanding of the underlying statistical concepts).</p>"},{"location":"choosing_a_solver/#calibration","title":"Calibration","text":"<p>Use a <code>solvers.solver_dynamic</code> solver if you expect that the output scale of your IVP solution varies greatly. Otherwise, use an <code>solvers.solver_mle</code> solver. Try a <code>solvers.solver</code> for parameter-estimation.</p>"},{"location":"choosing_a_solver/#miscellaneous","title":"Miscellaneous","text":"<p>If you use a <code>ts0</code>, choose an <code>isotropic</code> factorisation instead of a <code>dense</code> factorisation. They do the same, but the <code>isotropic</code> factorisation is cheaper.</p> <p>These guidelines are a work in progress and may change soon. If you have any input, let me know!</p>"},{"location":"migration_guide/","title":"Probdiffeq Migration Guide","text":"<p>This guide helps you get started with Probdiffeq for solving ordinary differential equations (ODEs), especially if you are familiar with other probabilistic or non-probabilistic ODE solvers in Python or Julia.</p> <p>Probdiffeq is a JAX library that focuses on state-space-model-based formulations of probabilistic IVP solvers. For what this means, have a look at this thesis.</p>"},{"location":"migration_guide/#transitioning-from-probnumdiffeqjl-julia","title":"Transitioning from ProbNumDiffEq.jl (Julia)","text":"<p>ProbNumDiffEq.jl is a library for probabilistic IVP solvers in Julia, similar to Probdiffeq. However, both libraries are unrelated.</p> <ul> <li>Probdiffeq is Python/JAX-based; ProbNumDiffEq is Julia-based.</li> <li>Probdiffeq provides additional solvers, dense output, and posterior sampling.</li> <li>ProbNumDiffEq handles mass-matrix problems and callbacks, which Probdiffeq does not (yet).</li> </ul> <p>To translate ProbNumDiffEq.jl code to Probdiffeq code:</p> ProbNumDiffEq.jl ProbDiffEq Equivalent <code>EK0</code> / <code>EK1</code> <code>ts0()</code> / <code>ts1()</code> <code>DynamicDiffusion</code> / <code>FixedDiffusion</code> <code>ivpsolvers.solver_dynamic()</code> or <code>ivpsolvers.solver_mle()</code> <code>IWP(diffusion=x^2)</code> <code>prior_wiener_integrated(output_scale=x)</code> Filtering and smoothing via <code>smooth=true/false</code> Solver strategy constructions, including one for fixed-point smoothing <p>Both libraries are evolving; consult the latest API documentation when in doubt.</p>"},{"location":"migration_guide/#transitioning-from-probnum-python-numpy","title":"Transitioning from ProbNum (Python, Numpy)","text":"<p>ProbNum is a general probabilistic numerics library based on Numpy. Probdiffeq specializes in IVP solvers using pure JAX, offering:</p> <ul> <li>Greater efficiency for ODE problems because of JAX (e.g. jit)</li> <li>Probdiffeq implements more mature solvers. The algorithms are generally faster (eg state-space model factorisations, improved adaptive step-size selection)</li> <li>Probdiffeq offers more solvers and somewhat richer outputs (sampling, marginal likelihoods, etc.).</li> </ul>"},{"location":"migration_guide/#transitioning-from-diffrax","title":"Transitioning from Diffrax","text":"<p>Diffrax is a JAX-based library for differential equations. Key differences:</p> <ul> <li>Diffrax solvers are non-probabilistic; Probdiffeq solvers are probabilistic.</li> <li>Vector fields: Diffrax uses <code>ODETerm()</code>; Probdiffeq uses plain functions <code>(*ys, t)</code>.</li> <li>Solver construction: Diffrax requires (<code>diffrax.Tsit5()</code>); Probdiffeq constructs probabilistic state-space models.</li> </ul> <p>Approximate solver mapping:</p> Diffrax ProbDiffEq Equivalent <code>Heun()</code>, <code>Midpoint()</code> <code>prior_ibm(num_derivatives=1)</code> or <code>ts0()</code> <code>Tsit5()</code>, <code>Dopri5()</code> Increase <code>num_derivatives=4</code> <code>Dopri8()</code> Increase <code>num_derivatives=5-7</code>; <code>ts1()</code> recommended but not required <code>Kvaerno3()</code>\u2013<code>Kvaerno5()</code> Use <code>num_derivatives=2-4</code> with <code>ts1()</code> correction Other methods Work in progress"},{"location":"migration_guide/#general-differences-from-conventional-ode-solvers-eg-scipy-jaxodeint","title":"General differences from conventional ODE solvers (e.g., SciPy, jax.odeint)","text":"<ul> <li>Solutions are posterior distributions instead of point estimates, enabling uncertainty quantification and more sophisticated models (eg easy switch to second-order problems).</li> <li>Solver modes are explicit: <code>simulate_terminal_values()</code>, <code>solve_adaptive_save_every_step()</code>, <code>solve_adaptive_save_at()</code> instead of a one-size-fits-all <code>solve()</code> method</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#general-troubleshooting","title":"General troubleshooting","text":"<p>If you encounter unexpected issues, please ensure you have the latest version of JAX installed.  If you're not already using virtual environments, now might be a good time to start, as they can help manage dependencies more effectively.</p> <p>With these points covered, try to execute some of the examples in Probdiffeq's documentation, for example the easy example. If these examples work \\(-\\) great! If not, reach out. </p> <p>Unlike many other JAX-based scientific computing libraries, probdiffeq works best with double precision.  This is because, during solver initialization, it computes the Cholesky factor of a Hilbert matrix (with somewhere between 2-12 rows), which needs high precision.</p>"},{"location":"troubleshooting/#long-compilation-times","title":"Long compilation times","text":"<p>If a solution routine takes an unexpectedly long time to compile but runs quickly afterward, the issue might be related to how Taylor coefficients are computed.  Some functions in <code>probdiffeq.taylor</code> unroll a small loop, which can slow down compilation. To avoid this, try using the padded scan, which replaces loop unrolling with a scan. If the problem persists, consider:  </p> <ul> <li>Reducing the number of derivatives (if appropriate for your problem).  </li> <li>Switching to a different Taylor-coefficient routine, such as a Runge-Kutta starter.</li> </ul> <p>For \\(\\nu &lt; 5\\), using a Runge-Kutta starter should maintain solver performance. However, for higher-order methods (e.g., \\(\\nu = 9\\)), Taylor-mode (\"jets\") appears to be the best choice.  </p>"},{"location":"troubleshooting/#taylor-derivative-routines-yield-nans","title":"Taylor-derivative routines yield NaNs","text":"<p>If you encounter unexpected NaNs while estimating Taylor derivative routines, the issue might come from the vector field itself! For instance, in the Pleiades problem, there's a term like \\(\\|x\\|^2 / (\\|x\\|^2 + \\|y\\|^2)\\), which can have differentiability issues near zero, depending on how it's implemented.  See this issue (external) for more details. In some cases, the fix is as simple as wrapping the quotient in <code>jax.numpy.nan_to_num</code>.  You can also check out Probdiffeq's Pleiades benchmark for a concrete example.</p>"},{"location":"troubleshooting/#other-problems","title":"Other problems","text":"<p>Your problem is not discussed here? Feel free to reach out \\(-\\) opening an issue is a great way to get help!</p>"},{"location":"api_docs/impl/","title":"impl","text":"<p>State-space model implementations.</p>"},{"location":"api_docs/ivpsolve/","title":"ivpsolve","text":"<p>Routines for estimating solutions of initial value problems.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution","title":"<code>IVPSolution</code>","text":"<p>The probabilistic numerical solution of an initial value problem (IVP).</p> <p>This class stores the computed solution, its uncertainty estimates, and details of the probabilistic model used in probabilistic numerical integration.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>@containers.dataclass\nclass IVPSolution:\n    \"\"\"The probabilistic numerical solution of an initial value problem (IVP).\n\n    This class stores the computed solution,\n    its uncertainty estimates, and details of the probabilistic model\n    used in probabilistic numerical integration.\n    \"\"\"\n\n    t: Array\n    \"\"\"Time points at which the IVP solution has been computed.\"\"\"\n\n    u: Array\n    \"\"\"The mean of the IVP solution at each computed time point.\"\"\"\n\n    u_std: Array\n    \"\"\"The standard deviation of the IVP solution, indicating uncertainty.\"\"\"\n\n    output_scale: Array\n    \"\"\"The calibrated output scale of the probabilistic model.\"\"\"\n\n    marginals: Any\n    \"\"\"Marginal distributions for each time point in the posterior distribution.\"\"\"\n\n    posterior: Any\n    \"\"\"A the full posterior distribution of the probabilistic numerical solution.\n\n    Typically, a backward factorisation of the posterior.\n    \"\"\"\n\n    num_steps: Array\n    \"\"\"The number of solver steps taken at each time point.\"\"\"\n\n    ssm: Any\n    \"\"\"State-space model implementation used by the solver.\"\"\"\n\n    @staticmethod\n    def _register_pytree_node():\n        def _sol_flatten(sol):\n            children = (\n                sol.t,\n                sol.u,\n                sol.u_std,\n                sol.marginals,\n                sol.posterior,\n                sol.output_scale,\n                sol.num_steps,\n            )\n            aux = (sol.ssm,)\n            return children, aux\n\n        def _sol_unflatten(aux, children):\n            (ssm,) = aux\n            t, u, u_std, marginals, posterior, output_scale, n = children\n            return IVPSolution(\n                t=t,\n                u=u,\n                u_std=u_std,\n                marginals=marginals,\n                posterior=posterior,\n                output_scale=output_scale,\n                num_steps=n,\n                ssm=ssm,\n            )\n\n        tree_util.register_pytree_node(IVPSolution, _sol_flatten, _sol_unflatten)\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.marginals","title":"<code>marginals: Any</code>  <code>instance-attribute</code>","text":"<p>Marginal distributions for each time point in the posterior distribution.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.num_steps","title":"<code>num_steps: Array</code>  <code>instance-attribute</code>","text":"<p>The number of solver steps taken at each time point.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.output_scale","title":"<code>output_scale: Array</code>  <code>instance-attribute</code>","text":"<p>The calibrated output scale of the probabilistic model.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.posterior","title":"<code>posterior: Any</code>  <code>instance-attribute</code>","text":"<p>A the full posterior distribution of the probabilistic numerical solution.</p> <p>Typically, a backward factorisation of the posterior.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.ssm","title":"<code>ssm: Any</code>  <code>instance-attribute</code>","text":"<p>State-space model implementation used by the solver.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.t","title":"<code>t: Array</code>  <code>instance-attribute</code>","text":"<p>Time points at which the IVP solution has been computed.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.u","title":"<code>u: Array</code>  <code>instance-attribute</code>","text":"<p>The mean of the IVP solution at each computed time point.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.u_std","title":"<code>u_std: Array</code>  <code>instance-attribute</code>","text":"<p>The standard deviation of the IVP solution, indicating uncertainty.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.dt0","title":"<code>dt0(vf_autonomous, initial_values, /, scale=0.01, nugget=1e-05)</code>","text":"<p>Propose an initial time-step.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def dt0(vf_autonomous, initial_values, /, scale=0.01, nugget=1e-5):\n    \"\"\"Propose an initial time-step.\"\"\"\n    u0, *_ = initial_values\n    f0 = vf_autonomous(*initial_values)\n\n    u0, _ = tree_util.ravel_pytree(u0)\n    f0, _ = tree_util.ravel_pytree(f0)\n\n    norm_y0 = linalg.vector_norm(u0)\n    norm_dy0 = linalg.vector_norm(f0) + nugget\n\n    return scale * norm_y0 / norm_dy0\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.dt0_adaptive","title":"<code>dt0_adaptive(vf, initial_values, /, t0, *, error_contraction_rate, rtol, atol)</code>","text":"<p>Propose an initial time-step as a function of the tolerances.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def dt0_adaptive(vf, initial_values, /, t0, *, error_contraction_rate, rtol, atol):\n    \"\"\"Propose an initial time-step as a function of the tolerances.\"\"\"\n    # Algorithm from:\n    # E. Hairer, S. P. Norsett G. Wanner,\n    # Solving Ordinary Differential Equations I: Nonstiff Problems, Sec. II.4.\n    # Implementation mostly copied from\n    #\n    # https://github.com/google/jax/blob/main/jax/experimental/ode.py\n    #\n\n    if len(initial_values) &gt; 1:\n        raise ValueError\n    y0 = initial_values[0]\n\n    f0 = vf(y0, t=t0)\n\n    y0, unravel = tree_util.ravel_pytree(y0)\n    f0, _ = tree_util.ravel_pytree(f0)\n\n    scale = atol + np.abs(y0) * rtol\n    d0, d1 = linalg.vector_norm(y0), linalg.vector_norm(f0)\n\n    dt0 = np.where((d0 &lt; 1e-5) | (d1 &lt; 1e-5), 1e-6, 0.01 * d0 / d1)\n\n    y1 = y0 + dt0 * f0\n    f1 = vf(unravel(y1), t=t0 + dt0)\n    f1, _ = tree_util.ravel_pytree(f1)\n    d2 = linalg.vector_norm((f1 - f0) / scale) / dt0\n\n    dt1 = np.where(\n        (d1 &lt;= 1e-15) &amp; (d2 &lt;= 1e-15),\n        np.maximum(1e-6, dt0 * 1e-3),\n        (0.01 / np.maximum(d1, d2)) ** (1.0 / (error_contraction_rate + 1.0)),\n    )\n    return np.minimum(100.0 * dt0, dt1)\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_adaptive_save_at","title":"<code>solve_adaptive_save_at(ssm_init, /, *, save_at, adaptive_solver, dt0, ssm, warn=True) -&gt; IVPSolution</code>","text":"<p>Solve an initial value problem and return the solution at a pre-determined grid.</p> <p>This algorithm implements the method by Kr\u00e4mer (2024). Please consider citing it if you use it for your research. A PDF is available here and Kr\u00e4mer's (2024) experiments are available here.</p> BibTex for Kr\u00e4mer (2024) <pre><code>@InProceedings{kramer2024adaptive,\n    title     = {Adaptive Probabilistic ODE Solvers Without Adaptive Memory\n                Requirements},\n    author    = {Kr\\\"{a}mer, Nicholas},\n    booktitle = {Proceedings of the First International Conference on\n                Probabilistic Numerics},\n    pages     = {12--24},\n    year      = {2025},\n    editor    = {Kanagawa, Motonobu and Cockayne, Jon and Gessner, Alexandra\n                and Hennig, Philipp},\n    volume    = {271},\n    series    = {Proceedings of Machine Learning Research},\n    publisher = {PMLR},\n    url       = {https://proceedings.mlr.press/v271/kramer25a.html}\n}\n</code></pre> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_adaptive_save_at(\n    ssm_init, /, *, save_at, adaptive_solver, dt0, ssm, warn=True\n) -&gt; IVPSolution:\n    r\"\"\"Solve an initial value problem and return the solution at a pre-determined grid.\n\n    This algorithm implements the method by Kr\u00e4mer (2024). Please consider citing it\n    if you use it for your research. A PDF is available\n    [here](https://arxiv.org/abs/2410.10530) and Kr\u00e4mer's (2024) experiments are\n    available [here](https://github.com/pnkraemer/code-adaptive-prob-ode-solvers).\n\n    ??? note \"BibTex for Kr\u00e4mer (2024)\"\n        ```bibtex\n        @InProceedings{kramer2024adaptive,\n            title     = {Adaptive Probabilistic ODE Solvers Without Adaptive Memory\n                        Requirements},\n            author    = {Kr\\\"{a}mer, Nicholas},\n            booktitle = {Proceedings of the First International Conference on\n                        Probabilistic Numerics},\n            pages     = {12--24},\n            year      = {2025},\n            editor    = {Kanagawa, Motonobu and Cockayne, Jon and Gessner, Alexandra\n                        and Hennig, Philipp},\n            volume    = {271},\n            series    = {Proceedings of Machine Learning Research},\n            publisher = {PMLR},\n            url       = {https://proceedings.mlr.press/v271/kramer25a.html}\n        }\n        ```\n    \"\"\"\n    if not adaptive_solver.solver.is_suitable_for_save_at and warn:\n        msg = (\n            f\"Strategy {adaptive_solver.solver} should not \"\n            f\"be used in solve_adaptive_save_at. \"\n        )\n        warnings.warn(msg, stacklevel=1)\n\n    (_t, solution_save_at), _, num_steps = _solve_adaptive_save_at(\n        save_at[0],\n        ssm_init,\n        save_at=save_at[1:],\n        adaptive_solver=adaptive_solver,\n        dt0=dt0,\n    )\n\n    # I think the user expects the initial condition to be part of the state\n    # (as well as marginals), so we compute those things here\n    posterior_save_at, output_scale = solution_save_at\n    _tmp = _userfriendly_output(posterior=posterior_save_at, ssm_init=ssm_init, ssm=ssm)\n    marginals, posterior = _tmp\n    u = ssm.stats.qoi_from_sample(marginals.mean)\n    std = ssm.stats.standard_deviation(marginals)\n    u_std = ssm.stats.qoi_from_sample(std)\n    return IVPSolution(\n        t=save_at,\n        u=u,\n        u_std=u_std,\n        marginals=marginals,\n        posterior=posterior,\n        output_scale=output_scale,\n        num_steps=num_steps,\n        ssm=ssm,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_adaptive_save_every_step","title":"<code>solve_adaptive_save_every_step(ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm) -&gt; IVPSolution</code>","text":"<p>Solve an initial value problem and save every step.</p> <p>This function uses a native-Python while loop.</p> <p>Warning</p> <p>Not JITable, not reverse-mode-differentiable.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_adaptive_save_every_step(\n    ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm\n) -&gt; IVPSolution:\n    \"\"\"Solve an initial value problem and save every step.\n\n    This function uses a native-Python while loop.\n\n    !!! warning\n        Not JITable, not reverse-mode-differentiable.\n    \"\"\"\n    if not adaptive_solver.solver.is_suitable_for_save_every_step:\n        msg = (\n            f\"Strategy {adaptive_solver.solver} should not \"\n            f\"be used in solve_adaptive_save_every_step.\"\n        )\n        warnings.warn(msg, stacklevel=1)\n\n    generator = _solution_generator(\n        t0, ssm_init, t1=t1, adaptive_solver=adaptive_solver, dt0=dt0\n    )\n    tmp = tree_array_util.tree_stack(list(generator))\n    (t, solution_every_step), _dt, num_steps = tmp\n\n    # I think the user expects the initial time-point to be part of the grid\n    # (Even though t0 is not computed by this function)\n    t = np.concatenate((np.atleast_1d(t0), t))\n\n    # I think the user expects marginals, so we compute them here\n    posterior, output_scale = solution_every_step\n    _tmp = _userfriendly_output(posterior=posterior, ssm_init=ssm_init, ssm=ssm)\n    marginals, posterior = _tmp\n\n    u = ssm.stats.qoi_from_sample(marginals.mean)\n    std = ssm.stats.standard_deviation(marginals)\n    u_std = ssm.stats.qoi_from_sample(std)\n    return IVPSolution(\n        t=t,\n        u=u,\n        u_std=u_std,\n        ssm=ssm,\n        marginals=marginals,\n        posterior=posterior,\n        output_scale=output_scale,\n        num_steps=num_steps,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_adaptive_terminal_values","title":"<code>solve_adaptive_terminal_values(ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm) -&gt; IVPSolution</code>","text":"<p>Simulate the terminal values of an initial value problem.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_adaptive_terminal_values(\n    ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm\n) -&gt; IVPSolution:\n    \"\"\"Simulate the terminal values of an initial value problem.\"\"\"\n    save_at = np.asarray([t0, t1])\n    solution = solve_adaptive_save_at(\n        ssm_init,\n        save_at=save_at,\n        adaptive_solver=adaptive_solver,\n        dt0=dt0,\n        ssm=ssm,\n        warn=False,  # Turn off warnings because any solver goes for terminal values\n    )\n    return tree_util.tree_map(lambda s: s[-1], solution)\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_fixed_grid","title":"<code>solve_fixed_grid(ssm_init, /, *, grid, solver, ssm) -&gt; IVPSolution</code>","text":"<p>Solve an initial value problem on a fixed, pre-determined grid.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_fixed_grid(ssm_init, /, *, grid, solver, ssm) -&gt; IVPSolution:\n    \"\"\"Solve an initial value problem on a fixed, pre-determined grid.\"\"\"\n    # Compute the solution\n\n    def body_fn(s, dt):\n        _error, s_new = solver.step(state=s, dt=dt)\n        return s_new, s_new\n\n    t0 = grid[0]\n    state0 = solver.init(t0, ssm_init)\n    _, result_state = control_flow.scan(body_fn, init=state0, xs=np.diff(grid))\n    _t, (posterior, output_scale) = solver.extract(result_state)\n\n    # I think the user expects marginals, so we compute them here\n    _tmp = _userfriendly_output(posterior=posterior, ssm_init=ssm_init, ssm=ssm)\n    marginals, posterior = _tmp\n\n    u = ssm.stats.qoi_from_sample(marginals.mean)\n    std = ssm.stats.standard_deviation(marginals)\n    u_std = ssm.stats.qoi_from_sample(std)\n    return IVPSolution(\n        t=grid,\n        u=u,\n        u_std=u_std,\n        ssm=ssm,\n        marginals=marginals,\n        posterior=posterior,\n        output_scale=output_scale,\n        num_steps=np.arange(1.0, len(grid)),\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/","title":"ivpsolvers","text":"<p>Probabilistic IVP solvers.</p>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.adaptive","title":"<code>adaptive(slvr, /, *, ssm, atol=0.0001, rtol=0.01, control=None, norm_ord=None, clip_dt: bool = False, eps: float | None = None)</code>","text":"<p>Make an IVP solver adaptive.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def adaptive(\n    slvr,\n    /,\n    *,\n    ssm,\n    atol=1e-4,\n    rtol=1e-2,\n    control=None,\n    norm_ord=None,\n    clip_dt: bool = False,\n    eps: float | None = None,\n):\n    \"\"\"Make an IVP solver adaptive.\"\"\"\n    if control is None:\n        control = control_proportional_integral()\n    if eps is None:\n        eps = 10 * np.finfo_eps(float)\n    return _AdaSolver(\n        slvr,\n        ssm=ssm,\n        atol=atol,\n        rtol=rtol,\n        control=control,\n        norm_ord=norm_ord,\n        clip_dt=clip_dt,\n        eps=eps,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.control_integral","title":"<code>control_integral(*, safety=0.95, factor_min=0.2, factor_max=10.0) -&gt; _Controller[None]</code>","text":"<p>Construct an integral-controller.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def control_integral(\n    *, safety=0.95, factor_min=0.2, factor_max=10.0\n) -&gt; _Controller[None]:\n    \"\"\"Construct an integral-controller.\"\"\"\n\n    def init(_dt, /) -&gt; None:\n        return None\n\n    def apply(dt, _state, /, *, error_power):\n        # error_power = error_norm ** (-1.0 / error_contraction_rate)\n        scale_factor_unclipped = safety * error_power\n\n        scale_factor_clipped_min = np.minimum(scale_factor_unclipped, factor_max)\n        scale_factor = np.maximum(factor_min, scale_factor_clipped_min)\n        return scale_factor * dt, None\n\n    return _Controller(init=init, apply=apply)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.control_proportional_integral","title":"<code>control_proportional_integral(*, safety=0.95, factor_min=0.2, factor_max=10.0, power_integral_unscaled=0.3, power_proportional_unscaled=0.4) -&gt; _Controller[float]</code>","text":"<p>Construct a proportional-integral-controller with time-clipping.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def control_proportional_integral(\n    *,\n    safety=0.95,\n    factor_min=0.2,\n    factor_max=10.0,\n    power_integral_unscaled=0.3,\n    power_proportional_unscaled=0.4,\n) -&gt; _Controller[float]:\n    \"\"\"Construct a proportional-integral-controller with time-clipping.\"\"\"\n\n    def init(_dt: float, /) -&gt; float:\n        return 1.0\n\n    def apply(dt: float, error_power_prev: float, /, *, error_power):\n        # Equivalent: error_power = error_norm ** (-1.0 / error_contraction_rate)\n        a1 = error_power**power_integral_unscaled\n        a2 = (error_power / error_power_prev) ** power_proportional_unscaled\n        scale_factor_unclipped = safety * a1 * a2\n\n        scale_factor_clipped_min = np.minimum(scale_factor_unclipped, factor_max)\n        scale_factor = np.maximum(factor_min, scale_factor_clipped_min)\n\n        # &gt;= 1.0 because error_power is 1/scaled_error_norm\n        error_power_prev = np.where(error_power &gt;= 1.0, error_power, error_power_prev)\n\n        dt_proposed = scale_factor * dt\n        return dt_proposed, error_power_prev\n\n    return _Controller(init=init, apply=apply)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_slr0","title":"<code>correction_slr0(vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0) -&gt; _Correction</code>","text":"<p>Zeroth-order statistical linear regression.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_slr0(\n    vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0\n) -&gt; _Correction:\n    \"\"\"Zeroth-order statistical linear regression.\"\"\"\n    linearize = ssm.linearise.ode_statistical_0th(cubature_fun, damp=damp)\n    return _Correction(\n        ssm=ssm,\n        vector_field=vector_field,\n        ode_order=1,\n        linearize=linearize,\n        name=\"SLR0\",\n        re_linearize=True,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_slr1","title":"<code>correction_slr1(vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0) -&gt; _Correction</code>","text":"<p>First-order statistical linear regression.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_slr1(\n    vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0\n) -&gt; _Correction:\n    \"\"\"First-order statistical linear regression.\"\"\"\n    linearize = ssm.linearise.ode_statistical_1st(cubature_fun, damp=damp)\n    return _Correction(\n        ssm=ssm,\n        vector_field=vector_field,\n        ode_order=1,\n        linearize=linearize,\n        name=\"SLR1\",\n        re_linearize=True,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_ts0","title":"<code>correction_ts0(vector_field, *, ssm, ode_order=1, damp: float = 0.0) -&gt; _Correction</code>","text":"<p>Zeroth-order Taylor linearisation.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_ts0(vector_field, *, ssm, ode_order=1, damp: float = 0.0) -&gt; _Correction:\n    \"\"\"Zeroth-order Taylor linearisation.\"\"\"\n    linearize = ssm.linearise.ode_taylor_0th(ode_order=ode_order, damp=damp)\n    return _Correction(\n        name=\"TS0\",\n        vector_field=vector_field,\n        ode_order=ode_order,\n        ssm=ssm,\n        linearize=linearize,\n        re_linearize=False,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_ts1","title":"<code>correction_ts1(vector_field, *, ssm, ode_order=1, damp: float = 0.0, jvp_probes=10, jvp_probes_seed=1) -&gt; _Correction</code>","text":"<p>First-order Taylor linearisation.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_ts1(\n    vector_field,\n    *,\n    ssm,\n    ode_order=1,\n    damp: float = 0.0,\n    jvp_probes=10,\n    jvp_probes_seed=1,\n) -&gt; _Correction:\n    \"\"\"First-order Taylor linearisation.\"\"\"\n    assert jvp_probes &gt; 0\n    linearize = ssm.linearise.ode_taylor_1st(\n        ode_order=ode_order,\n        damp=damp,\n        jvp_probes=jvp_probes,\n        jvp_probes_seed=jvp_probes_seed,\n    )\n    return _Correction(\n        name=\"TS1\",\n        vector_field=vector_field,\n        ode_order=ode_order,\n        ssm=ssm,\n        linearize=linearize,\n        re_linearize=False,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.cubature_gauss_hermite","title":"<code>cubature_gauss_hermite(input_shape, degree=5) -&gt; _PositiveCubatureRule</code>","text":"<p>(Statistician's) Gauss-Hermite cubature.</p> <p>The number of cubature points is <code>prod(input_shape)**degree</code>.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def cubature_gauss_hermite(input_shape, degree=5) -&gt; _PositiveCubatureRule:\n    \"\"\"(Statistician's) Gauss-Hermite cubature.\n\n    The number of cubature points is `prod(input_shape)**degree`.\n    \"\"\"\n    assert len(input_shape) == 1\n    (dim,) = input_shape\n\n    # Roots of the probabilist/statistician's Hermite polynomials (in Numpy...)\n    _roots = special.roots_hermitenorm(n=degree, mu=True)\n    pts, weights, sum_of_weights = _roots\n    weights = weights / sum_of_weights\n\n    # Transform into jax arrays and take square root of weights\n    pts = np.asarray(pts)\n    weights_sqrtm = np.sqrt(np.asarray(weights))\n\n    # Build a tensor grid and return class\n    tensor_pts = _tensor_points(pts, d=dim)\n    tensor_weights_sqrtm = _tensor_weights(weights_sqrtm, d=dim)\n    return _PositiveCubatureRule(points=tensor_pts, weights_sqrtm=tensor_weights_sqrtm)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.cubature_third_order_spherical","title":"<code>cubature_third_order_spherical(input_shape) -&gt; _PositiveCubatureRule</code>","text":"<p>Third-order spherical cubature integration.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def cubature_third_order_spherical(input_shape) -&gt; _PositiveCubatureRule:\n    \"\"\"Third-order spherical cubature integration.\"\"\"\n    assert len(input_shape) &lt;= 1\n    if len(input_shape) == 1:\n        (d,) = input_shape\n        points_mat, weights_sqrtm = _third_order_spherical_params(d=d)\n        return _PositiveCubatureRule(points=points_mat, weights_sqrtm=weights_sqrtm)\n\n    # If input_shape == (), compute weights via input_shape=(1,)\n    # and 'squeeze' the points.\n    points_mat, weights_sqrtm = _third_order_spherical_params(d=1)\n    (S, _) = points_mat.shape\n    points = np.reshape(points_mat, (S,))\n    return _PositiveCubatureRule(points=points, weights_sqrtm=weights_sqrtm)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.cubature_unscented_transform","title":"<code>cubature_unscented_transform(input_shape, r=1.0) -&gt; _PositiveCubatureRule</code>","text":"<p>Unscented transform.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def cubature_unscented_transform(input_shape, r=1.0) -&gt; _PositiveCubatureRule:\n    \"\"\"Unscented transform.\"\"\"\n    assert len(input_shape) &lt;= 1\n    if len(input_shape) == 1:\n        (d,) = input_shape\n        points_mat, weights_sqrtm = _unscented_transform_params(d=d, r=r)\n        return _PositiveCubatureRule(points=points_mat, weights_sqrtm=weights_sqrtm)\n\n    # If input_shape == (), compute weights via input_shape=(1,)\n    # and 'squeeze' the points.\n    points_mat, weights_sqrtm = _unscented_transform_params(d=1, r=r)\n    (S, _) = points_mat.shape\n    points = np.reshape(points_mat, (S,))\n    return _PositiveCubatureRule(points=points, weights_sqrtm=weights_sqrtm)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.prior_wiener_integrated","title":"<code>prior_wiener_integrated(tcoeffs, *, ssm_fact: str, output_scale: ArrayLike | None = None, damp: float = 0.0)</code>","text":"<p>Construct an adaptive(/continuous-time), multiply-integrated Wiener process.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def prior_wiener_integrated(\n    tcoeffs, *, ssm_fact: str, output_scale: ArrayLike | None = None, damp: float = 0.0\n):\n    \"\"\"Construct an adaptive(/continuous-time), multiply-integrated Wiener process.\"\"\"\n    ssm = impl.choose(ssm_fact, tcoeffs_like=tcoeffs)\n\n    # TODO: should the output_scale be an argument to solve()?\n    # TODO: should the output scale (and all 'damp'-like factors)\n    #       mirror the pytree structure of 'tcoeffs'?\n    if output_scale is None:\n        output_scale = np.ones_like(ssm.prototypes.output_scale())\n\n    discretize = ssm.conditional.ibm_transitions(base_scale=output_scale)\n\n    # Increase damping to get visually more pleasing uncertainties\n    #  and more numerical robustness for\n    #  high-order solvers in low precision arithmetic\n    init = ssm.normal.from_tcoeffs(tcoeffs, damp=damp)\n    return init, discretize, ssm\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.prior_wiener_integrated_discrete","title":"<code>prior_wiener_integrated_discrete(ts, *args, **kwargs)</code>","text":"<p>Compute a time-discretized, multiply-integrated Wiener process.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def prior_wiener_integrated_discrete(ts, *args, **kwargs):\n    \"\"\"Compute a time-discretized, multiply-integrated Wiener process.\"\"\"\n    init, discretize, ssm = prior_wiener_integrated(*args, **kwargs)\n    scales = np.ones_like(ssm.prototypes.output_scale())\n    discretize_vmap = functools.vmap(discretize, in_axes=(0, None))\n    conditionals = discretize_vmap(np.diff(ts), scales)\n    return init, conditionals, ssm\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.solver","title":"<code>solver(strategy, *, correction, prior, ssm)</code>","text":"<p>Create a solver that does not calibrate the output scale automatically.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def solver(strategy, *, correction, prior, ssm):\n    \"\"\"Create a solver that does not calibrate the output scale automatically.\"\"\"\n\n    def step(state: _State, *, dt, calibration):\n        del calibration  # unused\n\n        u_step_from = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n\n        # Estimate the error\n        transition = prior(dt, state.output_scale)\n        mean = ssm.stats.mean(state.rv)\n        hidden = ssm.conditional.apply(mean, transition)\n        t = state.t + dt\n        error, _, correction_state = correction.estimate_error(\n            hidden, state.correction_state, t=t\n        )\n\n        # Do the full extrapolation step (reuse the transition)\n        hidden, extra = strategy.extrapolate(\n            state.rv, state.strategy_state, transition=transition\n        )\n\n        # Do the full correction step\n        hidden, _, correction_state = correction.correct(hidden, correction_state, t=t)\n        state = _State(\n            t=t,\n            rv=hidden,\n            strategy_state=extra,\n            correction_state=correction_state,\n            output_scale=state.output_scale,\n        )\n\n        # Normalise the error\n        u_proposed = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n        reference = np.maximum(np.abs(u_proposed), np.abs(u_step_from))\n        error = _ErrorEstimate(dt * error, reference=reference)\n        return error, state\n\n    return _ProbabilisticSolver(\n        ssm=ssm,\n        prior=prior,\n        strategy=strategy,\n        correction=correction,\n        calibration=_calibration_none(ssm=ssm),\n        step_implementation=step,\n        name=\"Probabilistic solver\",\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.solver_dynamic","title":"<code>solver_dynamic(strategy, *, correction, prior, ssm)</code>","text":"<p>Create a solver that calibrates the output scale dynamically.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def solver_dynamic(strategy, *, correction, prior, ssm):\n    \"\"\"Create a solver that calibrates the output scale dynamically.\"\"\"\n\n    def step_dynamic(state, /, *, dt, calibration):\n        u_step_from = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n\n        # Estimate error and calibrate the output scale\n        ones = np.ones_like(ssm.prototypes.output_scale())\n        transition = prior(dt, ones)\n        mean = ssm.stats.mean(state.rv)\n        hidden = ssm.conditional.apply(mean, transition)\n\n        t = state.t + dt\n        error, observed, correction_state = correction.estimate_error(\n            hidden, state.correction_state, t=t\n        )\n        output_scale = calibration.update(state.output_scale, observed=observed)\n\n        # Do the full extrapolation with the calibrated output scale\n        scale, _ = calibration.extract(output_scale)\n        transition = prior(dt, scale)\n        hidden, extra = strategy.extrapolate(\n            state.rv, state.strategy_state, transition=transition\n        )\n\n        # Do the full correction step\n        hidden, _, correction_state = correction.correct(hidden, correction_state, t=t)\n\n        # Return solution\n        state = _State(\n            t=t,\n            rv=hidden,\n            strategy_state=extra,\n            correction_state=correction_state,\n            output_scale=output_scale,\n        )\n\n        # Normalise the error\n        u_proposed = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n        reference = np.maximum(np.abs(u_proposed), np.abs(u_step_from))\n        error = _ErrorEstimate(dt * error, reference=reference)\n        return error, state\n\n    return _ProbabilisticSolver(\n        prior=prior,\n        ssm=ssm,\n        strategy=strategy,\n        correction=correction,\n        calibration=_calibration_most_recent(ssm=ssm),\n        name=\"Dynamic probabilistic solver\",\n        step_implementation=step_dynamic,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.solver_mle","title":"<code>solver_mle(strategy, *, correction, prior, ssm)</code>","text":"<p>Create a solver that calibrates the output scale via maximum-likelihood.</p> <p>Warning: needs to be combined with a call to stats.calibrate() after solving if the MLE-calibration shall be used.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def solver_mle(strategy, *, correction, prior, ssm):\n    \"\"\"Create a solver that calibrates the output scale via maximum-likelihood.\n\n    Warning: needs to be combined with a call to stats.calibrate()\n    after solving if the MLE-calibration shall be *used*.\n    \"\"\"\n\n    def step_mle(state, /, *, dt, calibration):\n        u_step_from = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n\n        # Estimate the error\n        output_scale_prior, _calibrated = calibration.extract(state.output_scale)\n        transition = prior(dt, output_scale_prior)\n        mean = ssm.stats.mean(state.rv)\n        mean_extra = ssm.conditional.apply(mean, transition)\n        t = state.t + dt\n        error, _, correction_state = correction.estimate_error(\n            mean_extra, state.correction_state, t=t\n        )\n\n        # Do the full prediction step (reuse previous discretisation)\n        hidden, extra = strategy.extrapolate(\n            state.rv, state.strategy_state, transition=transition\n        )\n\n        # Do the full correction step\n        hidden, observed, corr_state = correction.correct(hidden, correction_state, t=t)\n\n        # Calibrate the output scale\n        output_scale = calibration.update(state.output_scale, observed=observed)\n\n        # Normalise the error\n\n        state = _State(\n            t=t,\n            rv=hidden,\n            strategy_state=extra,\n            correction_state=corr_state,\n            output_scale=output_scale,\n        )\n        u_proposed = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n        reference = np.maximum(np.abs(u_proposed), np.abs(u_step_from))\n        error = _ErrorEstimate(dt * error, reference=reference)\n        return error, state\n\n    return _ProbabilisticSolver(\n        ssm=ssm,\n        name=\"Probabilistic solver with MLE calibration\",\n        prior=prior,\n        calibration=_calibration_running_mean(ssm=ssm),\n        step_implementation=step_mle,\n        strategy=strategy,\n        correction=correction,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.strategy_filter","title":"<code>strategy_filter(*, ssm) -&gt; _Strategy</code>","text":"<p>Construct a filter.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def strategy_filter(*, ssm) -&gt; _Strategy:\n    \"\"\"Construct a filter.\"\"\"\n\n    @containers.dataclass\n    class Filter(_Strategy):\n        def init(self, sol, /):\n            return sol, None\n\n        def extrapolate(self, rv, aux, /, *, transition):\n            del aux\n            rv = self.ssm.conditional.marginalise(rv, transition)\n\n            return rv, None\n\n        def extract(self, hidden_state, _extra, /):\n            return hidden_state\n\n        def interpolate(self, state_t0, state_t1, dt0, dt1, output_scale, *, prior):\n            # todo: by ditching marginal_t1 and dt1, this function _extrapolates\n            #  (no *inter*polation happening)\n            del dt1\n            marginal_t1, _ = state_t1\n\n            hidden, extra = state_t0\n            prior0 = prior(dt0, output_scale)\n            hidden, extra = self.extrapolate(hidden, extra, transition=prior0)\n\n            # Consistent state-types in interpolation result.\n            interp = (hidden, extra)\n            step_from = (marginal_t1, None)\n            return _InterpRes(\n                step_from=step_from, interpolated=interp, interp_from=interp\n            )\n\n        def interpolate_at_t1(\n            self, state_t0, state_t1, dt0, dt1, output_scale, *, prior\n        ):\n            del prior\n            del state_t0\n            del dt0\n            del dt1\n            del output_scale\n            rv, extra = state_t1\n            return _InterpRes((rv, extra), (rv, extra), (rv, extra))\n\n    return Filter(\n        ssm=ssm,\n        is_suitable_for_save_at=True,\n        is_suitable_for_save_every_step=True,\n        is_suitable_for_offgrid_marginals=True,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.strategy_fixedpoint","title":"<code>strategy_fixedpoint(*, ssm) -&gt; _Strategy</code>","text":"<p>Construct a fixedpoint-smoother.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def strategy_fixedpoint(*, ssm) -&gt; _Strategy:\n    \"\"\"Construct a fixedpoint-smoother.\"\"\"\n\n    @containers.dataclass\n    class FixedPoint(_Strategy):\n        def init(self, sol, /):\n            cond = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            return sol, cond\n\n        def extrapolate(self, rv, bw0, /, *, transition):\n            extrapolated, cond = self.ssm.conditional.revert(rv, transition)\n            cond = self.ssm.conditional.merge(bw0, cond)\n            return extrapolated, cond\n\n        def extract(self, hidden_state, extra, /):\n            return stats.MarkovSeq(init=hidden_state, conditional=extra)\n\n        def interpolate_at_t1(\n            self, state_t0, state_t1, *, dt0, dt1, output_scale, prior\n        ):\n            del prior\n            del state_t0\n            del dt0\n            del dt1\n            del output_scale\n            rv, extra = state_t1\n            cond_identity = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            return _InterpRes((rv, cond_identity), (rv, extra), (rv, cond_identity))\n\n        def interpolate(self, state_t0, state_t1, *, dt0, dt1, output_scale, prior):\n            \"\"\"Interpolate.\n\n            A fixed-point smoother interpolates by\n\n            * Extrapolating from t0 to t, which gives the \"filtering\" marginal\n            and the backward transition from t to t0.\n            * Extrapolating from t to t1, which gives another \"filtering\" marginal\n            and the backward transition from t1 to t.\n            * Applying the t1-to-t backward transition\n            to compute the interpolation result.\n            This intermediate result is informed about its \"right-hand side\" datum.\n\n            The difference to smoother-interpolation is quite subtle:\n\n            * The backward transition of the solution at 't'\n            is merged with that at 't0'.\n            The reason is that the backward transition at 't0' knows\n            \"how to get to the quantity of interest\",\n            and this is precisely what we want to interpolate.\n            * Subsequent interpolations do not continue from the value at 't', but\n            from a very similar value where the backward transition\n            is replaced with an identity. The reason is that the interpolated solution\n            becomes the new quantity of interest, and subsequent interpolations\n            need to learn how to get here.\n            * Subsequent solver steps do not continue from the value at 't1',\n            but the value at 't1' where the backward model is replaced by\n            the 't1-to-t' backward model. The reason is similar to the above:\n            future steps need to know \"how to get back to the quantity of interest\",\n            which is the interpolated solution.\n\n            These distinctions are precisely why we need three fields\n            in every interpolation result:\n                the solution,\n                the continue-interpolation-from-here,\n                and the continue-stepping-from-here.\n            All three are different for fixed point smoothers.\n            (Really, I try removing one of them monthly and\n            then don't understand why tests fail.)\n            \"\"\"\n            marginal_t1, _ = state_t1\n            # Extrapolate from t0 to t, and from t to t1.\n            # This yields all building blocks.\n            prior0 = prior(dt0, output_scale)\n            extrapolated_t = self.extrapolate(*state_t0, transition=prior0)\n            conditional_id = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            previous_new = (extrapolated_t[0], conditional_id)\n\n            prior1 = prior(dt1, output_scale)\n            extrapolated_t1 = self.extrapolate(*previous_new, transition=prior1)\n\n            # Marginalise from t1 to t to obtain the interpolated solution.\n            conditional_t1_to_t = extrapolated_t1[1]\n            rv_at_t = self.ssm.conditional.marginalise(marginal_t1, conditional_t1_to_t)\n\n            # Return the right combination of marginals and conditionals.\n            return _InterpRes(\n                step_from=(marginal_t1, conditional_t1_to_t),\n                interpolated=(rv_at_t, extrapolated_t[1]),\n                interp_from=previous_new,\n            )\n\n    return FixedPoint(\n        ssm=ssm,\n        is_suitable_for_save_at=True,\n        is_suitable_for_save_every_step=False,\n        is_suitable_for_offgrid_marginals=False,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.strategy_smoother","title":"<code>strategy_smoother(*, ssm) -&gt; _Strategy</code>","text":"<p>Construct a smoother.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def strategy_smoother(*, ssm) -&gt; _Strategy:\n    \"\"\"Construct a smoother.\"\"\"\n\n    @containers.dataclass\n    class Smoother(_Strategy):\n        def init(self, sol, /):\n            # Special case for implementing offgrid-marginals...\n            if isinstance(sol, stats.MarkovSeq):\n                rv = sol.init\n                cond = sol.conditional\n            else:\n                rv = sol\n                cond = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            return rv, cond\n\n        def extrapolate(self, rv, aux, /, *, transition):\n            del aux\n            return self.ssm.conditional.revert(rv, transition)\n\n        def extract(self, hidden_state, extra, /):\n            return stats.MarkovSeq(init=hidden_state, conditional=extra)\n\n        def interpolate(self, state_t0, state_t1, *, dt0, dt1, output_scale, prior):\n            \"\"\"Interpolate.\n\n            A smoother interpolates by_\n            * Extrapolating from t0 to t, which gives the \"filtering\" marginal\n            and the backward transition from t to t0.\n            * Extrapolating from t to t1, which gives another \"filtering\" marginal\n            and the backward transition from t1 to t.\n            * Applying the new t1-to-t backward transition to compute the interpolation.\n            This intermediate result is informed about its \"right-hand side\" datum.\n\n            Subsequent interpolations continue from the value at 't'.\n            Subsequent IVP solver steps continue from the value at 't1'.\n            \"\"\"\n            # TODO: if we pass prior1 and prior2, then\n            #       we don't have to pass dt0, dt1, output_scale, and prior...\n\n            # Extrapolate from t0 to t, and from t to t1.\n            prior0 = prior(dt0, output_scale)\n            extrapolated_t = self.extrapolate(*state_t0, transition=prior0)\n            prior1 = prior(dt1, output_scale)\n            extrapolated_t1 = self.extrapolate(*extrapolated_t, transition=prior1)\n\n            # Marginalise from t1 to t to obtain the interpolated solution.\n            marginal_t1, _ = state_t1\n            conditional_t1_to_t = extrapolated_t1[1]\n            rv_at_t = self.ssm.conditional.marginalise(marginal_t1, conditional_t1_to_t)\n            solution_at_t = (rv_at_t, extrapolated_t[1])\n\n            # The state at t1 gets a new backward model;\n            # (it must remember how to get back to t, not to t0).\n            solution_at_t1 = (marginal_t1, conditional_t1_to_t)\n            return _InterpRes(\n                step_from=solution_at_t1,\n                interpolated=solution_at_t,\n                interp_from=solution_at_t,\n            )\n\n        def interpolate_at_t1(\n            self, state_t0, state_t1, *, dt0, dt1, output_scale, prior\n        ):\n            del prior\n            del state_t0\n            del dt0\n            del dt1\n            del output_scale\n            return _InterpRes(state_t1, state_t1, state_t1)\n\n    return Smoother(\n        ssm=ssm,\n        is_suitable_for_save_at=False,\n        is_suitable_for_save_every_step=True,\n        is_suitable_for_offgrid_marginals=True,\n    )\n</code></pre>"},{"location":"api_docs/stats/","title":"stats","text":"<p>Interact with IVP solutions.</p> <p>For example, this module contains functionality to compute off-grid marginals, or to evaluate marginal likelihoods of observations of the solutions.</p>"},{"location":"api_docs/stats/#probdiffeq.stats.MarkovSeq","title":"<code>MarkovSeq</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Markov sequence.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>class MarkovSeq(containers.NamedTuple):\n    \"\"\"Markov sequence.\"\"\"\n\n    init: Any\n    conditional: Any\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.calibrate","title":"<code>calibrate(x, /, output_scale, *, ssm)</code>","text":"<p>Calibrated a posterior distribution of an IVP solution.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def calibrate(x, /, output_scale, *, ssm):\n    \"\"\"Calibrated a posterior distribution of an IVP solution.\"\"\"\n    if np.ndim(output_scale) &gt; np.ndim(ssm.prototypes.output_scale()):\n        output_scale = output_scale[-1]\n    if isinstance(x, MarkovSeq):\n        return _markov_rescale_cholesky(x, output_scale, ssm=ssm)\n    return ssm.stats.rescale_cholesky(x, output_scale)\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.log_marginal_likelihood","title":"<code>log_marginal_likelihood(u, /, *, standard_deviation, posterior, ssm)</code>","text":"<p>Compute the log-marginal-likelihood of observations of the IVP solution.</p> <p>Note</p> <p>Use <code>log_marginal_likelihood_terminal_values</code> to compute the log-likelihood at the terminal values.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <p>Observation. Expected to match the ODE's type/shape.</p> required <code>standard_deviation</code> <p>Standard deviation of the observation. Expected to match 'u's Pytree structure, but every leaf must be a scalar.</p> required <code>posterior</code> <p>Posterior distribution. Expected to correspond to a solution of an ODE with shape (d,).</p> required Source code in <code>probdiffeq/stats.py</code> <pre><code>def log_marginal_likelihood(u, /, *, standard_deviation, posterior, ssm):\n    \"\"\"Compute the log-marginal-likelihood of observations of the IVP solution.\n\n    !!! note\n        Use `log_marginal_likelihood_terminal_values`\n        to compute the log-likelihood at the terminal values.\n\n    Parameters\n    ----------\n    u\n        Observation. Expected to match the ODE's type/shape.\n    standard_deviation\n        Standard deviation of the observation. Expected to match 'u's\n        Pytree structure, but every leaf must be a scalar.\n    posterior\n        Posterior distribution.\n        Expected to correspond to a solution of an ODE with shape (d,).\n    \"\"\"\n    [u_leaves], u_structure = tree_util.tree_flatten(u)\n    [std_leaves], std_structure = tree_util.tree_flatten(standard_deviation)\n\n    if u_structure != std_structure:\n        msg = (\n            f\"Observation-noise tree structure {std_structure} \"\n            f\"does not match the observation structure {u_structure}. \"\n        )\n        raise ValueError(msg)\n\n    qoi_flat, _ = tree_util.ravel_pytree(ssm.prototypes.qoi())\n    if np.ndim(std_leaves) &lt; 1 or np.ndim(u_leaves) != np.ndim(qoi_flat) + 1:\n        msg = (\n            f\"Time-series solution expected. \"\n            f\"ndim={np.ndim(u_leaves)}, shape={np.shape(u_leaves)} received.\"\n        )\n        raise ValueError(msg)\n\n    if len(u_leaves) != len(np.asarray(std_leaves)):\n        msg = (\n            f\"Observation-noise shape {np.shape(std_leaves)} \"\n            f\"does not match the observation shape {np.shape(u_leaves)}. \"\n        )\n        raise ValueError(msg)\n\n    if not isinstance(posterior, MarkovSeq):\n        msg1 = \"Time-series marginal likelihoods \"\n        msg2 = \"cannot be computed with a filtering solution.\"\n        raise TypeError(msg1 + msg2)\n\n    # Generate an observation-model for the QOI\n\n    model_fun = functools.vmap(ssm.conditional.to_derivative, in_axes=(None, 0, 0))\n    models = model_fun(0, u, standard_deviation)\n\n    # Select the terminal variable\n    rv = tree_util.tree_map(lambda s: s[-1, ...], posterior.init)\n\n    # Run the reverse Kalman filter\n    estimator = filter_util.kalmanfilter_with_marginal_likelihood(ssm=ssm)\n    (_corrected, _num_data, logpdf), _ = filter_util.estimate_rev(\n        np.zeros_like(u_leaves),\n        init=rv,\n        prior_transitions=posterior.conditional,\n        observation_model=models,\n        estimator=estimator,\n    )\n\n    # Return only the logpdf\n    return logpdf\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.log_marginal_likelihood_terminal_values","title":"<code>log_marginal_likelihood_terminal_values(u, /, *, standard_deviation, posterior, ssm)</code>","text":"<p>Compute the log-marginal-likelihood at the terminal value.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <p>Observation. Expected to have shape (d,) for an ODE with shape (d,).</p> required <code>standard_deviation</code> <p>Standard deviation of the observation. Expected to be a scalar.</p> required <code>posterior</code> <p>Posterior distribution. Expected to correspond to a solution of an ODE with shape (d,).</p> required Source code in <code>probdiffeq/stats.py</code> <pre><code>def log_marginal_likelihood_terminal_values(\n    u, /, *, standard_deviation, posterior, ssm\n):\n    \"\"\"Compute the log-marginal-likelihood at the terminal value.\n\n    Parameters\n    ----------\n    u\n        Observation. Expected to have shape (d,) for an ODE with shape (d,).\n    standard_deviation\n        Standard deviation of the observation. Expected to be a scalar.\n    posterior\n        Posterior distribution.\n        Expected to correspond to a solution of an ODE with shape (d,).\n    \"\"\"\n    [u_leaves], u_structure = tree_util.tree_flatten(u)\n    [std_leaves], std_structure = tree_util.tree_flatten(standard_deviation)\n\n    if u_structure != std_structure:\n        msg = (\n            f\"Observation-noise tree structure {std_structure} \"\n            f\"does not match the observation structure {u_structure}. \"\n        )\n        raise ValueError(msg)\n\n    # Generate an observation-model for the QOI\n    model = ssm.conditional.to_derivative(0, u, standard_deviation)\n    rv = posterior.init if isinstance(posterior, MarkovSeq) else posterior\n\n    data = np.zeros_like(u_leaves)  # 'u' is baked into the observation model\n    _corrected, logpdf = _condition_and_logpdf(rv, data, model, ssm=ssm)\n    return logpdf\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.markov_marginals","title":"<code>markov_marginals(markov_seq: MarkovSeq, *, reverse, ssm)</code>","text":"<p>Extract the (time-)marginals from a Markov sequence.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def markov_marginals(markov_seq: MarkovSeq, *, reverse, ssm):\n    \"\"\"Extract the (time-)marginals from a Markov sequence.\"\"\"\n    _assert_filtering_solution_removed(markov_seq)\n\n    def step(x, cond):\n        extrapolated = ssm.conditional.marginalise(x, cond)\n        return extrapolated, extrapolated\n\n    init, xs = markov_seq.init, markov_seq.conditional\n    _, marg = control_flow.scan(step, init=init, xs=xs, reverse=reverse)\n    return marg\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.markov_sample","title":"<code>markov_sample(key, markov_seq: MarkovSeq, *, reverse, ssm, shape=())</code>","text":"<p>Sample from a Markov sequence.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def markov_sample(key, markov_seq: MarkovSeq, *, reverse, ssm, shape=()):\n    \"\"\"Sample from a Markov sequence.\"\"\"\n    _assert_filtering_solution_removed(markov_seq)\n    # A smoother samples on the grid by sampling i.i.d values\n    # from the terminal RV x_N and the backward noises z_(1:N)\n    # and then combining them backwards as\n    # x_(n-1) = l_n @ x_n + z_n, for n=1,...,N.\n    markov_seq_shape = _sample_shape(markov_seq, ssm=ssm)\n    base_samples = random.normal(key, shape=shape + markov_seq_shape)\n    return _transform_unit_sample(markov_seq, base_samples, reverse=reverse, ssm=ssm)\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.markov_select_terminal","title":"<code>markov_select_terminal(markov_seq: MarkovSeq) -&gt; MarkovSeq</code>","text":"<p>Discard all intermediate filtering solutions from a Markov sequence.</p> <p>This function is useful to convert a smoothing-solution into a Markov sequence that is compatible with sampling or marginalisation.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def markov_select_terminal(markov_seq: MarkovSeq) -&gt; MarkovSeq:\n    \"\"\"Discard all intermediate filtering solutions from a Markov sequence.\n\n    This function is useful to convert a smoothing-solution into a Markov sequence\n    that is compatible with sampling or marginalisation.\n    \"\"\"\n    init = tree_util.tree_map(lambda x: x[-1, ...], markov_seq.init)\n    return MarkovSeq(init, markov_seq.conditional)\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.offgrid_marginals_searchsorted","title":"<code>offgrid_marginals_searchsorted(*, ts, solution, solver)</code>","text":"<p>Compute off-grid marginals on a dense grid via jax.numpy.searchsorted.</p> <p>Warning</p> <p>The elements in ts and the elements in the solution grid must be disjoint. Otherwise, anything can happen and the solution will be incorrect. At the moment, we do not check this.</p> <p>Warning</p> <p>The elements in ts must be strictly in (t0, t1). They must not lie outside the interval, and they must not coincide with the interval boundaries. At the moment, we do not check this.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def offgrid_marginals_searchsorted(*, ts, solution, solver):\n    \"\"\"Compute off-grid marginals on a dense grid via jax.numpy.searchsorted.\n\n    !!! warning\n        The elements in ts and the elements in the solution grid must be disjoint.\n        Otherwise, anything can happen and the solution will be incorrect.\n        At the moment, we do not check this.\n\n    !!! warning\n        The elements in ts must be strictly in (t0, t1).\n        They must not lie outside the interval, and they must not coincide\n        with the interval boundaries.\n        At the moment, we do not check this.\n    \"\"\"\n    offgrid_marginals_vmap = functools.vmap(_offgrid_marginals, in_axes=(0, None, None))\n    return offgrid_marginals_vmap(ts, solution, solver)\n</code></pre>"},{"location":"api_docs/taylor/","title":"taylor","text":"<p>Taylor-expand the solution of an initial value problem (IVP).</p>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_affine","title":"<code>odejet_affine(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Evaluate the Taylor series of an affine differential equation.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop of length <code>num</code>.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_affine(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Evaluate the Taylor series of an affine differential equation.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop of length `num`.\n\n    \"\"\"\n    if num == 0:\n        return inits\n\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_affine(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    fx, jvp_fn = functools.linearize(vf, *inits)\n\n    tmp = fx\n    fx_evaluations = [tmp := jvp_fn(tmp) for _ in range(num - 1)]\n    return [*inits, fx, *fx_evaluations]\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_doubling_unroll","title":"<code>odejet_doubling_unroll(vf: Callable, inits: Sequence[Array], /, num_doublings: int)</code>","text":"<p>Combine Taylor-mode differentiation and Newton's doubling.</p> <p>Warning: highly EXPERIMENTAL feature!</p> <p>Support for Newton's doubling is highly experimental. There is no guarantee that it works correctly. It might be deleted tomorrow and without any deprecation policy.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_doubling_unroll(vf: Callable, inits: Sequence[Array], /, num_doublings: int):\n    \"\"\"Combine Taylor-mode differentiation and Newton's doubling.\n\n    !!! warning \"Warning: highly EXPERIMENTAL feature!\"\n        Support for Newton's doubling is highly experimental.\n        There is no guarantee that it works correctly.\n        It might be deleted tomorrow\n        and without any deprecation policy.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop.\n\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_doubling_unroll(\n            vf_wrapped, inits_flat, num_doublings=num_doublings\n        )\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    (u0,) = inits\n    zeros = np.zeros_like(u0)\n\n    def jet_embedded(*c, degree):\n        \"\"\"Call a modified jet().\n\n        The modifications include:\n        * We merge \"primals\" and \"series\" into a single set of coefficients\n        * We expect and return _normalised_ Taylor coefficients.\n\n        The reason for the latter is that the doubling-recursion\n        simplifies drastically for normalised coefficients\n        (compared to unnormalised coefficients).\n        \"\"\"\n        coeffs_emb = [*c] + [zeros] * degree\n        p, *s = coeffs_emb\n        p_new, s_new = functools.jet(vf, (p,), (s,), is_tcoeff=True)\n        return p_new, *s_new\n\n    taylor_coefficients = [u0]\n    degrees = list(itertools.accumulate(map(lambda s: 2**s, range(num_doublings))))\n    for deg in degrees:\n        jet_embedded_deg = tree_util.Partial(jet_embedded, degree=deg)\n        fx, jvp = functools.linearize(jet_embedded_deg, *taylor_coefficients)\n\n        # Compute the next set of coefficients.\n        # TODO: can we fori_loop() this loop?\n        #  the running variable (cs_padded) should have constant size\n        cs = [(fx[deg - 1] / deg)]\n        cs_padded = cs + [zeros] * (deg - 1)\n        for i, fx_i in enumerate(fx[deg : 2 * deg]):\n            # The Jacobian of the embedded jet is block-banded,\n            # i.e., of the form (for j=3)\n            # (A0, 0, 0; A1, A0, 0; A2, A1, A0; *, *, *; *, *, *; *, *, *)\n            # Thus, by attaching zeros to the current set of coefficients\n            # until the input and output shapes match, we compute\n            # the convolution-like sum of matrix-vector products with\n            # a single call to the JVP function.\n            # Bettencourt et al. (2019;\n            # \"Taylor-mode autodiff for higher-order derivatives in JAX\")\n            # explain details.\n            # i = k - deg\n            linear_combination = jvp(*cs_padded)[i]\n            cs_ = cs_padded[: (i + 1)]\n            cs_ += [(fx_i + linear_combination) / (i + deg + 1)]\n            cs_padded = cs_ + [zeros] * (deg - i - 2)\n\n        # Store all new coefficients\n        taylor_coefficients.extend(cs_padded)\n\n    return _unnormalise(*taylor_coefficients)\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_padded_scan","title":"<code>odejet_padded_scan(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Taylor-expand the solution of an IVP with Taylor-mode differentiation.</p> <p>Other than <code>odejet_unroll()</code>, this function implements the loop via a scan, which comes at the price of padding the loop variable with zeros as appropriate. It is expected to compile more quickly than <code>odejet_unroll()</code>, but may execute more slowly.</p> <p>The differences should be small. Consult the benchmarks if performance is critical.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_padded_scan(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Taylor-expand the solution of an IVP with Taylor-mode differentiation.\n\n    Other than `odejet_unroll()`, this function implements the loop via a scan,\n    which comes at the price of padding the loop variable with zeros as appropriate.\n    It is expected to compile more quickly than `odejet_unroll()`, but may\n    execute more slowly.\n\n    The differences should be small.\n    Consult the benchmarks if performance is critical.\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_padded_scan(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    # Number of positional arguments in f\n    num_arguments = len(inits)\n\n    # Initial Taylor series (u_0, u_1, ..., u_k)\n    primals = vf(*inits)\n    taylor_coeffs = [*inits, primals]\n\n    def body(tcoeffs, _):\n        # Pad the Taylor coefficients in zeros, call jet, and return the solution.\n        # This works, because the $i$th output coefficient of jet()\n        # is independent of the $i+j$th input coefficient\n        # (see also the explanation in odejet_doubling_unroll)\n        series = _subsets(tcoeffs[1:], num_arguments)  # for high-order ODEs\n        p, s_new = functools.jet(vf, primals=inits, series=series)\n\n        # The final values in s_new are nonsensical\n        # (well, they are not; but we don't care about them)\n        # so we remove them\n        tcoeffs = [*inits, p, *s_new[:-1]]\n        return tcoeffs, None\n\n    # Pad the initial Taylor series with zeros\n    num_outputs = num_arguments + num\n    zeros = np.zeros_like(primals)\n    taylor_coeffs = _pad_to_length(taylor_coeffs, length=num_outputs, value=zeros)\n\n    # Early exit for num=1.\n    #  Why? because zero-length scan and disable_jit() don't work together.\n    if num == 1:\n        return taylor_coeffs\n\n    # Compute all coefficients with scan().\n    taylor_coeffs, _ = control_flow.scan(\n        body, init=taylor_coeffs, xs=None, length=num - 1\n    )\n    return taylor_coeffs\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_unroll","title":"<code>odejet_unroll(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Taylor-expand the solution of an IVP with Taylor-mode differentiation.</p> <p>Other than <code>odejet_padded_scan()</code>, this function does not depend on zero-padding the coefficients at the price of unrolling a loop of length <code>num-1</code>. It is expected to compile more slowly than <code>odejet_padded_scan()</code>, but execute more quickly.</p> <p>The differences should be small. Consult the benchmarks if performance is critical.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_unroll(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Taylor-expand the solution of an IVP with Taylor-mode differentiation.\n\n    Other than `odejet_padded_scan()`, this function does not depend on zero-padding\n    the coefficients at the price of unrolling a loop of length `num-1`.\n    It is expected to compile more slowly than `odejet_padded_scan()`,\n    but execute more quickly.\n\n    The differences should be small.\n    Consult the benchmarks if performance is critical.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop.\n\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_unroll(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    # Number of positional arguments in f\n    num_arguments = len(inits)\n\n    # Initial Taylor series (u_0, u_1, ..., u_k)\n    primals = vf(*inits)\n    taylor_coeffs = [*inits, primals]\n\n    for _ in range(num - 1):\n        series = _subsets(taylor_coeffs[1:], num_arguments)  # for high-order ODEs\n        p, s_new = functools.jet(vf, primals=inits, series=series)\n        taylor_coeffs = [*inits, p, *s_new]\n    return taylor_coeffs\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_via_jvp","title":"<code>odejet_via_jvp(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Taylor-expand the solution of an IVP with recursive forward-mode differentiation.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_via_jvp(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Taylor-expand the solution of an IVP with recursive forward-mode differentiation.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop.\n\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_via_jvp(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    g_n, g_0 = vf, vf\n    taylor_coeffs = [*inits, vf(*inits)]\n    for _ in range(num - 1):\n        g_n = _fwd_recursion_iterate(fun_n=g_n, fun_0=g_0)\n        taylor_coeffs = [*taylor_coeffs, g_n(*inits)]\n    return taylor_coeffs\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.runge_kutta_starter","title":"<code>runge_kutta_starter(dt, *, num: int, prior, ssm, atol=1e-12, rtol=1e-10)</code>","text":"<p>Create an estimator that uses a Runge-Kutta starter.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def runge_kutta_starter(dt, *, num: int, prior, ssm, atol=1e-12, rtol=1e-10):\n    \"\"\"Create an estimator that uses a Runge-Kutta starter.\"\"\"\n\n    def starter(vf, initial_values: tuple, /, t):\n        # TODO: higher-order ODEs\n        # TODO: allow flexible \"solve\" method?\n\n        # Assertions and early exits\n\n        if len(initial_values) &gt; 1:\n            msg = \"Higher-order ODEs are not supported at the moment.\"\n            raise ValueError(msg)\n\n        if num == 0:\n            return [*initial_values]\n\n        if num == 1:\n            return [*initial_values, vf(*initial_values, t=t)]\n\n        # Generate data\n\n        k = num + 1  # important: k &gt; num\n        ts = np.linspace(t, t + dt * (k - 1), num=k, endpoint=True)\n        ys = ode.odeint_and_save_at(\n            vf, initial_values, save_at=ts, atol=atol, rtol=rtol\n        )\n\n        # Initial condition\n        scale = ssm.prototypes.output_scale()\n        rv_t0 = ssm.normal.standard(num + 1, scale)\n        estimator = filter_util.fixedpointsmoother_precon(ssm=ssm)\n        conditional_t0 = ssm.conditional.identity(num + 1)\n        init = (rv_t0, conditional_t0)\n\n        # Discretised prior\n        scale = ssm.prototypes.output_scale()\n        prior_vmap = functools.vmap(prior, in_axes=(0, None))\n        ibm_transitions = prior_vmap(np.diff(ts), scale)\n\n        # Generate an observation-model for the QOI\n        # (1e-7 observation noise for nuggets and for reusing existing code)\n        model_fun = functools.vmap(ssm.conditional.to_derivative, in_axes=(None, 0, 0))\n        std = tree_util.tree_map(lambda s: 1e-7 * np.ones((len(s),)), ys)\n        models = model_fun(0, ys, std)\n\n        zeros = np.zeros_like(models.noise.mean)\n\n        # Run the preconditioned fixedpoint smoother\n        (corrected, conditional), _ = filter_util.estimate_fwd(\n            zeros,\n            init=init,\n            prior_transitions=ibm_transitions,\n            observation_model=models,\n            estimator=estimator,\n        )\n        initial = ssm.conditional.marginalise(corrected, conditional)\n        mean = ssm.stats.mean(initial)\n        return ssm.unravel(mean)\n\n    return starter\n</code></pre>"},{"location":"dev_docs/continuous_integration/","title":"Continuous Integration","text":"<p>This guide explains how to install dependencies, run linting and formatting checks, execute tests, and build documentation as part of the continuous integration (CI) process.</p>"},{"location":"dev_docs/continuous_integration/#installation","title":"Installation","text":"<p>After cloning the repository, in the root of the project, and assuming JAX is already installed, do the following: To install all development dependencies, use one or more of the following commands:</p> <pre><code>pip install .[test]  \npip install .[format-and-lint] \npip install .[doc] \n</code></pre> <p>To install everything required for development, you can install all extras at once:</p> <pre><code>pip install .[test,format-and-lint,doc]\n</code></pre>"},{"location":"dev_docs/continuous_integration/#running-checks","title":"Running Checks","text":"<p>The project uses a <code>Makefile</code> to streamline common CI tasks.  You can run the following commands to check code quality and correctness:</p>"},{"location":"dev_docs/continuous_integration/#1-formatting-and-linting","title":"1. Formatting and Linting","text":"<p>To check code formatting and linting rules, run:</p> <pre><code>make format-and-lint\n</code></pre> <p>This will: - Ensure code is properly formatted. - Verify that imports are correctly ordered. - Check for style violations and linting issues. - Enforce documentation conventions.</p>"},{"location":"dev_docs/continuous_integration/#2-running-tests","title":"2. Running Tests","text":"<p>To execute all tests, use:</p> <pre><code>make test\n</code></pre> <p>This will: - Run all tests. - Execute tests in parallel for efficiency.</p>"},{"location":"dev_docs/continuous_integration/#3-running-benchmarks","title":"3. Running Benchmarks","text":"<p>To evaluate the performance of the code, benchmarks can be executed. There are different configurations for benchmarks.</p> <p>To run the full benchmark suite, use:</p> <pre><code>make benchmarks-run\nmake benchmarks-plot-results\n</code></pre> <p>This will: - Execute benchmarking scripts to assess performance. - Plot the results so that the next documentation build displays the results.</p> <p>Benchmarking parameters and configurations can be adjusted in the relevant benchmark scripts, located in the <code>doc/benchmarks/</code> directory.</p> <p>If the goal is not a full benchmark run, but simply a check whether the benchmark scripts execute correctly, use: <pre><code>make benchmarks-run-dry-run\n</code></pre> This is helpful to verify that API changes are reflected in the benchmark code.</p>"},{"location":"dev_docs/continuous_integration/#4-building-documentation","title":"4. Building Documentation","text":"<p>To generate the documentation, use:</p> <pre><code>make doc\n</code></pre> <p>This will: - Sync content in docs/* with the rest of the repo. - Process Jupyter notebooks and Markdown files. - Build the documentation site.</p>"},{"location":"dev_docs/continuous_integration/#5-cleaning-up","title":"5. Cleaning Up","text":"<p>To remove auxiliary files generated during testing or documentation builds, run:</p> <pre><code>make clean\n</code></pre> <p>This removes unnecessary files (eg pytest or mypy caches) to keep the repository clean.</p>"},{"location":"dev_docs/continuous_integration/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>To ensure code quality before committing, the project uses <code>pre-commit</code> hooks. These automatically format, lint, and check files before they are committed to the repository.</p>"},{"location":"dev_docs/continuous_integration/#setting-up-pre-commit","title":"Setting Up Pre-commit","text":"<p>Install <code>pre-commit</code> and set up the hooks by running:</p> <pre><code>pip install pre-commit  # Included in `pip install -e .[format-and-lint]`\npre-commit install\n</code></pre>"},{"location":"dev_docs/continuous_integration/#running-pre-commit-manually","title":"Running Pre-commit Manually","text":"<p>To check all files, not just the staged ones, run:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>To check only the files staged for commit, run:</p> <pre><code>pre-commit run\n</code></pre> <p>This ensures that only properly formatted and linted code is committed.</p>"},{"location":"dev_docs/creating_example_notebook/","title":"Creating a new example/benchmark","text":"<p>Probdiffeq hosts numerous tutorials and benchmarks that demonstrate the library. The differences between examples and benchmarks are minimal: they are all Jupyter notebooks (paired to <code>py:light</code> files via jupytext for version control) and each demonstrates one functionality. Examples show what probdiffeq offers, while benchmarks show how well it performs, often compared to other solver libraries. Each tutorial or benchmark should run in under a minute. New contributions are welcome!</p>"},{"location":"dev_docs/creating_example_notebook/#steps","title":"Steps","text":"<ol> <li>Create the script:    Create a new Jupyter notebook in the appropriate subdirectory of <code>docs/</code>. Example paths include:</li> <li><code>docs/examples_benchmarks/benchmark-name.ipynb</code></li> <li><code>docs/examples_advanced/example-name.ipynb</code>    Choose a meaningful name (e.g., <code>work-precision-hires</code>, <code>demonstrate-calibration</code>). The notebook should run the full example/benchmark and produce its plots. Ensure execution time stays well below one minute to keep CI manageable.</li> </ol> <p>If your example requires external dependencies (e.g., sampling or optimization libraries), place it in <code>examples_advanced</code>.</p> <ol> <li> <p>Sync to py:light:    Install documentation dependencies and pre-commit hooks if you haven't already:    <pre><code>pip install .[doc,format-and-lint]\npre-commit install\n</code></pre>    Link the notebook to a py:light script using jupytext (preferred for version control and formatting):    <pre><code>jupytext --set-formats ipynb,py:light &lt;new-notebook.ipynb&gt;\n</code></pre>    Or link all notebooks at once:    <pre><code>jupytext --set-formats ipynb,py:light docs/examples_*/*.ipynb\n</code></pre>    Notebooks placed correctly according to the directory structure will be included by the previous command.</p> </li> <li> <p>Docs:    Add the new <code>.ipynb</code> file to the documentation navigation in <code>mkdocs.yml</code> under <code>nav:</code>.    Ensure the corresponding script is excluded under <code>mkdocs.yml -&gt; exclude:</code>; if needed, add it there.</p> </li> <li> <p>Makefile:    Add the new example or benchmark to the appropriate Makefile target (e.g., <code>examples-and-benchmarks</code>).</p> </li> <li> <p>Pyproject.toml:    If your example requires external dependencies, list them under the <code>doc</code> optional dependencies in <code>pyproject.toml</code>.</p> </li> <li> <p>Pull request:    Commit the new notebook (the pre-commit hook will handle formatting and linting). Open a pull request and you're done.</p> </li> </ol>"},{"location":"dev_docs/public_api/","title":"Private and public API","text":"<p>All public functions and classes that are in the online documentation  are considered public API. At the moment, this affects the following:</p> <ul> <li><code>ivpsolve.py</code></li> <li><code>adaptive.py</code></li> <li><code>taylor/*</code></li> <li><code>ivpsolvers.py</code></li> <li><code>stats.py</code></li> <li><code>impl.impl.select()</code></li> </ul> <p>Exceptions to this rule are all functions and class that are  marked as <code>warning: highly experimental</code>, e.g., <code>ivpsolve.solve_adaptive_save_at</code>.</p> <p>Everything else (e.g. <code>backend</code>, <code>util</code>, <code>impl</code>) is not public and breaking changes here will not necessarily increase the version.</p>"},{"location":"examples_advanced/equinox_while_loop/","title":"Equinox's while-loops","text":"In\u00a0[1]: Copied! <pre>\"\"\"Use Equinox's while loop to compute gradients of `simulate_terminal_values`.\"\"\"\n\nimport equinox\nimport jax\nimport jax.numpy as jnp\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\nfrom probdiffeq.backend import control_flow\n</pre> \"\"\"Use Equinox's while loop to compute gradients of `simulate_terminal_values`.\"\"\"  import equinox import jax import jax.numpy as jnp  from probdiffeq import ivpsolve, ivpsolvers, taylor from probdiffeq.backend import control_flow  <p>Overwrite the while-loop (via a context manager):</p> In\u00a0[2]: Copied! <pre>def while_loop_func(*a, **kw):\n    \"\"\"Evaluate a bounded while loop.\"\"\"\n    return equinox.internal.while_loop(*a, **kw, kind=\"bounded\", max_steps=100)\n\n\ncontext_compute_gradient = control_flow.context_overwrite_while_loop(while_loop_func)\n</pre> def while_loop_func(*a, **kw):     \"\"\"Evaluate a bounded while loop.\"\"\"     return equinox.internal.while_loop(*a, **kw, kind=\"bounded\", max_steps=100)   context_compute_gradient = control_flow.context_overwrite_while_loop(while_loop_func) <p>The rest is the similar to the \"easy example\" in the quickstart, except for simulating adaptively and computing the value and the gradient (which is impossible without the specialised while-loop implementation).</p> In\u00a0[3]: Copied! <pre>def solution_routine():\n    \"\"\"Construct a parameter-to-solution function and an initial value.\"\"\"\n\n    def vf(y, *, t):  # noqa: ARG001\n        \"\"\"Evaluate the vector field.\"\"\"\n        return 0.5 * y * (1 - y)\n\n    t0, t1 = 0.0, 1.0\n    u0 = jnp.asarray([0.1])\n\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"isotropic\")\n    ts0 = ivpsolvers.correction_ts0(vf, ode_order=1, ssm=ssm)\n\n    strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n\n    def simulate(init_val):\n        \"\"\"Evaluate the parameter-to-solution function.\"\"\"\n        sol = ivpsolve.solve_adaptive_terminal_values(\n            init_val, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n\n        # Any scalar function of the IVP solution would do\n        return jnp.dot(sol.u[0], sol.u[0])\n\n    return simulate, init\n</pre> def solution_routine():     \"\"\"Construct a parameter-to-solution function and an initial value.\"\"\"      def vf(y, *, t):  # noqa: ARG001         \"\"\"Evaluate the vector field.\"\"\"         return 0.5 * y * (1 - y)      t0, t1 = 0.0, 1.0     u0 = jnp.asarray([0.1])      tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"isotropic\")     ts0 = ivpsolvers.correction_ts0(vf, ode_order=1, ssm=ssm)      strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)      def simulate(init_val):         \"\"\"Evaluate the parameter-to-solution function.\"\"\"         sol = ivpsolve.solve_adaptive_terminal_values(             init_val, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm         )          # Any scalar function of the IVP solution would do         return jnp.dot(sol.u[0], sol.u[0])      return simulate, init In\u00a0[4]: Copied! <pre>try:\n    solve, x = solution_routine()\n    solution, gradient = jax.value_and_grad(solve)(x)\nexcept ValueError as err:\n    print(f\"Caught error:\\n\\t {err}\")\n</pre> try:     solve, x = solution_routine()     solution, gradient = jax.value_and_grad(solve)(x) except ValueError as err:     print(f\"Caught error:\\n\\t {err}\") <pre>Caught error:\n\t Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop.\n</pre> In\u00a0[5]: Copied! <pre>with context_compute_gradient:\n    # Construct the solution routine inside the context\n    solve, x = solution_routine()\n\n    # Compute gradients\n    solution, gradient = jax.value_and_grad(solve)(x)\n\n    print(solution)\n    print(gradient)\n</pre> with context_compute_gradient:     # Construct the solution routine inside the context     solve, x = solution_routine()      # Compute gradients     solution, gradient = jax.value_and_grad(solve)(x)      print(solution)     print(gradient) <pre>0.023939388\nNormal(mean=Array([[0.4424412 ],\n       [0.01854868]], dtype=float32), cholesky=Array([[0., 0.],\n       [0., 0.]], dtype=float32))\n</pre>"},{"location":"examples_advanced/equinox_while_loop/#equinoxs-while-loops","title":"Equinox's while-loops\u00b6","text":"<p>Use Equinox's bounded while loop to enable reverse-mode differentiation of adaptive IVP solvers.</p>"},{"location":"examples_advanced/neural_ode/","title":"Diffusion tempering &amp; NODEs","text":"In\u00a0[1]: Copied! <pre>\"\"\"Train a neural ODE with ProbDiffEq and Optax using diffusion tempering.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats\n\n\ndef main(num_data=100, epochs=1_000, print_every=100, hidden=(20,), lr=0.2):\n    \"\"\"Train a neural ODE using diffusion tempering.\"\"\"\n    # Create some data and construct a neural ODE\n    grid = jnp.linspace(0, 1, num=num_data)\n    data = jnp.sin(2.5 * jnp.pi * grid) * jnp.pi * grid\n    stdev = 1e-1\n    output_scale = 1e2\n    vf, u0, (t0, t1), f_args = vf_neural_ode(hidden=hidden, t0=0.0, t1=1)\n\n    # Create a loss (this is where probabilistic numerics enters!)\n    loss = loss_log_marginal_likelihood(vf=vf, t0=t0)\n    loss0, info0 = loss(\n        f_args, u0=u0, grid=grid, data=data, stdev=stdev, output_scale=output_scale\n    )\n\n    # Plot the data and the initial guess\n    plt.title(f\"Initial estimate | Loss: {loss0:.2f}\")\n    plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")\n    plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Estimate\", color=\"C1\")\n    plt.legend()\n    plt.show()\n\n    # Construct an optimiser\n    optim = optax.adam(lr)\n    train_step = train_step_optax(optim, loss=loss)\n\n    # Train the model\n    state = optim.init(f_args)\n    print(\"Loss after...\")\n    for i in range(epochs):\n        (f_args, state), info = train_step(\n            f_args,\n            state,\n            u0=u0,\n            grid=grid,\n            data=data,\n            stdev=stdev,\n            output_scale=output_scale,\n        )\n\n        # Print progressbar\n        if i % print_every == print_every - 1:\n            print(f\"...{(i + 1)} epochs: loss={info['loss']:.3e}\")\n\n        # Diffusion tempering: https://arxiv.org/abs/2402.12231\n        # To all users: Adjust this tempering and\n        # see how it affects parameter estimation.\n        if i % 100 == 0:\n            output_scale /= 10.0\n\n    # Plot the results\n    plt.title(f\"Final estimate | Loss: {info['loss']:.2f}\")\n    plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")\n    plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Initial estimate\", color=\"C1\")\n    plt.plot(grid, info[\"sol\"].u[0], \"-\", label=\"Final estimate\", color=\"C2\")\n    plt.legend()\n    plt.show()\n\n\ndef vf_neural_ode(*, hidden: tuple, t0: float, t1: float):\n    \"\"\"Build a neural ODE.\"\"\"\n    f_args, mlp = model_mlp(hidden=hidden, shape_in=(2,), shape_out=(1,))\n    u0 = jnp.asarray([0.0])\n\n    @jax.jit\n    def vf(y, *, t, p):\n        \"\"\"Evaluate the neural ODE vector field.\"\"\"\n        y_and_t = jnp.concatenate([y, t[None]])\n        return mlp(p, y_and_t)\n\n    return vf, (u0,), (t0, t1), f_args\n\n\ndef model_mlp(\n    *, hidden: tuple, shape_in: tuple = (), shape_out: tuple = (), activation=jnp.tanh\n):\n    \"\"\"Construct an MLP.\"\"\"\n    assert len(shape_in) &lt;= 1\n    assert len(shape_out) &lt;= 1\n\n    shape_prev = shape_in\n    weights = []\n    for h in hidden:\n        W = jnp.empty((h, *shape_prev))\n        b = jnp.empty((h,))\n        shape_prev = (h,)\n        weights.append((W, b))\n\n    W = jnp.empty((*shape_out, *shape_prev))\n    b = jnp.empty(shape_out)\n    weights.append((W, b))\n\n    p_flat, unravel = jax.flatten_util.ravel_pytree(weights)\n\n    def fwd(w, x):\n        for A, b in w[:-1]:\n            x = jnp.dot(A, x) + b\n            x = activation(x)\n\n        A, b = w[-1]\n        return jnp.dot(A, x) + b\n\n    key = jax.random.PRNGKey(1)\n    p_init = jax.random.normal(key, shape=p_flat.shape, dtype=p_flat.dtype)\n    return unravel(p_init), fwd\n\n\ndef loss_log_marginal_likelihood(vf, *, t0):\n    \"\"\"Build a loss function from an ODE problem.\"\"\"\n\n    @jax.jit\n    def loss(\n        p: jax.Array,\n        *,\n        u0: tuple,\n        grid: jax.Array,\n        data: jax.Array,\n        stdev: jax.Array,\n        output_scale: jax.Array,\n    ):\n        \"\"\"Loss function: log-marginal likelihood of the data.\"\"\"\n        # Build a solver\n        tcoeffs = (*u0, vf(*u0, t=t0, p=p))\n        init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n            tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n        )\n        ts0 = ivpsolvers.correction_ts0(lambda *a, **kw: vf(*a, **kw, p=p), ssm=ssm)\n        strategy = ivpsolvers.strategy_smoother(ssm=ssm)\n        solver_ts0 = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n\n        # Solve\n        sol = ivpsolve.solve_fixed_grid(init, grid=grid, solver=solver_ts0, ssm=ssm)\n\n        # Evaluate loss\n        marginal_likelihood = stats.log_marginal_likelihood(\n            data[:, None],\n            standard_deviation=jnp.ones_like(grid) * stdev,\n            posterior=sol.posterior,\n            ssm=sol.ssm,\n        )\n        return -1 * marginal_likelihood, {\"sol\": sol}\n\n    return loss\n\n\ndef train_step_optax(optimizer, loss):\n    \"\"\"Implement a training step using Optax.\"\"\"\n\n    @jax.jit\n    def update(params, opt_state, **loss_kwargs):\n        \"\"\"Update the optimiser state.\"\"\"\n        value_and_grad = jax.value_and_grad(loss, argnums=0, has_aux=True)\n        (value, info), grads = value_and_grad(params, **loss_kwargs)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n\n        info[\"loss\"] = value\n        return (params, opt_state), info\n\n    return update\n\n\nif __name__ == \"__main__\":\n    main()\n</pre> \"\"\"Train a neural ODE with ProbDiffEq and Optax using diffusion tempering.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax  from probdiffeq import ivpsolve, ivpsolvers, stats   def main(num_data=100, epochs=1_000, print_every=100, hidden=(20,), lr=0.2):     \"\"\"Train a neural ODE using diffusion tempering.\"\"\"     # Create some data and construct a neural ODE     grid = jnp.linspace(0, 1, num=num_data)     data = jnp.sin(2.5 * jnp.pi * grid) * jnp.pi * grid     stdev = 1e-1     output_scale = 1e2     vf, u0, (t0, t1), f_args = vf_neural_ode(hidden=hidden, t0=0.0, t1=1)      # Create a loss (this is where probabilistic numerics enters!)     loss = loss_log_marginal_likelihood(vf=vf, t0=t0)     loss0, info0 = loss(         f_args, u0=u0, grid=grid, data=data, stdev=stdev, output_scale=output_scale     )      # Plot the data and the initial guess     plt.title(f\"Initial estimate | Loss: {loss0:.2f}\")     plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")     plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Estimate\", color=\"C1\")     plt.legend()     plt.show()      # Construct an optimiser     optim = optax.adam(lr)     train_step = train_step_optax(optim, loss=loss)      # Train the model     state = optim.init(f_args)     print(\"Loss after...\")     for i in range(epochs):         (f_args, state), info = train_step(             f_args,             state,             u0=u0,             grid=grid,             data=data,             stdev=stdev,             output_scale=output_scale,         )          # Print progressbar         if i % print_every == print_every - 1:             print(f\"...{(i + 1)} epochs: loss={info['loss']:.3e}\")          # Diffusion tempering: https://arxiv.org/abs/2402.12231         # To all users: Adjust this tempering and         # see how it affects parameter estimation.         if i % 100 == 0:             output_scale /= 10.0      # Plot the results     plt.title(f\"Final estimate | Loss: {info['loss']:.2f}\")     plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")     plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Initial estimate\", color=\"C1\")     plt.plot(grid, info[\"sol\"].u[0], \"-\", label=\"Final estimate\", color=\"C2\")     plt.legend()     plt.show()   def vf_neural_ode(*, hidden: tuple, t0: float, t1: float):     \"\"\"Build a neural ODE.\"\"\"     f_args, mlp = model_mlp(hidden=hidden, shape_in=(2,), shape_out=(1,))     u0 = jnp.asarray([0.0])      @jax.jit     def vf(y, *, t, p):         \"\"\"Evaluate the neural ODE vector field.\"\"\"         y_and_t = jnp.concatenate([y, t[None]])         return mlp(p, y_and_t)      return vf, (u0,), (t0, t1), f_args   def model_mlp(     *, hidden: tuple, shape_in: tuple = (), shape_out: tuple = (), activation=jnp.tanh ):     \"\"\"Construct an MLP.\"\"\"     assert len(shape_in) &lt;= 1     assert len(shape_out) &lt;= 1      shape_prev = shape_in     weights = []     for h in hidden:         W = jnp.empty((h, *shape_prev))         b = jnp.empty((h,))         shape_prev = (h,)         weights.append((W, b))      W = jnp.empty((*shape_out, *shape_prev))     b = jnp.empty(shape_out)     weights.append((W, b))      p_flat, unravel = jax.flatten_util.ravel_pytree(weights)      def fwd(w, x):         for A, b in w[:-1]:             x = jnp.dot(A, x) + b             x = activation(x)          A, b = w[-1]         return jnp.dot(A, x) + b      key = jax.random.PRNGKey(1)     p_init = jax.random.normal(key, shape=p_flat.shape, dtype=p_flat.dtype)     return unravel(p_init), fwd   def loss_log_marginal_likelihood(vf, *, t0):     \"\"\"Build a loss function from an ODE problem.\"\"\"      @jax.jit     def loss(         p: jax.Array,         *,         u0: tuple,         grid: jax.Array,         data: jax.Array,         stdev: jax.Array,         output_scale: jax.Array,     ):         \"\"\"Loss function: log-marginal likelihood of the data.\"\"\"         # Build a solver         tcoeffs = (*u0, vf(*u0, t=t0, p=p))         init, ibm, ssm = ivpsolvers.prior_wiener_integrated(             tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"         )         ts0 = ivpsolvers.correction_ts0(lambda *a, **kw: vf(*a, **kw, p=p), ssm=ssm)         strategy = ivpsolvers.strategy_smoother(ssm=ssm)         solver_ts0 = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)          # Solve         sol = ivpsolve.solve_fixed_grid(init, grid=grid, solver=solver_ts0, ssm=ssm)          # Evaluate loss         marginal_likelihood = stats.log_marginal_likelihood(             data[:, None],             standard_deviation=jnp.ones_like(grid) * stdev,             posterior=sol.posterior,             ssm=sol.ssm,         )         return -1 * marginal_likelihood, {\"sol\": sol}      return loss   def train_step_optax(optimizer, loss):     \"\"\"Implement a training step using Optax.\"\"\"      @jax.jit     def update(params, opt_state, **loss_kwargs):         \"\"\"Update the optimiser state.\"\"\"         value_and_grad = jax.value_and_grad(loss, argnums=0, has_aux=True)         (value, info), grads = value_and_grad(params, **loss_kwargs)         updates, opt_state = optimizer.update(grads, opt_state)         params = optax.apply_updates(params, updates)          info[\"loss\"] = value         return (params, opt_state), info      return update   if __name__ == \"__main__\":     main() <pre>Loss after...\n</pre> <pre>...100 epochs: loss=3.024e+01\n...200 epochs: loss=3.369e+01\n</pre> <pre>...300 epochs: loss=1.849e+01\n...400 epochs: loss=-7.341e-02\n</pre> <pre>...500 epochs: loss=3.565e+00\n...600 epochs: loss=-1.072e+00\n</pre> <pre>...700 epochs: loss=-9.700e-01\n...800 epochs: loss=1.050e-01\n</pre> <pre>...900 epochs: loss=-1.253e+00\n...1000 epochs: loss=-1.281e+00\n</pre>"},{"location":"examples_advanced/neural_ode/#diffusion-tempering-nodes","title":"Diffusion tempering &amp; NODEs\u00b6","text":""},{"location":"examples_advanced/parameter_estimation_blackjax/","title":"Parameter estimation (BlackJAX)","text":"In\u00a0[1]: Copied! <pre>\"\"\"Estimate ODE paramaters with ProbDiffEq and BlackJAX.\"\"\"\n\nimport functools\n\nimport blackjax\nimport jax\nimport jax.experimental.ode\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n</pre> \"\"\"Estimate ODE paramaters with ProbDiffEq and BlackJAX.\"\"\"  import functools  import blackjax import jax import jax.experimental.ode import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor In\u00a0[2]: Copied! <pre># IVP examples in JAX\nif not backend.has_been_selected:\n    backend.select(\"jax\")\n</pre>  # IVP examples in JAX if not backend.has_been_selected:     backend.select(\"jax\")  In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.lotka_volterra()\n\n\n@jax.jit\ndef vf(y, *, t):  # noqa: ARG001\n    \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"\n    return f(y, *f_args)\n\n\ntheta_true = u0 + 0.5 * jnp.flip(u0)\ntheta_guess = u0  # initial guess\n</pre> f, u0, (t0, t1), f_args = ivps.lotka_volterra()   @jax.jit def vf(y, *, t):  # noqa: ARG001     \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"     return f(y, *f_args)   theta_true = u0 + 0.5 * jnp.flip(u0) theta_guess = u0  # initial guess In\u00a0[4]: Copied! <pre>def plot_solution(t, u, *, ax, marker=\".\", **plotting_kwargs):\n    \"\"\"Plot the IVP solution.\"\"\"\n    for d in [0, 1]:\n        ax.plot(t, u[:, d], marker=\"None\", **plotting_kwargs)\n        ax.plot(t[0], u[0, d], marker=marker, **plotting_kwargs)\n        ax.plot(t[-1], u[-1, d], marker=marker, **plotting_kwargs)\n    return ax\n\n\n@jax.jit\ndef solve_fixed(theta, *, ts):\n    \"\"\"Evaluate the parameter-to-solution map, solving on a fixed grid.\"\"\"\n    # Create a probabilistic solver\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)\n    output_scale = 10.0\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n        tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n    )\n    ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_filter(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)\n\n\n@jax.jit\ndef solve_adaptive(theta, *, save_at):\n    \"\"\"Evaluate the parameter-to-solution map, solving on an adaptive grid.\"\"\"\n    # Create a probabilistic solver\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)\n    output_scale = 10.0\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n        tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n    )\n    ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_filter(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n    return ivpsolve.solve_adaptive_save_at(\n        init, save_at=save_at, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm\n    )\n\n\nsave_at = jnp.linspace(t0, t1, num=250, endpoint=True)\nsolve_save_at = functools.partial(solve_adaptive, save_at=save_at)\n</pre> def plot_solution(t, u, *, ax, marker=\".\", **plotting_kwargs):     \"\"\"Plot the IVP solution.\"\"\"     for d in [0, 1]:         ax.plot(t, u[:, d], marker=\"None\", **plotting_kwargs)         ax.plot(t[0], u[0, d], marker=marker, **plotting_kwargs)         ax.plot(t[-1], u[-1, d], marker=marker, **plotting_kwargs)     return ax   @jax.jit def solve_fixed(theta, *, ts):     \"\"\"Evaluate the parameter-to-solution map, solving on a fixed grid.\"\"\"     # Create a probabilistic solver     tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)     output_scale = 10.0     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(         tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"     )     ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)     strategy = ivpsolvers.strategy_filter(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)   @jax.jit def solve_adaptive(theta, *, save_at):     \"\"\"Evaluate the parameter-to-solution map, solving on an adaptive grid.\"\"\"     # Create a probabilistic solver     tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)     output_scale = 10.0     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(         tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"     )     ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)     strategy = ivpsolvers.strategy_filter(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)     return ivpsolve.solve_adaptive_save_at(         init, save_at=save_at, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm     )   save_at = jnp.linspace(t0, t1, num=250, endpoint=True) solve_save_at = functools.partial(solve_adaptive, save_at=save_at) In\u00a0[5]: Copied! <pre># Visualise the initial guess and the data\n\nfig, ax = plt.subplots(figsize=(5, 3))\n\ndata_kwargs = {\"alpha\": 0.5, \"color\": \"gray\"}\nax.annotate(\"Data\", (13.0, 30.0), **data_kwargs)\nsol = solve_save_at(theta_true)\nax = plot_solution(sol.t, sol.u[0], ax=ax, **data_kwargs)\n\nguess_kwargs = {\"color\": \"C3\"}\nax.annotate(\"Initial guess\", (7.5, 20.0), **guess_kwargs)\nsol = solve_save_at(theta_guess)\nax = plot_solution(sol.t, sol.u[0], ax=ax, **guess_kwargs)\nplt.show()\n</pre> # Visualise the initial guess and the data  fig, ax = plt.subplots(figsize=(5, 3))  data_kwargs = {\"alpha\": 0.5, \"color\": \"gray\"} ax.annotate(\"Data\", (13.0, 30.0), **data_kwargs) sol = solve_save_at(theta_true) ax = plot_solution(sol.t, sol.u[0], ax=ax, **data_kwargs)  guess_kwargs = {\"color\": \"C3\"} ax.annotate(\"Initial guess\", (7.5, 20.0), **guess_kwargs) sol = solve_save_at(theta_guess) ax = plot_solution(sol.t, sol.u[0], ax=ax, **guess_kwargs) plt.show() In\u00a0[6]: Copied! <pre>mean = theta_guess\ncov = jnp.eye(2) * 30  # fairly uninformed prior\n\n\n@jax.jit\ndef logposterior_fn(theta, *, data, ts, obs_stdev=0.1):\n    \"\"\"Evaluate the logposterior-function of the data.\"\"\"\n    solution = solve_fixed(theta, ts=ts)\n    y_T = jax.tree.map(lambda s: s[-1], solution.posterior)\n    logpdf_data = stats.log_marginal_likelihood_terminal_values(\n        data, standard_deviation=obs_stdev, posterior=y_T, ssm=solution.ssm\n    )\n    logpdf_prior = jax.scipy.stats.multivariate_normal.logpdf(theta, mean=mean, cov=cov)\n    return logpdf_data + logpdf_prior\n\n\n# Fixed steps for reverse-mode differentiability:\n\n\nts = jnp.linspace(t0, t1, endpoint=True, num=100)\ndata = solve_fixed(theta_true, ts=ts).u[0][-1]\n\nlog_M = functools.partial(logposterior_fn, data=data, ts=ts)\n</pre> mean = theta_guess cov = jnp.eye(2) * 30  # fairly uninformed prior   @jax.jit def logposterior_fn(theta, *, data, ts, obs_stdev=0.1):     \"\"\"Evaluate the logposterior-function of the data.\"\"\"     solution = solve_fixed(theta, ts=ts)     y_T = jax.tree.map(lambda s: s[-1], solution.posterior)     logpdf_data = stats.log_marginal_likelihood_terminal_values(         data, standard_deviation=obs_stdev, posterior=y_T, ssm=solution.ssm     )     logpdf_prior = jax.scipy.stats.multivariate_normal.logpdf(theta, mean=mean, cov=cov)     return logpdf_data + logpdf_prior   # Fixed steps for reverse-mode differentiability:   ts = jnp.linspace(t0, t1, endpoint=True, num=100) data = solve_fixed(theta_true, ts=ts).u[0][-1]  log_M = functools.partial(logposterior_fn, data=data, ts=ts) In\u00a0[7]: Copied! <pre>print(jnp.exp(log_M(theta_true)), \"&gt;=\", jnp.exp(log_M(theta_guess)), \"?\")\n</pre> print(jnp.exp(log_M(theta_true)), \"&gt;=\", jnp.exp(log_M(theta_guess)), \"?\") <pre>0.002049896 &gt;= 0.0 ?\n</pre> <p>Set up a sampler.</p> In\u00a0[8]: Copied! <pre>@functools.partial(jax.jit, static_argnames=[\"kernel\", \"num_samples\"])\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    \"\"\"Run BlackJAX' inference loop.\"\"\"\n\n    def one_step(state, rng_key):\n        state, _ = kernel.step(rng_key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n\n    return states\n</pre> @functools.partial(jax.jit, static_argnames=[\"kernel\", \"num_samples\"]) def inference_loop(rng_key, kernel, initial_state, num_samples):     \"\"\"Run BlackJAX' inference loop.\"\"\"      def one_step(state, rng_key):         state, _ = kernel.step(rng_key, state)         return state, state      keys = jax.random.split(rng_key, num_samples)     _, states = jax.lax.scan(one_step, initial_state, keys)      return states <p>Initialise the sampler, warm it up, and run the inference loop.</p> In\u00a0[9]: Copied! <pre>initial_position = theta_guess\nrng_key = jax.random.PRNGKey(0)\n</pre> initial_position = theta_guess rng_key = jax.random.PRNGKey(0) In\u00a0[10]: Copied! <pre># WARMUP\nwarmup = blackjax.window_adaptation(blackjax.nuts, log_M, progress_bar=True)\n\nwarmup_results, _ = warmup.run(rng_key, initial_position, num_steps=200)\n\ninitial_state = warmup_results.state\nstep_size = warmup_results.parameters[\"step_size\"]\ninverse_mass_matrix = warmup_results.parameters[\"inverse_mass_matrix\"]\nnuts_kernel = blackjax.nuts(\n    logdensity_fn=log_M, step_size=step_size, inverse_mass_matrix=inverse_mass_matrix\n)\n</pre> # WARMUP warmup = blackjax.window_adaptation(blackjax.nuts, log_M, progress_bar=True)  warmup_results, _ = warmup.run(rng_key, initial_position, num_steps=200)  initial_state = warmup_results.state step_size = warmup_results.parameters[\"step_size\"] inverse_mass_matrix = warmup_results.parameters[\"inverse_mass_matrix\"] nuts_kernel = blackjax.nuts(     logdensity_fn=log_M, step_size=step_size, inverse_mass_matrix=inverse_mass_matrix ) <pre>Running window adaptation\n</pre>        100.00% [200/200 00:00&lt;?]      In\u00a0[11]: Copied! <pre># INFERENCE LOOP\nrng_key, _ = jax.random.split(rng_key, 2)\nstates = inference_loop(\n    rng_key, kernel=nuts_kernel, initial_state=initial_state, num_samples=150\n)\n</pre> # INFERENCE LOOP rng_key, _ = jax.random.split(rng_key, 2) states = inference_loop(     rng_key, kernel=nuts_kernel, initial_state=initial_state, num_samples=150 ) In\u00a0[12]: Copied! <pre>solution_samples = jax.vmap(solve_save_at)(states.position)\n</pre> solution_samples = jax.vmap(solve_save_at)(states.position) In\u00a0[13]: Copied! <pre># Visualise the initial guess and the data\n\nfig, ax = plt.subplots()\n\nsample_kwargs = {\"color\": \"C0\"}\nax.annotate(\"Samples\", (2.75, 31.0), **sample_kwargs)\nfor ts, us in zip(solution_samples.t, solution_samples.u[0]):\n    ax = plot_solution(ts, us, ax=ax, linewidth=0.1, alpha=0.75, **sample_kwargs)\n\ndata_kwargs = {\"color\": \"gray\"}\nax.annotate(\"Data\", (18.25, 40.0), **data_kwargs)\nsol = solve_save_at(theta_true)\nax = plot_solution(sol.t, sol.u[0], ax=ax, linewidth=4, alpha=0.5, **data_kwargs)\n\nguess_kwargs = {\"color\": \"gray\"}\nax.annotate(\"Initial guess\", (6.0, 12.0), **guess_kwargs)\nsol = solve_save_at(theta_guess)\nax = plot_solution(\n    sol.t, sol.u[0], ax=ax, linestyle=\"dashed\", alpha=0.75, **guess_kwargs\n)\nplt.show()\n</pre> # Visualise the initial guess and the data  fig, ax = plt.subplots()  sample_kwargs = {\"color\": \"C0\"} ax.annotate(\"Samples\", (2.75, 31.0), **sample_kwargs) for ts, us in zip(solution_samples.t, solution_samples.u[0]):     ax = plot_solution(ts, us, ax=ax, linewidth=0.1, alpha=0.75, **sample_kwargs)  data_kwargs = {\"color\": \"gray\"} ax.annotate(\"Data\", (18.25, 40.0), **data_kwargs) sol = solve_save_at(theta_true) ax = plot_solution(sol.t, sol.u[0], ax=ax, linewidth=4, alpha=0.5, **data_kwargs)  guess_kwargs = {\"color\": \"gray\"} ax.annotate(\"Initial guess\", (6.0, 12.0), **guess_kwargs) sol = solve_save_at(theta_guess) ax = plot_solution(     sol.t, sol.u[0], ax=ax, linestyle=\"dashed\", alpha=0.75, **guess_kwargs ) plt.show() <p>The samples cover a perhaps surpringly large range of potential initial conditions, but lead to the \"correct\" data.</p> <p>In parameter space, this is what it looks like:</p> In\u00a0[14]: Copied! <pre>plt.title(\"Posterior samples (parameter space)\")\nplt.plot(states.position[:, 0], states.position[:, 1], \"o\", alpha=0.5, markersize=4)\nplt.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8)\nplt.plot(theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8)\nplt.legend()\nplt.show()\n</pre> plt.title(\"Posterior samples (parameter space)\") plt.plot(states.position[:, 0], states.position[:, 1], \"o\", alpha=0.5, markersize=4) plt.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8) plt.plot(theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8) plt.legend() plt.show() <p>Let's add the value of $M$ to the plot to see whether the sampler covers the entire region of interest.</p> In\u00a0[15]: Copied! <pre>xlim = 14, jnp.amax(states.position[:, 0]) + 0.5\nylim = 14, jnp.amax(states.position[:, 1]) + 0.5\n\nxs = jnp.linspace(*xlim, endpoint=True, num=300)\nys = jnp.linspace(*ylim, endpoint=True, num=300)\nXs, Ys = jnp.meshgrid(xs, ys)\n\nThetas = jnp.stack((Xs, Ys))\nlog_M_vmapped_x = jax.vmap(log_M, in_axes=-1, out_axes=-1)\nlog_M_vmapped = jax.vmap(log_M_vmapped_x, in_axes=-1, out_axes=-1)\nZs = log_M_vmapped(Thetas)\n</pre> xlim = 14, jnp.amax(states.position[:, 0]) + 0.5 ylim = 14, jnp.amax(states.position[:, 1]) + 0.5  xs = jnp.linspace(*xlim, endpoint=True, num=300) ys = jnp.linspace(*ylim, endpoint=True, num=300) Xs, Ys = jnp.meshgrid(xs, ys)  Thetas = jnp.stack((Xs, Ys)) log_M_vmapped_x = jax.vmap(log_M, in_axes=-1, out_axes=-1) log_M_vmapped = jax.vmap(log_M_vmapped_x, in_axes=-1, out_axes=-1) Zs = log_M_vmapped(Thetas) In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 3))\n\nax_samples, ax_heatmap = ax\n\nfig.suptitle(\"Posterior samples (parameter space)\")\nax_samples.plot(\n    states.position[:, 0], states.position[:, 1], \".\", alpha=0.5, markersize=4\n)\nax_samples.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8)\nax_samples.plot(\n    theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8\n)\nax_samples.legend()\nim = ax_heatmap.contourf(Xs, Ys, jnp.exp(Zs), cmap=\"cividis\", alpha=0.8)\nplt.colorbar(im)\nplt.show()\n</pre> fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 3))  ax_samples, ax_heatmap = ax  fig.suptitle(\"Posterior samples (parameter space)\") ax_samples.plot(     states.position[:, 0], states.position[:, 1], \".\", alpha=0.5, markersize=4 ) ax_samples.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8) ax_samples.plot(     theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8 ) ax_samples.legend() im = ax_heatmap.contourf(Xs, Ys, jnp.exp(Zs), cmap=\"cividis\", alpha=0.8) plt.colorbar(im) plt.show() <p>Looks great!</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#parameter-estimation-blackjax","title":"Parameter estimation (BlackJAX)\u00b6","text":"<p>This tutorial explains how to estimate unknown parameters of initial value problems (IVPs) using Markov Chain Monte Carlo (MCMC) methods as provided by BlackJAX.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#tldr","title":"TL;DR\u00b6","text":"<p>Compute log-posterior of IVP parameters given observations of the IVP solution with ProbDiffEq. Sample from this posterior using BlackJAX. Evaluating the log-likelihood of the data is described in this paper. Based on this log-likelihood, sampling from the log-posterior is as done in this paper.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#technical-setup","title":"Technical setup\u00b6","text":"<p>Let $f$ be a known vector field. In this example, we use the Lotka-Volterra model. Consider an ordinary differential equation</p> <p>$$ \\dot y(t) = f(y(t)), \\quad 0 \\leq t \\leq T $$</p> <p>subject to an unknown initial condition $y(0) = \\theta$. Recall from the previous tutorials that the probabilistic IVP solution is an approximation of the posterior distribution</p> <p>$$ p\\left(y ~|~ [\\dot y(t_n) = f(y(t_n))]_{n=0}^N, y(0) = \\theta\\right) $$</p> <p>for a Gaussian prior over $y$ and a pre-determined or adaptively selected grid $t_0, ..., t_N$.</p> <p>We don't know the initial condition of the IVP, but assume that we have noisy observations of the IVP soution $y$ at the terminal time $T$ of the integration problem,</p> <p>$$ p(\\text{data}~|~ y(T)) = N(y(T), \\sigma^2 I) $$</p> <p>for some $\\sigma &gt; 0$. We can use these observations to reconstruct $\\theta$, for example by sampling from $p(\\text{data}~|~\\theta)$ (which is a function of $\\theta$).</p> <p>Now, one way of evaluating $p(\\text{data} ~|~ \\theta)$ is to use any numerical solver, for example a Runge-Kutta method, to approximate $y(T)$ from $\\theta$ and evaluate $N(y(T), \\sigma^2 I)$. But this ignores a few crucial concepts (e.g., the numerical error of the approximation; we refer to the references linked above). Instead, we can use a probabilistic solver instead of \"any\" numerical solver and build a more comprehensive model:</p> <p>We can combine probabilistic IVP solvers with MCMC methods to estimate $\\theta$ from $\\text{data}$ in a way that quantifies numerical approximation errors (and other model mismatches). To do so, we approximate the distribution of the IVP solution given the parameter $p(y(T) \\mid \\theta)$ and evaluate the marginal distribution of $N(y(T), \\sigma^2I)$ given the probabilistic IVP solution. More formally, we use ProbDiffEq to evaluate the density of the unnormalised posterior</p> <p>$$ M(\\theta) := p(\\theta \\mid \\text{data})\\propto p(\\text{data} \\mid \\theta)p(\\theta) $$</p> <p>where \"$\\propto$\" means \"proportional to\" and the likelihood stems from the IVP solution</p> <p>$$ p(\\text{data} \\mid \\theta) = \\int p(\\text{data} \\mid y(T)) p(y(T) \\mid [\\dot y(t_n) = f(y(t_n))]_{n=0}^N, y(0) = \\theta), \\theta) d y(T) $$ Loosely speaking, this distribution averages $N(y(T), \\sigma^2I)$ over all IVP solutions $y(T)$ that are realistic given the differential equation, grid $t_0, ..., t_N$, and prior distribution $p(y)$. This is useful, because if the approximation error is large, $M(\\theta)$ \"knows this\". If the approximation error is ridiculously small, $M(\\theta)$ \"knows this\" too and  we recover the procedure described for non-probabilistic solvers above. Interestingly, non-probabilistic solvers cannot do this averaging because they do not yield a statistical description of estimated IVP solutions. Non-probabilistic solvers would also fail if the observations were noise-free (i.e. $\\sigma = 0$), but the present example notebook remains stable. (Try it yourself!)</p> <p>To sample $\\theta$ according to $M$ (respectively $\\log M$), we evaluate $M(\\theta)$ with ProbDiffEq, compute its gradient with JAX, and use this gradient to sample $\\theta$ with BlackJAX:</p> <ol> <li><p>ProbDiffEq: Compute the probabilistic IVP solution by approximating $p(y(T) ~|~ [\\dot y(t_n) = f(y(t_n))]_n, y(0) = \\theta)$</p> </li> <li><p>ProbDiffEq: Evaluate  $M(\\theta)$ by marginalising over the IVP solution computed in step 1.</p> </li> <li><p>JAX: Compute the gradient $\\nabla_\\theta M(\\theta)$</p> </li> <li><p>BlackJAX: Sample from $\\log M(\\theta)$ using, for example, the No-U-Turn-Sampler (which requires $\\nabla_\\theta M(\\theta))$.</p> </li> </ol> <p>Here is how:</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#problem-setting","title":"Problem setting\u00b6","text":"<p>First, we set up an IVP and create some artificial data by simulating the system with \"incorrect\" initial conditions.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#log-posterior-densities-via-probdiffeq","title":"Log-posterior densities via ProbDiffEq\u00b6","text":"<p>Set up a log-posterior density function that we can plug into BlackJAX. Choose a Gaussian prior centered at the initial guess with a large variance.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#sampling-with-blackjax","title":"Sampling with BlackJAX\u00b6","text":"<p>From here on, BlackJAX takes over:</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#visualisation","title":"Visualisation\u00b6","text":"<p>Now that we have samples of $\\theta$, let's plot the corresponding solutions:</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#conclusion","title":"Conclusion\u00b6","text":"<p>In conclusion, a log-posterior density function can be provided by ProbDiffEq such that any of BlackJAX' samplers yield parameter estimates of IVPs.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#whats-next","title":"What's next\u00b6","text":"<p>Try to get a feeling for how the sampler reacts to changing observation noises, solver parameters, and so on. We could extend the sampling problem from $\\theta \\mapsto \\log M(\\theta)$ to some $(\\theta, \\sigma) \\mapsto \\log \\tilde M(\\theta, \\sigma)$, i.e., treat the observation noise as unknown and run Hamiltonian Monte Carlo in a higher-dimensional parameter space. We could also add a more suitable prior distribution $p(\\theta)$ to regularise the problem.</p> <p>A final side note: We could also replace the sampler with an optimisation algorithm and use this procedure to solve boundary value problems (albeit this may not be very efficient; use this algorithm instead).</p>"},{"location":"examples_advanced/parameter_estimation_optax/","title":"Parameter estimation (Optax)","text":"In\u00a0[1]: Copied! <pre>\"\"\"Estimate ODE parameters with ProbDiffEq and Optax.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats\n</pre> \"\"\"Estimate ODE parameters with ProbDiffEq and Optax.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, stats In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax  <p>Create a problem and some fake-data:</p> In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.lotka_volterra()\nf_args = jnp.asarray(f_args)\n\n\n@jax.jit\ndef vf(y, t, *, p):  # noqa: ARG001\n    \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"\n    return f(y, *p)\n\n\ndef solve(p):\n    \"\"\"Evaluate the parameter-to-solution map.\"\"\"\n    tcoeffs = (u0, vf(u0, t0, p=p))\n    output_scale = 10.0\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n        tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n    )\n    ts0 = ivpsolvers.correction_ts0(lambda y, t: vf(y, t, p=p), ssm=ssm)\n    strategy = ivpsolvers.strategy_smoother(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)\n\n\nparameter_true = f_args + 0.05\nparameter_guess = f_args\n\n\nts = jnp.linspace(t0, t1, endpoint=True, num=100)\nsolution_true = solve(parameter_true)\ndata = solution_true.u[0]\nplt.plot(ts, data, \"P-\")\nplt.show()\n</pre> f, u0, (t0, t1), f_args = ivps.lotka_volterra() f_args = jnp.asarray(f_args)   @jax.jit def vf(y, t, *, p):  # noqa: ARG001     \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"     return f(y, *p)   def solve(p):     \"\"\"Evaluate the parameter-to-solution map.\"\"\"     tcoeffs = (u0, vf(u0, t0, p=p))     output_scale = 10.0     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(         tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"     )     ts0 = ivpsolvers.correction_ts0(lambda y, t: vf(y, t, p=p), ssm=ssm)     strategy = ivpsolvers.strategy_smoother(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)   parameter_true = f_args + 0.05 parameter_guess = f_args   ts = jnp.linspace(t0, t1, endpoint=True, num=100) solution_true = solve(parameter_true) data = solution_true.u[0] plt.plot(ts, data, \"P-\") plt.show() <p>We make an initial guess, but it does not lead to a good data fit:</p> In\u00a0[4]: Copied! <pre>solution_guess = solve(parameter_guess)\nplt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125)\nplt.plot(ts, solution_guess.u[0])\nplt.show()\n</pre> solution_guess = solve(parameter_guess) plt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125) plt.plot(ts, solution_guess.u[0]) plt.show() <p>Use the probdiffeq functionality to compute a parameter-to-data fit function.</p> <p>This incorporates the likelihood of the data under the distribution induced by the probabilistic ODE solution (which was generated with the current parameter guess).</p> In\u00a0[5]: Copied! <pre>@jax.jit\ndef parameter_to_data_fit(parameters_, /, standard_deviation=1e-1):\n    \"\"\"Evaluate the data fit as a function of the parameters.\"\"\"\n    sol_ = solve(parameters_)\n    return -1.0 * stats.log_marginal_likelihood(\n        data,\n        standard_deviation=jnp.ones_like(sol_.t) * standard_deviation,\n        posterior=sol_.posterior,\n        ssm=sol_.ssm,\n    )\n\n\nsensitivities = jax.jit(jax.grad(parameter_to_data_fit))\n</pre> @jax.jit def parameter_to_data_fit(parameters_, /, standard_deviation=1e-1):     \"\"\"Evaluate the data fit as a function of the parameters.\"\"\"     sol_ = solve(parameters_)     return -1.0 * stats.log_marginal_likelihood(         data,         standard_deviation=jnp.ones_like(sol_.t) * standard_deviation,         posterior=sol_.posterior,         ssm=sol_.ssm,     )   sensitivities = jax.jit(jax.grad(parameter_to_data_fit)) <p>We can differentiate the function forward- and reverse-mode (the latter is possible because we use fixed steps)</p> In\u00a0[6]: Copied! <pre>parameter_to_data_fit(parameter_guess)\nsensitivities(parameter_guess)\n</pre> parameter_to_data_fit(parameter_guess) sensitivities(parameter_guess) Out[6]: <pre>Array([44.874245, 68.57649 , 51.921116, 24.462992], dtype=float32)</pre> <p>Now, enter optax: build an optimizer, and optimise the parameter-to-model-fit function. The following is more or less taken from the optax-documentation.</p> In\u00a0[7]: Copied! <pre>def build_update_fn(*, optimizer, loss_fn):\n    \"\"\"Build a function for executing a single step in the optimization.\"\"\"\n\n    @jax.jit\n    def update(params, opt_state):\n        \"\"\"Update the optimiser state.\"\"\"\n        _loss, grads = jax.value_and_grad(loss_fn)(params)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state\n\n    return update\n\n\noptim = optax.adam(learning_rate=1e-2)\nupdate_fn = build_update_fn(optimizer=optim, loss_fn=parameter_to_data_fit)\n</pre> def build_update_fn(*, optimizer, loss_fn):     \"\"\"Build a function for executing a single step in the optimization.\"\"\"      @jax.jit     def update(params, opt_state):         \"\"\"Update the optimiser state.\"\"\"         _loss, grads = jax.value_and_grad(loss_fn)(params)         updates, opt_state = optimizer.update(grads, opt_state)         params = optax.apply_updates(params, updates)         return params, opt_state      return update   optim = optax.adam(learning_rate=1e-2) update_fn = build_update_fn(optimizer=optim, loss_fn=parameter_to_data_fit) In\u00a0[8]: Copied! <pre>p = parameter_guess\nstate = optim.init(p)\n\nchunk_size = 10\nfor i in range(chunk_size):\n    for _ in range(chunk_size):\n        p, state = update_fn(p, state)\n\n    print(f\"After {(i + 1) * chunk_size} iterations:\", p)\n</pre> p = parameter_guess state = optim.init(p)  chunk_size = 10 for i in range(chunk_size):     for _ in range(chunk_size):         p, state = update_fn(p, state)      print(f\"After {(i + 1) * chunk_size} iterations:\", p) <pre>After 10 iterations: [0.42702356 0.04230684 0.42326686 0.05160703]\nAfter 20 iterations: [0.45761824 0.07951873 0.45698848 0.04045166]\nAfter 30 iterations: [0.479498   0.07957117 0.47714958 0.05495602]\nAfter 40 iterations: [0.49850866 0.07780019 0.49430028 0.07007415]\nAfter 50 iterations: [0.5140376  0.08094417 0.5093278  0.08073121]\nAfter 60 iterations: [0.5262163  0.08694369 0.5220063  0.08696918]\nAfter 70 iterations: [0.5352239  0.0924586  0.53167146 0.09090954]\nAfter 80 iterations: [0.5416279  0.09584909 0.5384373  0.09399506]\nAfter 90 iterations: [0.54584557 0.09759764 0.5427567  0.09657454]\nAfter 100 iterations: [0.5485108  0.09855766 0.54545856 0.09833144]\n</pre> <p>The solution looks much better:</p> In\u00a0[9]: Copied! <pre>solution_better = solve(p)\nplt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125)\nplt.plot(ts, solution_better.u[0])\nplt.show()\n</pre> solution_better = solve(p) plt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125) plt.plot(ts, solution_better.u[0]) plt.show()"},{"location":"examples_advanced/parameter_estimation_optax/#parameter-estimation-optax","title":"Parameter estimation (Optax)\u00b6","text":"<p>We create some data, compute the marginal likelihood of this data under the ODE posterior (which is something you cannot do with non-probabilistic solvers!), and optimize the parameters with <code>optax</code>.</p> <p>Link to paper: https://arxiv.org/abs/2202.01287</p>"},{"location":"examples_advanced/solve_pde/","title":"Solve a PDE","text":"In\u00a0[1]: Copied! <pre>\"\"\"Solve a PDE.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\njax.config.update(\"jax_enable_x64\", True)\n\n\ndef main():\n    \"\"\"Simulate a PDE.\"\"\"\n    key = jax.random.PRNGKey(1)\n    f, (u0,), (t0, t1) = fhn_2d(key, num=40, t1=10.0)\n\n    @jax.jit\n    def vf(y, *, t):  # noqa: ARG001\n        \"\"\"Evaluate the dynamics of the PDE.\"\"\"\n        return f(y)\n\n    print(\"Problem dimension:\", u0.size)\n\n    # Set up a state-space model\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\")\n\n    # Build a solver\n    ts = ivpsolvers.correction_ts1(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\n    solver = ivpsolvers.solver_dynamic(\n        ssm=ssm, strategy=strategy, prior=ibm, correction=ts\n    )\n    adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n\n    # Solve the ODE\n    save_at = jnp.linspace(t0, t1, num=5, endpoint=True)\n    simulate = simulator(save_at=save_at, adaptive_solver=adaptive_solver, ssm=ssm)\n    (u, u_std) = simulate(init)\n\n    fig, axes = plt.subplots(\n        nrows=2, ncols=len(u), figsize=(2 * len(u), 3), tight_layout=True\n    )\n    for t_i, u_i, std_i, ax_i in zip(save_at, u, u_std, axes.T):\n        ax_i[0].set_title(f\"t = {t_i:.1f}\")\n        img = ax_i[0].imshow(u_i[0], cmap=\"copper\", vmin=-1, vmax=1)\n        plt.colorbar(img)\n\n        uncertainty = jnp.log10(jnp.abs(std_i[0]) + 1e-10)\n        img = ax_i[1].imshow(uncertainty, cmap=\"bone\", vmin=-7, vmax=-3)\n        plt.colorbar(img)\n\n        ax_i[0].set_xticks(())\n        ax_i[1].set_xticks(())\n        ax_i[0].set_yticks(())\n        ax_i[1].set_yticks(())\n\n    axes[0][0].set_ylabel(\"PDE solution\")\n    axes[1][0].set_ylabel(\"log(stdev)\")\n    plt.show()\n\n\ndef simulator(save_at, adaptive_solver, ssm):\n    \"\"\"Simulate a PDE.\"\"\"\n\n    @jax.jit\n    def solve(init):\n        solution = ivpsolve.solve_adaptive_save_at(\n            init, save_at=save_at, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n        return (solution.u[0], solution.u_std[0])\n\n    return solve\n\n\ndef fhn_2d(prng_key, *, num, t1, t0=0.0, a=2.8e-4, b=5e-3, k=-0.005, tau=1.0):\n    \"\"\"Construct the FitzHugh-Nagumo PDE.\n\n    Source: https://github.com/pnkraemer/tornadox/blob/main/tornadox/ivp.py\n\n    But simplified since Probdiffeq can handle matrix-valued ODEs.\n    Here, we also set tau = 1.0 to make the example quick to execute.\n    \"\"\"\n    y0 = jax.random.uniform(prng_key, shape=(2, num, num))\n\n    @jax.jit\n    def fhn_2d(x):\n        u, v = x\n        du = _laplace_2d(u, dx=1.0 / num)\n        dv = _laplace_2d(v, dx=1.0 / num)\n        u_new = a * du + u - u**3 - v + k\n        v_new = (b * dv + u - v) / tau\n        return jnp.stack((u_new, v_new))\n\n    return fhn_2d, (y0,), (t0, t1)\n\n\ndef _laplace_2d(grid, dx):\n    \"\"\"2D Laplace operator on a vectorized 2d grid.\"\"\"\n    # Set the boundary values to the nearest interior node\n    # This enforces Neumann conditions.\n    padded_grid = jnp.pad(grid, pad_width=1, mode=\"edge\")\n\n    # Laplacian via convolve2d()\n    kernel = jnp.array([[0.0, 1.0, 0.0], [1.0, -4.0, 1.0], [0.0, 1.0, 0.0]])\n    kernel /= dx**2\n    grid = jax.scipy.signal.convolve2d(padded_grid, kernel, mode=\"same\")\n    return grid[1:-1, 1:-1]\n\n\nif __name__ == \"__main__\":\n    main()\n</pre> \"\"\"Solve a PDE.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt  from probdiffeq import ivpsolve, ivpsolvers, taylor  jax.config.update(\"jax_enable_x64\", True)   def main():     \"\"\"Simulate a PDE.\"\"\"     key = jax.random.PRNGKey(1)     f, (u0,), (t0, t1) = fhn_2d(key, num=40, t1=10.0)      @jax.jit     def vf(y, *, t):  # noqa: ARG001         \"\"\"Evaluate the dynamics of the PDE.\"\"\"         return f(y)      print(\"Problem dimension:\", u0.size)      # Set up a state-space model     tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\")      # Build a solver     ts = ivpsolvers.correction_ts1(vf, ssm=ssm)     strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)     solver = ivpsolvers.solver_dynamic(         ssm=ssm, strategy=strategy, prior=ibm, correction=ts     )     adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)      # Solve the ODE     save_at = jnp.linspace(t0, t1, num=5, endpoint=True)     simulate = simulator(save_at=save_at, adaptive_solver=adaptive_solver, ssm=ssm)     (u, u_std) = simulate(init)      fig, axes = plt.subplots(         nrows=2, ncols=len(u), figsize=(2 * len(u), 3), tight_layout=True     )     for t_i, u_i, std_i, ax_i in zip(save_at, u, u_std, axes.T):         ax_i[0].set_title(f\"t = {t_i:.1f}\")         img = ax_i[0].imshow(u_i[0], cmap=\"copper\", vmin=-1, vmax=1)         plt.colorbar(img)          uncertainty = jnp.log10(jnp.abs(std_i[0]) + 1e-10)         img = ax_i[1].imshow(uncertainty, cmap=\"bone\", vmin=-7, vmax=-3)         plt.colorbar(img)          ax_i[0].set_xticks(())         ax_i[1].set_xticks(())         ax_i[0].set_yticks(())         ax_i[1].set_yticks(())      axes[0][0].set_ylabel(\"PDE solution\")     axes[1][0].set_ylabel(\"log(stdev)\")     plt.show()   def simulator(save_at, adaptive_solver, ssm):     \"\"\"Simulate a PDE.\"\"\"      @jax.jit     def solve(init):         solution = ivpsolve.solve_adaptive_save_at(             init, save_at=save_at, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm         )         return (solution.u[0], solution.u_std[0])      return solve   def fhn_2d(prng_key, *, num, t1, t0=0.0, a=2.8e-4, b=5e-3, k=-0.005, tau=1.0):     \"\"\"Construct the FitzHugh-Nagumo PDE.      Source: https://github.com/pnkraemer/tornadox/blob/main/tornadox/ivp.py      But simplified since Probdiffeq can handle matrix-valued ODEs.     Here, we also set tau = 1.0 to make the example quick to execute.     \"\"\"     y0 = jax.random.uniform(prng_key, shape=(2, num, num))      @jax.jit     def fhn_2d(x):         u, v = x         du = _laplace_2d(u, dx=1.0 / num)         dv = _laplace_2d(v, dx=1.0 / num)         u_new = a * du + u - u**3 - v + k         v_new = (b * dv + u - v) / tau         return jnp.stack((u_new, v_new))      return fhn_2d, (y0,), (t0, t1)   def _laplace_2d(grid, dx):     \"\"\"2D Laplace operator on a vectorized 2d grid.\"\"\"     # Set the boundary values to the nearest interior node     # This enforces Neumann conditions.     padded_grid = jnp.pad(grid, pad_width=1, mode=\"edge\")      # Laplacian via convolve2d()     kernel = jnp.array([[0.0, 1.0, 0.0], [1.0, -4.0, 1.0], [0.0, 1.0, 0.0]])     kernel /= dx**2     grid = jax.scipy.signal.convolve2d(padded_grid, kernel, mode=\"same\")     return grid[1:-1, 1:-1]   if __name__ == \"__main__\":     main() <pre>Problem dimension: 3200\n</pre>"},{"location":"examples_advanced/solve_pde/#solve-a-pde","title":"Solve a PDE\u00b6","text":"<p>This tutorial replicates Figure 1 from https://arxiv.org/abs/2110.11812, but uses some advanced features in Probdiffeq, namely, solving matrix-valued problems and adaptive simulation with fixedpoint smoothing.</p>"},{"location":"examples_basic/conditioning_on_zero_residual/","title":"How probabilistic solvers work","text":"In\u00a0[1]: Copied! <pre>\"\"\"Demonstrate how probabilistic solvers work via conditioning on constraints.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n</pre> \"\"\"Demonstrate how probabilistic solvers work via conditioning on constraints.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax In\u00a0[3]: Copied! <pre># Create an ODE problem\n\n\n@jax.jit\ndef vector_field(y, t):  # noqa: ARG001\n    \"\"\"Evaluate the logistic ODE vector field.\"\"\"\n    return 10.0 * y * (2.0 - y)\n\n\nt0, t1 = 0.0, 0.5\nu0 = jnp.asarray([0.1])\n</pre> # Create an ODE problem   @jax.jit def vector_field(y, t):  # noqa: ARG001     \"\"\"Evaluate the logistic ODE vector field.\"\"\"     return 10.0 * y * (2.0 - y)   t0, t1 = 0.0, 0.5 u0 = jnp.asarray([0.1]) In\u00a0[4]: Copied! <pre># Assemble the discretised prior (with and without the correct Taylor coefficients)\n\nNUM_DERIVATIVES = 2\ntcoeffs_like = [u0] * (NUM_DERIVATIVES + 1)\nts = jnp.linspace(t0, t1, num=500, endpoint=True)\ninit_raw, transitions, ssm = ivpsolvers.prior_wiener_integrated_discrete(\n    ts, tcoeffs_like, output_scale=100.0, ssm_fact=\"dense\"\n)\n\nmarkov_seq_prior = stats.MarkovSeq(init_raw, transitions)\n\n\ntcoeffs = taylor.odejet_padded_scan(\n    lambda y: vector_field(y, t=t0), (u0,), num=NUM_DERIVATIVES\n)\ninit_tcoeffs = ssm.normal.from_tcoeffs(tcoeffs)\nmarkov_seq_tcoeffs = stats.MarkovSeq(init_tcoeffs, transitions)\n</pre> # Assemble the discretised prior (with and without the correct Taylor coefficients)  NUM_DERIVATIVES = 2 tcoeffs_like = [u0] * (NUM_DERIVATIVES + 1) ts = jnp.linspace(t0, t1, num=500, endpoint=True) init_raw, transitions, ssm = ivpsolvers.prior_wiener_integrated_discrete(     ts, tcoeffs_like, output_scale=100.0, ssm_fact=\"dense\" )  markov_seq_prior = stats.MarkovSeq(init_raw, transitions)   tcoeffs = taylor.odejet_padded_scan(     lambda y: vector_field(y, t=t0), (u0,), num=NUM_DERIVATIVES ) init_tcoeffs = ssm.normal.from_tcoeffs(tcoeffs) markov_seq_tcoeffs = stats.MarkovSeq(init_tcoeffs, transitions) In\u00a0[5]: Copied! <pre># Compute the posterior\n\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"dense\"\n)\nts1 = ivpsolvers.correction_ts1(vector_field, ssm=ssm)\nstrategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\nsolver = ivpsolvers.solver(strategy, prior=ibm, correction=ts1, ssm=ssm)\nadaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-2, ssm=ssm)\n\ndt0 = ivpsolve.dt0(lambda y: vector_field(y, t=t0), (u0,))\nsol = ivpsolve.solve_adaptive_save_at(\n    init, save_at=ts, dt0=1.0, adaptive_solver=adaptive_solver, ssm=ssm\n)\nmarkov_seq_posterior = stats.markov_select_terminal(sol.posterior)\n</pre> # Compute the posterior  init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"dense\" ) ts1 = ivpsolvers.correction_ts1(vector_field, ssm=ssm) strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm) solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts1, ssm=ssm) adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-2, ssm=ssm)  dt0 = ivpsolve.dt0(lambda y: vector_field(y, t=t0), (u0,)) sol = ivpsolve.solve_adaptive_save_at(     init, save_at=ts, dt0=1.0, adaptive_solver=adaptive_solver, ssm=ssm ) markov_seq_posterior = stats.markov_select_terminal(sol.posterior) In\u00a0[6]: Copied! <pre># Compute marginals\n\nmargs_prior = stats.markov_marginals(markov_seq_prior, reverse=False, ssm=ssm)\nmargs_tcoeffs = stats.markov_marginals(markov_seq_tcoeffs, reverse=False, ssm=ssm)\nmargs_posterior = stats.markov_marginals(markov_seq_posterior, reverse=True, ssm=ssm)\n</pre> # Compute marginals  margs_prior = stats.markov_marginals(markov_seq_prior, reverse=False, ssm=ssm) margs_tcoeffs = stats.markov_marginals(markov_seq_tcoeffs, reverse=False, ssm=ssm) margs_posterior = stats.markov_marginals(markov_seq_posterior, reverse=True, ssm=ssm) In\u00a0[7]: Copied! <pre># Compute samples\n\nnum_samples = 5\nkey = jax.random.PRNGKey(seed=1)\nsamples_prior, _ = stats.markov_sample(\n    key, markov_seq_prior, shape=(num_samples,), reverse=False, ssm=ssm\n)\nsamples_tcoeffs, _ = stats.markov_sample(\n    key, markov_seq_tcoeffs, shape=(num_samples,), reverse=False, ssm=ssm\n)\nsamples_posterior, _ = stats.markov_sample(\n    key, markov_seq_posterior, shape=(num_samples,), reverse=True, ssm=ssm\n)\n</pre> # Compute samples  num_samples = 5 key = jax.random.PRNGKey(seed=1) samples_prior, _ = stats.markov_sample(     key, markov_seq_prior, shape=(num_samples,), reverse=False, ssm=ssm ) samples_tcoeffs, _ = stats.markov_sample(     key, markov_seq_tcoeffs, shape=(num_samples,), reverse=False, ssm=ssm ) samples_posterior, _ = stats.markov_sample(     key, markov_seq_posterior, shape=(num_samples,), reverse=True, ssm=ssm ) In\u00a0[8]: Copied! <pre># Plot the results\n\nfig, (axes_state, axes_residual, axes_log_abs) = plt.subplots(\n    nrows=3, ncols=3, sharex=True, sharey=\"row\", constrained_layout=True, figsize=(8, 5)\n)\naxes_state[0].set_title(\"Prior\")\naxes_state[1].set_title(\"w/ Initial condition\")\naxes_state[2].set_title(\"Posterior\")\n\nsample_style = {\"marker\": \"None\", \"alpha\": 0.99, \"linewidth\": 0.75}\nmean_style = {\n    \"marker\": \"None\",\n    \"color\": \"black\",\n    \"linestyle\": \"dashed\",\n    \"linewidth\": 0.99,\n}\n\n\ndef residual(x, t):\n    \"\"\"Evaluate the ODE residual.\"\"\"\n    return x[1] - jax.vmap(jax.vmap(vector_field), in_axes=(0, None))(x[0], t)\n\n\nresidual_prior = residual(samples_prior, ts[:-1])\nresidual_tcoeffs = residual(samples_tcoeffs, ts[:-1])\nresidual_posterior = residual(samples_posterior, ts[:-1])\n\n\nfor i in range(num_samples):\n    # Plot all state-samples\n    axes_state[0].plot(ts[1:], samples_prior[0][i, ..., 0], **sample_style, color=\"C0\")\n    axes_state[1].plot(\n        ts[1:], samples_tcoeffs[0][i, ..., 0], **sample_style, color=\"C1\"\n    )\n    axes_state[2].plot(\n        ts[:-1], samples_posterior[0][i, ..., 0], **sample_style, color=\"C2\"\n    )\n\n    # Plot all residual-samples\n    axes_residual[0].plot(ts[:-1], residual_prior[i, ...], **sample_style, color=\"C0\")\n    axes_residual[1].plot(ts[:-1], residual_tcoeffs[i, ...], **sample_style, color=\"C1\")\n    axes_residual[2].plot(\n        ts[:-1], residual_posterior[i, ...], **sample_style, color=\"C2\"\n    )\n\n    # Plot all log-residual samples\n    axes_log_abs[0].plot(\n        ts[:-1], jnp.log10(jnp.abs(residual_prior))[i, ...], **sample_style, color=\"C0\"\n    )\n    axes_log_abs[1].plot(\n        ts[:-1],\n        jnp.log10(jnp.abs(residual_tcoeffs))[i, ...],\n        **sample_style,\n        color=\"C1\",\n    )\n    axes_log_abs[2].plot(\n        ts[:-1],\n        jnp.log10(jnp.abs(residual_posterior))[i, ...],\n        **sample_style,\n        color=\"C2\",\n    )\n#\n\n\ndef residual_mean(x, t):\n    \"\"\"Evaluate the ODE residual.\"\"\"\n    return x[1] - jax.vmap(vector_field)(x[0], t)\n\n\n# # Plot state means\naxes_state[0].plot(ts[1:], ssm.stats.qoi(margs_prior)[0], **mean_style)\naxes_state[1].plot(ts[1:], ssm.stats.qoi(margs_tcoeffs)[0], **mean_style)\naxes_state[2].plot(ts[:-1], ssm.stats.qoi(margs_posterior)[0], **mean_style)\n\n# # Plot residual means\naxes_residual[0].plot(\n    ts[:-1], residual_mean(ssm.stats.qoi(margs_prior), ts[:-1]), **mean_style\n)\naxes_residual[1].plot(\n    ts[:-1], residual_mean(ssm.stats.qoi(margs_tcoeffs), ts[:-1]), **mean_style\n)\naxes_residual[2].plot(\n    ts[:-1], residual_mean(ssm.stats.qoi(margs_posterior), ts[:-1]), **mean_style\n)\n\n\n# Set the x- and y-ticks/limits\naxes_state[0].set_xticks((t0, (t0 + t1) / 2, t1))\naxes_state[0].set_xlim((t0, t1))\n\naxes_state[0].set_ylim((-1, 3))\naxes_state[0].set_yticks((-1, 1, 3))\n\naxes_residual[0].set_ylim((-10.0, 20))\naxes_residual[0].set_yticks((-10.0, 5, 20))\n\naxes_log_abs[0].set_ylim((-6, 4))\naxes_log_abs[0].set_yticks((-6, -1, 4))\n\n# Label the x- and y-axes\naxes_state[0].set_ylabel(\"Solution\")\naxes_residual[0].set_ylabel(\"Residual\")\naxes_log_abs[0].set_ylabel(r\"Log-residual\")\naxes_log_abs[0].set_xlabel(\"Time $t$\")\naxes_log_abs[1].set_xlabel(\"Time $t$\")\naxes_log_abs[2].set_xlabel(\"Time $t$\")\n\n# Show the result\nfig.align_ylabels()\nplt.show()\n</pre> # Plot the results  fig, (axes_state, axes_residual, axes_log_abs) = plt.subplots(     nrows=3, ncols=3, sharex=True, sharey=\"row\", constrained_layout=True, figsize=(8, 5) ) axes_state[0].set_title(\"Prior\") axes_state[1].set_title(\"w/ Initial condition\") axes_state[2].set_title(\"Posterior\")  sample_style = {\"marker\": \"None\", \"alpha\": 0.99, \"linewidth\": 0.75} mean_style = {     \"marker\": \"None\",     \"color\": \"black\",     \"linestyle\": \"dashed\",     \"linewidth\": 0.99, }   def residual(x, t):     \"\"\"Evaluate the ODE residual.\"\"\"     return x[1] - jax.vmap(jax.vmap(vector_field), in_axes=(0, None))(x[0], t)   residual_prior = residual(samples_prior, ts[:-1]) residual_tcoeffs = residual(samples_tcoeffs, ts[:-1]) residual_posterior = residual(samples_posterior, ts[:-1])   for i in range(num_samples):     # Plot all state-samples     axes_state[0].plot(ts[1:], samples_prior[0][i, ..., 0], **sample_style, color=\"C0\")     axes_state[1].plot(         ts[1:], samples_tcoeffs[0][i, ..., 0], **sample_style, color=\"C1\"     )     axes_state[2].plot(         ts[:-1], samples_posterior[0][i, ..., 0], **sample_style, color=\"C2\"     )      # Plot all residual-samples     axes_residual[0].plot(ts[:-1], residual_prior[i, ...], **sample_style, color=\"C0\")     axes_residual[1].plot(ts[:-1], residual_tcoeffs[i, ...], **sample_style, color=\"C1\")     axes_residual[2].plot(         ts[:-1], residual_posterior[i, ...], **sample_style, color=\"C2\"     )      # Plot all log-residual samples     axes_log_abs[0].plot(         ts[:-1], jnp.log10(jnp.abs(residual_prior))[i, ...], **sample_style, color=\"C0\"     )     axes_log_abs[1].plot(         ts[:-1],         jnp.log10(jnp.abs(residual_tcoeffs))[i, ...],         **sample_style,         color=\"C1\",     )     axes_log_abs[2].plot(         ts[:-1],         jnp.log10(jnp.abs(residual_posterior))[i, ...],         **sample_style,         color=\"C2\",     ) #   def residual_mean(x, t):     \"\"\"Evaluate the ODE residual.\"\"\"     return x[1] - jax.vmap(vector_field)(x[0], t)   # # Plot state means axes_state[0].plot(ts[1:], ssm.stats.qoi(margs_prior)[0], **mean_style) axes_state[1].plot(ts[1:], ssm.stats.qoi(margs_tcoeffs)[0], **mean_style) axes_state[2].plot(ts[:-1], ssm.stats.qoi(margs_posterior)[0], **mean_style)  # # Plot residual means axes_residual[0].plot(     ts[:-1], residual_mean(ssm.stats.qoi(margs_prior), ts[:-1]), **mean_style ) axes_residual[1].plot(     ts[:-1], residual_mean(ssm.stats.qoi(margs_tcoeffs), ts[:-1]), **mean_style ) axes_residual[2].plot(     ts[:-1], residual_mean(ssm.stats.qoi(margs_posterior), ts[:-1]), **mean_style )   # Set the x- and y-ticks/limits axes_state[0].set_xticks((t0, (t0 + t1) / 2, t1)) axes_state[0].set_xlim((t0, t1))  axes_state[0].set_ylim((-1, 3)) axes_state[0].set_yticks((-1, 1, 3))  axes_residual[0].set_ylim((-10.0, 20)) axes_residual[0].set_yticks((-10.0, 5, 20))  axes_log_abs[0].set_ylim((-6, 4)) axes_log_abs[0].set_yticks((-6, -1, 4))  # Label the x- and y-axes axes_state[0].set_ylabel(\"Solution\") axes_residual[0].set_ylabel(\"Residual\") axes_log_abs[0].set_ylabel(r\"Log-residual\") axes_log_abs[0].set_xlabel(\"Time $t$\") axes_log_abs[1].set_xlabel(\"Time $t$\") axes_log_abs[2].set_xlabel(\"Time $t$\")  # Show the result fig.align_ylabels() plt.show()"},{"location":"examples_basic/conditioning_on_zero_residual/#how-probabilistic-solvers-work","title":"How probabilistic solvers work\u00b6","text":"<p>Probabilistic solvers condition a prior distribution on satisfying a zero-ODE-residual on a specified grid.</p>"},{"location":"examples_basic/dynamic_output_scales/","title":"Solver types","text":"In\u00a0[1]: Copied! <pre>\"\"\"Display the behaviour of the solvers when the scale of the ODE varies.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers\n</pre> \"\"\"Display the behaviour of the solvers when the scale of the ODE varies.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.affine_independent(initial_values=(1.0,), a=2.0)\n\n\n@jax.jit\ndef vf(*ys, t):  # noqa: ARG001\n    \"\"\"Evaluate the affine vector field.\"\"\"\n    return f(*ys, *f_args)\n</pre> f, u0, (t0, t1), f_args = ivps.affine_independent(initial_values=(1.0,), a=2.0)   @jax.jit def vf(*ys, t):  # noqa: ARG001     \"\"\"Evaluate the affine vector field.\"\"\"     return f(*ys, *f_args) In\u00a0[4]: Copied! <pre>num_derivatives = 1\n\ntcoeffs = (u0, vf(u0, t=t0))\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"dense\"\n)\nts1 = ivpsolvers.correction_ts1(vf, ssm=ssm)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\ndynamic = ivpsolvers.solver_dynamic(strategy, prior=ibm, correction=ts1, ssm=ssm)\nmle = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts1, ssm=ssm)\n</pre> num_derivatives = 1  tcoeffs = (u0, vf(u0, t=t0)) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"dense\" ) ts1 = ivpsolvers.correction_ts1(vf, ssm=ssm) strategy = ivpsolvers.strategy_filter(ssm=ssm) dynamic = ivpsolvers.solver_dynamic(strategy, prior=ibm, correction=ts1, ssm=ssm) mle = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts1, ssm=ssm) In\u00a0[5]: Copied! <pre>t0, t1 = 0.0, 3.0\nnum_pts = 200\n\nts = jnp.linspace(t0, t1, num=num_pts, endpoint=True)\n\n\nsolution_dynamic = ivpsolve.solve_fixed_grid(init, grid=ts, solver=dynamic, ssm=ssm)\nsolution_mle = ivpsolve.solve_fixed_grid(init, grid=ts, solver=mle, ssm=ssm)\n</pre> t0, t1 = 0.0, 3.0 num_pts = 200  ts = jnp.linspace(t0, t1, num=num_pts, endpoint=True)   solution_dynamic = ivpsolve.solve_fixed_grid(init, grid=ts, solver=dynamic, ssm=ssm) solution_mle = ivpsolve.solve_fixed_grid(init, grid=ts, solver=mle, ssm=ssm) <p>Plot the solution.</p> In\u00a0[6]: Copied! <pre>fig, (axes_linear, axes_log) = plt.subplots(ncols=2, nrows=2, sharex=True, sharey=\"row\")\n\n\nu_dynamic = solution_dynamic.u[0]\nu_mle = solution_mle.u[0]\nscale_dynamic = solution_dynamic.output_scale\nscale_mle = jnp.ones_like(solution_mle.output_scale) * solution_mle.output_scale[-1]\n\nstyle_target = {\n    \"marker\": \"None\",\n    \"label\": \"Target\",\n    \"color\": \"black\",\n    \"linewidth\": 0.5,\n    \"alpha\": 1,\n    \"linestyle\": \"dashed\",\n}\nstyle_approx = {\n    \"marker\": \"None\",\n    \"label\": \"Posterior mean\",\n    \"color\": \"C0\",\n    \"linewidth\": 1.5,\n    \"alpha\": 0.75,\n}\nstyle_scale = {\n    \"marker\": \"None\",\n    \"color\": \"C3\",\n    \"linestyle\": \"solid\",\n    \"label\": \"Output scale\",\n    \"linewidth\": 1.5,\n    \"alpha\": 0.75,\n}\n\naxes_linear[0].set_title(\"Time-varying model\")\naxes_linear[0].plot(ts, jnp.exp(ts * 2), **style_target)\naxes_linear[0].plot(ts, u_dynamic, **style_approx)\naxes_linear[0].plot(ts[1:], scale_dynamic, **style_scale)\naxes_linear[0].legend()\n\naxes_linear[1].set_title(\"Constant model\")\naxes_linear[1].plot(ts, jnp.exp(ts * 2), **style_target)\naxes_linear[1].plot(ts, u_mle, **style_approx)\naxes_linear[1].plot(ts[1:], scale_mle, **style_scale)\naxes_linear[1].legend()\n\naxes_linear[0].set_ylabel(\"Linear scale\")\n\naxes_linear[0].set_xlim((t0, t1))\n\n\naxes_log[0].semilogy(ts, jnp.exp(ts * 2), **style_target)\naxes_log[0].semilogy(ts, u_dynamic, **style_approx)\naxes_log[0].semilogy(ts[1:], scale_dynamic, **style_scale)\naxes_log[0].legend()\n\naxes_log[1].semilogy(ts, jnp.exp(ts * 2), **style_target)\naxes_log[1].semilogy(ts, u_mle, **style_approx)\naxes_log[1].semilogy(ts[1:], scale_mle, **style_scale)\naxes_log[1].legend()\n\naxes_log[0].set_ylabel(\"Logarithmic scale\")\naxes_log[0].set_xlabel(\"Time t\")\naxes_log[1].set_xlabel(\"Time t\")\n\naxes_log[0].set_xlim((t0, t1))\n\nfig.align_ylabels()\nplt.show()\n</pre> fig, (axes_linear, axes_log) = plt.subplots(ncols=2, nrows=2, sharex=True, sharey=\"row\")   u_dynamic = solution_dynamic.u[0] u_mle = solution_mle.u[0] scale_dynamic = solution_dynamic.output_scale scale_mle = jnp.ones_like(solution_mle.output_scale) * solution_mle.output_scale[-1]  style_target = {     \"marker\": \"None\",     \"label\": \"Target\",     \"color\": \"black\",     \"linewidth\": 0.5,     \"alpha\": 1,     \"linestyle\": \"dashed\", } style_approx = {     \"marker\": \"None\",     \"label\": \"Posterior mean\",     \"color\": \"C0\",     \"linewidth\": 1.5,     \"alpha\": 0.75, } style_scale = {     \"marker\": \"None\",     \"color\": \"C3\",     \"linestyle\": \"solid\",     \"label\": \"Output scale\",     \"linewidth\": 1.5,     \"alpha\": 0.75, }  axes_linear[0].set_title(\"Time-varying model\") axes_linear[0].plot(ts, jnp.exp(ts * 2), **style_target) axes_linear[0].plot(ts, u_dynamic, **style_approx) axes_linear[0].plot(ts[1:], scale_dynamic, **style_scale) axes_linear[0].legend()  axes_linear[1].set_title(\"Constant model\") axes_linear[1].plot(ts, jnp.exp(ts * 2), **style_target) axes_linear[1].plot(ts, u_mle, **style_approx) axes_linear[1].plot(ts[1:], scale_mle, **style_scale) axes_linear[1].legend()  axes_linear[0].set_ylabel(\"Linear scale\")  axes_linear[0].set_xlim((t0, t1))   axes_log[0].semilogy(ts, jnp.exp(ts * 2), **style_target) axes_log[0].semilogy(ts, u_dynamic, **style_approx) axes_log[0].semilogy(ts[1:], scale_dynamic, **style_scale) axes_log[0].legend()  axes_log[1].semilogy(ts, jnp.exp(ts * 2), **style_target) axes_log[1].semilogy(ts, u_mle, **style_approx) axes_log[1].semilogy(ts[1:], scale_mle, **style_scale) axes_log[1].legend()  axes_log[0].set_ylabel(\"Logarithmic scale\") axes_log[0].set_xlabel(\"Time t\") axes_log[1].set_xlabel(\"Time t\")  axes_log[0].set_xlim((t0, t1))  fig.align_ylabels() plt.show() <p>The dynamic solver adapts the output-scale so that both the solution and the output-scale grow exponentially. The ODE-solution fits the truth well.</p> <p>The solver_mle does not have this tool, and the ODE solution is not able to follow the exponential: it drifts back to the origin. (This is expected, we are basically trying to fit an exponential with a piecewise polynomial.)</p>"},{"location":"examples_basic/dynamic_output_scales/#solver-types","title":"Solver types\u00b6","text":"<p>You can choose between a <code>adaptive.solver_calibrationfree()</code> (which does not calibrate the output-scale), a <code>adaptive.solver_mle()</code> (which calibrates a global output scale via quasi-maximum-likelihood-estimation), and a <code>adaptive.solver_dynamic()</code>, which calibrates a time-varying, piecewise constant output-scale via \"local' quasi-maximum-likelihood estimation, similar to how ODE solver estimate local errors.</p> <p>But are these good for? In short: choose a <code>solver_dynamic</code> if your ODE output-scale varies quite strongly, and choose an <code>solver_mle</code> otherwise.</p> <p>For example, consider the numerical solution of a linear ODE with fixed steps:</p>"},{"location":"examples_basic/posterior_uncertainties/","title":"Posterior uncertainties","text":"In\u00a0[1]: Copied! <pre>\"\"\"Display the marginal uncertainties of filters and smoothers.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n\n# Set up the ODE\n\n\ndef vf(y, *, t):  # noqa: ARG001\n    \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"\n    y0, y1 = y[0], y[1]\n\n    y0_new = 0.5 * y0 - 0.05 * y0 * y1\n    y1_new = -0.5 * y1 + 0.05 * y0 * y1\n    return jnp.asarray([y0_new, y1_new])\n\n\nt0 = 0.0\nt1 = 2.0\nu0 = jnp.asarray([20.0, 20.0])\n\n\n# Set up a solver\n# To all users: Try replacing the fixedpoint-smoother with a filter!\ntcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=3)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\")\nts = ivpsolvers.correction_ts1(vf, ssm=ssm)\nstrategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\nsolver = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts, ssm=ssm)\nadaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-1, ssm=ssm)\n\n# Solve the ODE\nts = jnp.linspace(t0, t1, endpoint=True, num=50)\nsol = ivpsolve.solve_adaptive_save_at(\n    init, save_at=ts, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n)\n\n# Calibrate\nmarginals = stats.calibrate(sol.marginals, output_scale=sol.output_scale, ssm=ssm)\nstd = ssm.stats.standard_deviation(marginals)\nu_std = ssm.stats.qoi_from_sample(std)\n\n# Plot the solution\nfig, axes = plt.subplots(\n    nrows=3,\n    ncols=len(tcoeffs),\n    sharex=\"col\",\n    tight_layout=True,\n    figsize=(len(u_std) * 2, 5),\n)\nfor i, (u_i, std_i, ax_i) in enumerate(zip(sol.u, u_std, axes.T)):\n    # Set up titles and axis descriptions\n    if i == 0:\n        ax_i[0].set_title(\"State\")\n        ax_i[0].set_ylabel(\"Prey\")\n        ax_i[1].set_ylabel(\"Predators\")\n        ax_i[2].set_ylabel(\"Std.-dev.\")\n    elif i == 1:\n        ax_i[0].set_title(f\"{i}st deriv.\")\n    elif i == 2:\n        ax_i[0].set_title(f\"{i}nd deriv.\")\n    elif i == 3:\n        ax_i[0].set_title(f\"{i}rd deriv.\")\n    else:\n        ax_i[0].set_title(f\"{i}th deriv.\")\n\n    ax_i[-1].set_xlabel(\"Time\")\n\n    for m, std, ax in zip(u_i.T, std_i.T, ax_i):\n        # Plot the mean\n        ax.plot(sol.t, m)\n\n        # Plot the standard deviation\n        lower, upper = m - 1.96 * std, m + 1.96 * std\n        ax.fill_between(sol.t, lower, upper, alpha=0.3)\n        ax.set_xlim((jnp.amin(ts), jnp.amax(ts)))\n\n    ax_i[2].semilogy(sol.t, std_i[:, 0], label=\"Prey\")\n    ax_i[2].semilogy(sol.t, std_i[:, 1], label=\"Predators\")\n    ax_i[2].legend(fontsize=\"x-small\")\n\nfig.align_ylabels()\nplt.show()\n</pre> \"\"\"Display the marginal uncertainties of filters and smoothers.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor  # Set up the ODE   def vf(y, *, t):  # noqa: ARG001     \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"     y0, y1 = y[0], y[1]      y0_new = 0.5 * y0 - 0.05 * y0 * y1     y1_new = -0.5 * y1 + 0.05 * y0 * y1     return jnp.asarray([y0_new, y1_new])   t0 = 0.0 t1 = 2.0 u0 = jnp.asarray([20.0, 20.0])   # Set up a solver # To all users: Try replacing the fixedpoint-smoother with a filter! tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=3) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\") ts = ivpsolvers.correction_ts1(vf, ssm=ssm) strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm) solver = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts, ssm=ssm) adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-1, ssm=ssm)  # Solve the ODE ts = jnp.linspace(t0, t1, endpoint=True, num=50) sol = ivpsolve.solve_adaptive_save_at(     init, save_at=ts, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm )  # Calibrate marginals = stats.calibrate(sol.marginals, output_scale=sol.output_scale, ssm=ssm) std = ssm.stats.standard_deviation(marginals) u_std = ssm.stats.qoi_from_sample(std)  # Plot the solution fig, axes = plt.subplots(     nrows=3,     ncols=len(tcoeffs),     sharex=\"col\",     tight_layout=True,     figsize=(len(u_std) * 2, 5), ) for i, (u_i, std_i, ax_i) in enumerate(zip(sol.u, u_std, axes.T)):     # Set up titles and axis descriptions     if i == 0:         ax_i[0].set_title(\"State\")         ax_i[0].set_ylabel(\"Prey\")         ax_i[1].set_ylabel(\"Predators\")         ax_i[2].set_ylabel(\"Std.-dev.\")     elif i == 1:         ax_i[0].set_title(f\"{i}st deriv.\")     elif i == 2:         ax_i[0].set_title(f\"{i}nd deriv.\")     elif i == 3:         ax_i[0].set_title(f\"{i}rd deriv.\")     else:         ax_i[0].set_title(f\"{i}th deriv.\")      ax_i[-1].set_xlabel(\"Time\")      for m, std, ax in zip(u_i.T, std_i.T, ax_i):         # Plot the mean         ax.plot(sol.t, m)          # Plot the standard deviation         lower, upper = m - 1.96 * std, m + 1.96 * std         ax.fill_between(sol.t, lower, upper, alpha=0.3)         ax.set_xlim((jnp.amin(ts), jnp.amax(ts)))      ax_i[2].semilogy(sol.t, std_i[:, 0], label=\"Prey\")     ax_i[2].semilogy(sol.t, std_i[:, 1], label=\"Predators\")     ax_i[2].legend(fontsize=\"x-small\")  fig.align_ylabels() plt.show()"},{"location":"examples_basic/posterior_uncertainties/#posterior-uncertainties","title":"Posterior uncertainties\u00b6","text":""},{"location":"examples_basic/second_order_problems/","title":"Second-order systems","text":"In\u00a0[1]: Copied! <pre>\"\"\"Demonstrate how to solve second-order IVPs without transforming them first.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n</pre> \"\"\"Demonstrate how to solve second-order IVPs without transforming them first.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, taylor In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax  <p>Quick refresher: first-order ODEs</p> In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.three_body_restricted_first_order()\n\n\n@jax.jit\ndef vf_1(y, t):  # noqa: ARG001\n    \"\"\"Evaluate the three-body problem as a first-order IVP.\"\"\"\n    return f(y, *f_args)\n\n\ntcoeffs = taylor.odejet_padded_scan(lambda y: vf_1(y, t=t0), (u0,), num=4)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\"\n)\nts0 = ivpsolvers.correction_ts0(vf_1, ssm=ssm)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\nsolver_1st = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm)\nadaptive_solver_1st = ivpsolvers.adaptive(solver_1st, atol=1e-5, rtol=1e-5, ssm=ssm)\n</pre> f, u0, (t0, t1), f_args = ivps.three_body_restricted_first_order()   @jax.jit def vf_1(y, t):  # noqa: ARG001     \"\"\"Evaluate the three-body problem as a first-order IVP.\"\"\"     return f(y, *f_args)   tcoeffs = taylor.odejet_padded_scan(lambda y: vf_1(y, t=t0), (u0,), num=4) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\" ) ts0 = ivpsolvers.correction_ts0(vf_1, ssm=ssm) strategy = ivpsolvers.strategy_filter(ssm=ssm) solver_1st = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm) adaptive_solver_1st = ivpsolvers.adaptive(solver_1st, atol=1e-5, rtol=1e-5, ssm=ssm) In\u00a0[4]: Copied! <pre>solution = ivpsolve.solve_adaptive_save_every_step(\n    init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_1st, ssm=ssm\n)\n</pre> solution = ivpsolve.solve_adaptive_save_every_step(     init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_1st, ssm=ssm ) In\u00a0[5]: Copied! <pre>norm = jnp.linalg.norm((solution.u[0][-1] - u0) / jnp.abs(1.0 + u0))\nplt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\")\nplt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\")\nplt.show()\n</pre> norm = jnp.linalg.norm((solution.u[0][-1] - u0) / jnp.abs(1.0 + u0)) plt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\") plt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\") plt.show() <p>The default configuration assumes that the ODE to be solved is of first order. Now, the same game with a second-order ODE</p> In\u00a0[6]: Copied! <pre>f, (u0, du0), (t0, t1), f_args = ivps.three_body_restricted()\n\n\n@jax.jit\ndef vf_2(y, dy, t):  # noqa: ARG001\n    \"\"\"Evaluate the three-body problem as a second-order IVP.\"\"\"\n    return f(y, dy, *f_args)\n\n\n# One derivative more than above because we don't transform to first order\ntcoeffs = taylor.odejet_padded_scan(lambda *ys: vf_2(*ys, t=t0), (u0, du0), num=3)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\"\n)\nts0 = ivpsolvers.correction_ts0(vf_2, ode_order=2, ssm=ssm)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\nsolver_2nd = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm)\nadaptive_solver_2nd = ivpsolvers.adaptive(solver_2nd, atol=1e-5, rtol=1e-5, ssm=ssm)\n</pre> f, (u0, du0), (t0, t1), f_args = ivps.three_body_restricted()   @jax.jit def vf_2(y, dy, t):  # noqa: ARG001     \"\"\"Evaluate the three-body problem as a second-order IVP.\"\"\"     return f(y, dy, *f_args)   # One derivative more than above because we don't transform to first order tcoeffs = taylor.odejet_padded_scan(lambda *ys: vf_2(*ys, t=t0), (u0, du0), num=3) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\" ) ts0 = ivpsolvers.correction_ts0(vf_2, ode_order=2, ssm=ssm) strategy = ivpsolvers.strategy_filter(ssm=ssm) solver_2nd = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm) adaptive_solver_2nd = ivpsolvers.adaptive(solver_2nd, atol=1e-5, rtol=1e-5, ssm=ssm)  In\u00a0[7]: Copied! <pre>solution = ivpsolve.solve_adaptive_save_every_step(\n    init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_2nd, ssm=ssm\n)\n</pre> solution = ivpsolve.solve_adaptive_save_every_step(     init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_2nd, ssm=ssm ) In\u00a0[8]: Copied! <pre>norm = jnp.linalg.norm((solution.u[0][-1, ...] - u0) / jnp.abs(1.0 + u0))\nplt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\")\nplt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\")\nplt.show()\n</pre> norm = jnp.linalg.norm((solution.u[0][-1, ...] - u0) / jnp.abs(1.0 + u0)) plt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\") plt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\") plt.show() <p>The results are indistinguishable from the plot. While the runtimes of both solvers are similar, the error of the second-order solver is much lower.</p> <p>See the benchmarks for more quantitative versions of this statement.</p>"},{"location":"examples_basic/second_order_problems/#second-order-systems","title":"Second-order systems\u00b6","text":""},{"location":"examples_basic/taylor_coefficients/","title":"Taylor coefficients","text":"In\u00a0[1]: Copied! <pre>\"\"\"Demonstrate how central Taylor coefficient estimation is to Probdiffeq.\"\"\"\n\nimport collections\n\nimport jax\nimport jax.numpy as jnp\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n\nif not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> \"\"\"Demonstrate how central Taylor coefficient estimation is to Probdiffeq.\"\"\"  import collections  import jax import jax.numpy as jnp from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor  if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax  <p>We start by defining an ODE.</p> In\u00a0[2]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.logistic()\n\n\ndef vf(*y, t):  # noqa: ARG001\n    \"\"\"Evaluate the vector field.\"\"\"\n    return f(*y, *f_args)\n</pre> f, u0, (t0, t1), f_args = ivps.logistic()   def vf(*y, t):  # noqa: ARG001     \"\"\"Evaluate the vector field.\"\"\"     return f(*y, *f_args) <p>Here is a wrapper arounds Probdiffeq's solution routine.</p> In\u00a0[3]: Copied! <pre>def solve(tc):\n    \"\"\"Solve the ODE.\"\"\"\n    init, prior, ssm = ivpsolvers.prior_wiener_integrated(tc, ssm_fact=\"dense\")\n    ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\n    solver = ivpsolvers.solver_mle(strategy, prior=prior, correction=ts0, ssm=ssm)\n    ts = jnp.linspace(t0, t1, endpoint=True, num=10)\n    adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-2, rtol=1e-2, ssm=ssm)\n    return ivpsolve.solve_adaptive_save_at(\n        init, save_at=ts, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm\n    )\n</pre> def solve(tc):     \"\"\"Solve the ODE.\"\"\"     init, prior, ssm = ivpsolvers.prior_wiener_integrated(tc, ssm_fact=\"dense\")     ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)     strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)     solver = ivpsolvers.solver_mle(strategy, prior=prior, correction=ts0, ssm=ssm)     ts = jnp.linspace(t0, t1, endpoint=True, num=10)     adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-2, rtol=1e-2, ssm=ssm)     return ivpsolve.solve_adaptive_save_at(         init, save_at=ts, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm     ) <p>It's time to solve some ODEs:</p> In\u00a0[4]: Copied! <pre>tcoeffs = taylor.odejet_padded_scan(lambda *y: vf(*y, t=t0), [u0], num=2)\nsolution = solve(tcoeffs)\nprint(jax.tree.map(jnp.shape, solution))\n</pre> tcoeffs = taylor.odejet_padded_scan(lambda *y: vf(*y, t=t0), [u0], num=2) solution = solve(tcoeffs) print(jax.tree.map(jnp.shape, solution))  <pre>IVPSolution(t=(10,), u=[(10,), (10,), (10,)], u_std=[(10,), (10,), (10,)], output_scale=(9,), marginals=Normal(mean=(10, 3), cholesky=(10, 3, 3)), posterior=MarkovSeq(init=Normal(mean=(10, 3), cholesky=(10, 3, 3)), conditional=LatentCond(A=(9, 3, 3), noise=Normal(mean=(9, 3), cholesky=(9, 3, 3)), to_latent=(9, 3), to_observed=(9, 3))), num_steps=(9,), ssm=FactImpl(name='dense', prototypes=&lt;probdiffeq.impl._prototypes.DensePrototype object at 0x7fdd35759820&gt;, normal=&lt;probdiffeq.impl._normal.DenseNormal object at 0x7fdd359c11f0&gt;, stats=&lt;probdiffeq.impl._stats.DenseStats object at 0x7fdd30086150&gt;, linearise=&lt;probdiffeq.impl._linearise.DenseLinearisation object at 0x7fdd30a66420&gt;, conditional=&lt;probdiffeq.impl._conditional.DenseConditional object at 0x7fdd30084ad0&gt;, num_derivatives=2, unravel=&lt;jax._src.util.HashablePartial object at 0x7fdd351a0c50&gt;))\n</pre> <p>The type of solution.u matches that of the initial condition.</p> In\u00a0[5]: Copied! <pre>print(jax.tree.map(jnp.shape, tcoeffs))\nprint(jax.tree.map(jnp.shape, solution.u))\n</pre>  print(jax.tree.map(jnp.shape, tcoeffs)) print(jax.tree.map(jnp.shape, solution.u)) <pre>[(), (), ()]\n[(10,), (10,), (10,)]\n</pre> <p>Anything that behaves like a list work. For example, we can use lists or tuples, but also named tuples.</p> In\u00a0[6]: Copied! <pre>Taylor = collections.namedtuple(\"Taylor\", [\"state\", \"velocity\", \"acceleration\"])\ntcoeffs = Taylor(*tcoeffs)\nsolution = solve(tcoeffs)\n\nprint(jax.tree.map(jnp.shape, tcoeffs))\nprint(jax.tree.map(jnp.shape, solution))\nprint(jax.tree.map(jnp.shape, solution.u))\n</pre>  Taylor = collections.namedtuple(\"Taylor\", [\"state\", \"velocity\", \"acceleration\"]) tcoeffs = Taylor(*tcoeffs) solution = solve(tcoeffs)  print(jax.tree.map(jnp.shape, tcoeffs)) print(jax.tree.map(jnp.shape, solution)) print(jax.tree.map(jnp.shape, solution.u)) <pre>Taylor(state=(), velocity=(), acceleration=())\nIVPSolution(t=(10,), u=Taylor(state=(10,), velocity=(10,), acceleration=(10,)), u_std=Taylor(state=(10,), velocity=(10,), acceleration=(10,)), output_scale=(9,), marginals=Normal(mean=(10, 3), cholesky=(10, 3, 3)), posterior=MarkovSeq(init=Normal(mean=(10, 3), cholesky=(10, 3, 3)), conditional=LatentCond(A=(9, 3, 3), noise=Normal(mean=(9, 3), cholesky=(9, 3, 3)), to_latent=(9, 3), to_observed=(9, 3))), num_steps=(9,), ssm=FactImpl(name='dense', prototypes=&lt;probdiffeq.impl._prototypes.DensePrototype object at 0x7fdd1062f6b0&gt;, normal=&lt;probdiffeq.impl._normal.DenseNormal object at 0x7fdd1062d730&gt;, stats=&lt;probdiffeq.impl._stats.DenseStats object at 0x7fdd351a2660&gt;, linearise=&lt;probdiffeq.impl._linearise.DenseLinearisation object at 0x7fdd8961bc80&gt;, conditional=&lt;probdiffeq.impl._conditional.DenseConditional object at 0x7fdd351a3500&gt;, num_derivatives=2, unravel=&lt;jax._src.util.HashablePartial object at 0x7fdd10399af0&gt;))\nTaylor(state=(10,), velocity=(10,), acceleration=(10,))\n</pre> <p>The same applies to statistical quantities that we can extract from the solution. For example, the standard deviation or samples from the solution object:</p> In\u00a0[7]: Copied! <pre>key = jax.random.PRNGKey(seed=15)\nposterior = stats.markov_select_terminal(solution.posterior)\nsamples, samples_init = stats.markov_sample(\n    key, posterior, reverse=True, ssm=solution.ssm\n)\n\nprint(jax.tree.map(jnp.shape, solution.u))\nprint(jax.tree.map(jnp.shape, solution.u_std))\nprint(jax.tree.map(jnp.shape, samples))\nprint(jax.tree.map(jnp.shape, samples_init))\n</pre>  key = jax.random.PRNGKey(seed=15) posterior = stats.markov_select_terminal(solution.posterior) samples, samples_init = stats.markov_sample(     key, posterior, reverse=True, ssm=solution.ssm )  print(jax.tree.map(jnp.shape, solution.u)) print(jax.tree.map(jnp.shape, solution.u_std)) print(jax.tree.map(jnp.shape, samples)) print(jax.tree.map(jnp.shape, samples_init))  <pre>Taylor(state=(10,), velocity=(10,), acceleration=(10,))\nTaylor(state=(10,), velocity=(10,), acceleration=(10,))\nTaylor(state=(9,), velocity=(9,), acceleration=(9,))\nTaylor(state=(), velocity=(), acceleration=())\n</pre>"},{"location":"examples_basic/taylor_coefficients/#taylor-coefficients","title":"Taylor coefficients\u00b6","text":"<p>To build a probabilistic solver, we need to build a specific state-space model. To build this specific state-space model, we interact with Taylor coefficients. Here are some examples how Taylor coefficients play a role in Probdiffeq's solution routines.</p>"},{"location":"examples_benchmarks/convergence-rates-lotka-volterra/","title":"CR: Lotka-Volterra","text":"In\u00a0[1]: Copied! <pre>\"\"\"Evaluate the convergence rates of the probabilistic solvers.\"\"\"\n\nimport functools\nimport statistics\nimport timeit\nfrom collections.abc import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.integrate\nimport tqdm\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\n\ndef main():\n    \"\"\"Run the script.\"\"\"\n    # High order solvers need double precision\n    jax.config.update(\"jax_enable_x64\", True)\n\n    # Assemble algorithms\n    algorithms = {\n        r\"TS1(1)\": solver_probdiffeq(1),\n        r\"TS1(2)\": solver_probdiffeq(2),\n        r\"TS1(3)\": solver_probdiffeq(3),\n        r\"TS1(4)\": solver_probdiffeq(4),\n        r\"TS1(5)\": solver_probdiffeq(5),\n        r\"TS1(6)\": solver_probdiffeq(6),\n        r\"TS1(7)\": solver_probdiffeq(7),\n        r\"TS1(8)\": solver_probdiffeq(8),\n        r\"TS1(9)\": solver_probdiffeq(9),\n        r\"TS1(10)\": solver_probdiffeq(10),\n        r\"TS1(11)\": solver_probdiffeq(11),\n        r\"TS1(12)\": solver_probdiffeq(12),\n        r\"TS1(13)\": solver_probdiffeq(13),\n        r\"TS1(14)\": solver_probdiffeq(14),\n        r\"TS1(15)\": solver_probdiffeq(15),\n        r\"TS1(16)\": solver_probdiffeq(16),\n        r\"TS1(17)\": solver_probdiffeq(17),\n    }\n\n    # Set up the benchmark (compute a reference etc.)\n    reference = solver_scipy(method=\"LSODA\")(1e-12)\n    tolerances = 0.1 ** jnp.arange(2, 8, step=0.25)\n    precision_fun = rmse_relative(reference)\n    timeit_fun = timer()\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in tqdm.tqdm(algorithms.items()):\n        param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)\n        results[label] = param_to_wp(tolerances)\n\n    layout = [[\"values\", \"trends\"]]\n    fig, ax = plt.subplot_mosaic(\n        layout,\n        figsize=(8, 3),\n        constrained_layout=True,\n        dpi=120,\n        sharex=True,\n        sharey=True,\n    )\n    for i, (keys, values) in enumerate(results.items()):\n        cmap = mpl.colormaps[\"managua\"]\n        i_clipped = i / len(results.keys())\n        color = mpl.colors.to_hex(cmap(i_clipped))\n\n        # Smooth curves\n        x, y = smooth(values[\"work_num_steps\"], values[\"precision\"])\n        x_lin, y_lin = linear_trend(values[\"work_num_steps\"], values[\"precision\"])\n\n        # All curves start at (1, 1)\n        ax[\"values\"].loglog(x / x.min(), y / y.max(), color=color, label=keys)\n        ax[\"trends\"].loglog(\n            x_lin / x_lin.min(), y_lin / y_lin.max(), color=color, label=keys\n        )\n\n    ax[\"values\"].set_title(\"Values (slightly smoothed)\")\n    ax[\"trends\"].set_title(\"Decay rates (linear fit)\")\n    ax[\"values\"].set_ylabel(\"RMSE (normalised)\")\n\n    for a in [ax[\"values\"], ax[\"trends\"]]:\n        a.grid(which=\"minor\", linestyle=\"dotted\")\n        a.set_xlabel(\"Num steps (normalised)\")\n        a.legend(fontsize=\"x-small\", ncols=2)\n    plt.show()\n\n\ndef solver_scipy(*, method: str) -&gt; Callable:\n    \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"\n\n    def vf_scipy(_t, y):\n        \"\"\"Lotka--Volterra dynamics.\"\"\"\n        dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]\n        dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]\n        return np.asarray([dy1, dy2])\n\n    u0 = jnp.asarray((20.0, 20.0))\n    time_span = np.asarray([0.0, 50.0])\n\n    def param_to_solution(tol):\n        solution = scipy.integrate.solve_ivp(\n            vf_scipy,\n            y0=u0,\n            t_span=time_span,\n            t_eval=time_span,\n            atol=1e-3 * tol,\n            rtol=tol,\n            method=method,\n        )\n        return jnp.asarray(solution.y[..., -1])\n\n    return param_to_solution\n\n\ndef rmse_relative(expected: jax.Array, *, nugget=1e-5) -&gt; Callable:\n    \"\"\"Compute the relative RMSE.\"\"\"\n    expected = jnp.asarray(expected)\n\n    def rmse(received):\n        received = jnp.asarray(received)\n        error_absolute = jnp.abs(expected - received)\n        error_relative = error_absolute / jnp.abs(nugget + expected)\n        return jnp.linalg.norm(error_relative) / jnp.sqrt(error_relative.size)\n\n    return rmse\n\n\ndef timer():\n    \"\"\"Construct a timing function.\"\"\"\n\n    def timeit_fun(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=1))\n\n    return timeit_fun\n\n\ndef solver_probdiffeq(num_derivatives: int) -&gt; Callable:\n    \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"\n\n    @jax.jit\n    def vf_probdiffeq(y, *, t):  # noqa: ARG001\n        \"\"\"Lotka--Volterra dynamics.\"\"\"\n        dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]\n        dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]\n        return jnp.asarray([dy1, dy2])\n\n    u0 = jnp.asarray((20.0, 20.0))\n    t0, t1 = (0.0, 50.0)\n\n    @jax.jit\n    def param_to_solution(tol):\n        # Build a solver\n        vf_auto = functools.partial(vf_probdiffeq, t=t0)\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num_derivatives)\n\n        init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")\n        strategy = ivpsolvers.strategy_filter(ssm=ssm)\n        corr = ivpsolvers.correction_ts1(vf_probdiffeq, ssm=ssm)\n        solver = ivpsolvers.solver(strategy, prior=ibm, correction=corr, ssm=ssm)\n        control = ivpsolvers.control_proportional_integral()\n        adaptive_solver = ivpsolvers.adaptive(\n            solver, atol=1e-2 * tol, rtol=tol, control=control, ssm=ssm\n        )\n\n        # Solve\n        dt0 = ivpsolve.dt0(vf_auto, (u0,))\n        solution = ivpsolve.solve_adaptive_terminal_values(\n            init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n\n        # Return the terminal value\n        return solution.u[0], solution.num_steps\n\n    return param_to_solution\n\n\ndef workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:\n    \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"\n\n    def parameter_list_to_workprecision(list_of_args, /):\n        works_num_steps = []\n        works_min = []\n        works_mean = []\n        works_std = []\n        precisions = []\n        for arg in list_of_args:\n            x, num_steps = fun(arg)\n\n            precision = precision_fun(fun(arg)[0].block_until_ready())\n            times = timeit_fun(lambda: fun(arg)[0].block_until_ready())  # noqa: B023\n\n            precisions.append(precision)\n            works_num_steps.append(num_steps)\n            works_min.append(min(times))\n            works_mean.append(statistics.mean(times))\n            if len(times) &gt; 1:\n                works_std.append(statistics.stdev(times))\n        return {\n            \"work_mean\": jnp.asarray(works_mean),\n            \"work_min\": jnp.asarray(works_min),\n            \"work_num_steps\": jnp.asarray(works_num_steps),\n            \"work_std\": jnp.asarray(works_std),\n            \"precision\": jnp.asarray(precisions),\n        }\n\n    return parameter_list_to_workprecision\n\n\ndef smooth(x, y, window=2):\n    \"\"\"Smooth a set of data points to improve visualisation.\"\"\"\n    kernel = jnp.ones((window,)) / window\n    x = jnp.convolve(x, kernel, mode=\"valid\")\n    y = jnp.convolve(y, kernel, mode=\"valid\")\n    return x, y\n\n\ndef linear_trend(x, y):\n    \"\"\"Fit a linear curve through the logarithms of x and y.\"\"\"\n    x = jnp.log10(x)\n    y = jnp.log10(y)\n    scale, bias = jnp.polyfit(x, y, 1)\n    return 10 ** (x), 10 ** (scale * x + bias)\n\n\nmain()\n</pre> \"\"\"Evaluate the convergence rates of the probabilistic solvers.\"\"\"  import functools import statistics import timeit from collections.abc import Callable  import jax import jax.numpy as jnp import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import scipy.integrate import tqdm  from probdiffeq import ivpsolve, ivpsolvers, taylor   def main():     \"\"\"Run the script.\"\"\"     # High order solvers need double precision     jax.config.update(\"jax_enable_x64\", True)      # Assemble algorithms     algorithms = {         r\"TS1(1)\": solver_probdiffeq(1),         r\"TS1(2)\": solver_probdiffeq(2),         r\"TS1(3)\": solver_probdiffeq(3),         r\"TS1(4)\": solver_probdiffeq(4),         r\"TS1(5)\": solver_probdiffeq(5),         r\"TS1(6)\": solver_probdiffeq(6),         r\"TS1(7)\": solver_probdiffeq(7),         r\"TS1(8)\": solver_probdiffeq(8),         r\"TS1(9)\": solver_probdiffeq(9),         r\"TS1(10)\": solver_probdiffeq(10),         r\"TS1(11)\": solver_probdiffeq(11),         r\"TS1(12)\": solver_probdiffeq(12),         r\"TS1(13)\": solver_probdiffeq(13),         r\"TS1(14)\": solver_probdiffeq(14),         r\"TS1(15)\": solver_probdiffeq(15),         r\"TS1(16)\": solver_probdiffeq(16),         r\"TS1(17)\": solver_probdiffeq(17),     }      # Set up the benchmark (compute a reference etc.)     reference = solver_scipy(method=\"LSODA\")(1e-12)     tolerances = 0.1 ** jnp.arange(2, 8, step=0.25)     precision_fun = rmse_relative(reference)     timeit_fun = timer()      # Compute all work-precision diagrams     results = {}     for label, algo in tqdm.tqdm(algorithms.items()):         param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)         results[label] = param_to_wp(tolerances)      layout = [[\"values\", \"trends\"]]     fig, ax = plt.subplot_mosaic(         layout,         figsize=(8, 3),         constrained_layout=True,         dpi=120,         sharex=True,         sharey=True,     )     for i, (keys, values) in enumerate(results.items()):         cmap = mpl.colormaps[\"managua\"]         i_clipped = i / len(results.keys())         color = mpl.colors.to_hex(cmap(i_clipped))          # Smooth curves         x, y = smooth(values[\"work_num_steps\"], values[\"precision\"])         x_lin, y_lin = linear_trend(values[\"work_num_steps\"], values[\"precision\"])          # All curves start at (1, 1)         ax[\"values\"].loglog(x / x.min(), y / y.max(), color=color, label=keys)         ax[\"trends\"].loglog(             x_lin / x_lin.min(), y_lin / y_lin.max(), color=color, label=keys         )      ax[\"values\"].set_title(\"Values (slightly smoothed)\")     ax[\"trends\"].set_title(\"Decay rates (linear fit)\")     ax[\"values\"].set_ylabel(\"RMSE (normalised)\")      for a in [ax[\"values\"], ax[\"trends\"]]:         a.grid(which=\"minor\", linestyle=\"dotted\")         a.set_xlabel(\"Num steps (normalised)\")         a.legend(fontsize=\"x-small\", ncols=2)     plt.show()   def solver_scipy(*, method: str) -&gt; Callable:     \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"      def vf_scipy(_t, y):         \"\"\"Lotka--Volterra dynamics.\"\"\"         dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]         dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]         return np.asarray([dy1, dy2])      u0 = jnp.asarray((20.0, 20.0))     time_span = np.asarray([0.0, 50.0])      def param_to_solution(tol):         solution = scipy.integrate.solve_ivp(             vf_scipy,             y0=u0,             t_span=time_span,             t_eval=time_span,             atol=1e-3 * tol,             rtol=tol,             method=method,         )         return jnp.asarray(solution.y[..., -1])      return param_to_solution   def rmse_relative(expected: jax.Array, *, nugget=1e-5) -&gt; Callable:     \"\"\"Compute the relative RMSE.\"\"\"     expected = jnp.asarray(expected)      def rmse(received):         received = jnp.asarray(received)         error_absolute = jnp.abs(expected - received)         error_relative = error_absolute / jnp.abs(nugget + expected)         return jnp.linalg.norm(error_relative) / jnp.sqrt(error_relative.size)      return rmse   def timer():     \"\"\"Construct a timing function.\"\"\"      def timeit_fun(fun, /):         return list(timeit.repeat(fun, number=1, repeat=1))      return timeit_fun   def solver_probdiffeq(num_derivatives: int) -&gt; Callable:     \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"      @jax.jit     def vf_probdiffeq(y, *, t):  # noqa: ARG001         \"\"\"Lotka--Volterra dynamics.\"\"\"         dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]         dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]         return jnp.asarray([dy1, dy2])      u0 = jnp.asarray((20.0, 20.0))     t0, t1 = (0.0, 50.0)      @jax.jit     def param_to_solution(tol):         # Build a solver         vf_auto = functools.partial(vf_probdiffeq, t=t0)         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num_derivatives)          init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")         strategy = ivpsolvers.strategy_filter(ssm=ssm)         corr = ivpsolvers.correction_ts1(vf_probdiffeq, ssm=ssm)         solver = ivpsolvers.solver(strategy, prior=ibm, correction=corr, ssm=ssm)         control = ivpsolvers.control_proportional_integral()         adaptive_solver = ivpsolvers.adaptive(             solver, atol=1e-2 * tol, rtol=tol, control=control, ssm=ssm         )          # Solve         dt0 = ivpsolve.dt0(vf_auto, (u0,))         solution = ivpsolve.solve_adaptive_terminal_values(             init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm         )          # Return the terminal value         return solution.u[0], solution.num_steps      return param_to_solution   def workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:     \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"      def parameter_list_to_workprecision(list_of_args, /):         works_num_steps = []         works_min = []         works_mean = []         works_std = []         precisions = []         for arg in list_of_args:             x, num_steps = fun(arg)              precision = precision_fun(fun(arg)[0].block_until_ready())             times = timeit_fun(lambda: fun(arg)[0].block_until_ready())  # noqa: B023              precisions.append(precision)             works_num_steps.append(num_steps)             works_min.append(min(times))             works_mean.append(statistics.mean(times))             if len(times) &gt; 1:                 works_std.append(statistics.stdev(times))         return {             \"work_mean\": jnp.asarray(works_mean),             \"work_min\": jnp.asarray(works_min),             \"work_num_steps\": jnp.asarray(works_num_steps),             \"work_std\": jnp.asarray(works_std),             \"precision\": jnp.asarray(precisions),         }      return parameter_list_to_workprecision   def smooth(x, y, window=2):     \"\"\"Smooth a set of data points to improve visualisation.\"\"\"     kernel = jnp.ones((window,)) / window     x = jnp.convolve(x, kernel, mode=\"valid\")     y = jnp.convolve(y, kernel, mode=\"valid\")     return x, y   def linear_trend(x, y):     \"\"\"Fit a linear curve through the logarithms of x and y.\"\"\"     x = jnp.log10(x)     y = jnp.log10(y)     scale, bias = jnp.polyfit(x, y, 1)     return 10 ** (x), 10 ** (scale * x + bias)   main() <pre>\r  0%|          | 0/17 [00:00&lt;?, ?it/s]</pre> <pre>\r  6%|\u258c         | 1/17 [00:16&lt;04:24, 16.53s/it]</pre> <pre>\r 12%|\u2588\u258f        | 2/17 [00:19&lt;02:04,  8.33s/it]</pre> <pre>\r 18%|\u2588\u258a        | 3/17 [00:20&lt;01:12,  5.19s/it]</pre> <pre>\r 24%|\u2588\u2588\u258e       | 4/17 [00:21&lt;00:47,  3.64s/it]</pre> <pre>\r 29%|\u2588\u2588\u2589       | 5/17 [00:23&lt;00:33,  2.77s/it]</pre> <pre>\r 35%|\u2588\u2588\u2588\u258c      | 6/17 [00:24&lt;00:25,  2.28s/it]</pre> <pre>\r 41%|\u2588\u2588\u2588\u2588      | 7/17 [00:25&lt;00:19,  1.93s/it]</pre> <pre>\r 47%|\u2588\u2588\u2588\u2588\u258b     | 8/17 [00:26&lt;00:15,  1.76s/it]</pre> <pre>\r 53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 9/17 [00:28&lt;00:13,  1.70s/it]</pre> <pre>\r 59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 10/17 [00:30&lt;00:11,  1.70s/it]</pre> <pre>\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 11/17 [00:31&lt;00:10,  1.70s/it]</pre> <pre>\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 12/17 [00:33&lt;00:09,  1.80s/it]</pre> <pre>\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 13/17 [00:36&lt;00:07,  1.97s/it]</pre> <pre>\r 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 14/17 [00:38&lt;00:06,  2.14s/it]</pre> <pre>\r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 15/17 [00:41&lt;00:04,  2.26s/it]</pre> <pre>\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:45&lt;00:02,  2.74s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:49&lt;00:00,  3.06s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:49&lt;00:00,  2.89s/it]</pre> <pre>\n</pre>"},{"location":"examples_benchmarks/convergence-rates-lotka-volterra/#cr-lotka-volterra","title":"CR: Lotka-Volterra\u00b6","text":""},{"location":"examples_benchmarks/taylor-init-fitzhughnagumo/","title":"Initialisation: FitzHugh-Nagumo","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark the initialisation methods on the FitzHugh-Nagumo problem.\"\"\"\n\nimport functools\nimport statistics\nimport time\nimport timeit\nfrom collections.abc import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom probdiffeq import taylor\n\n\ndef main(max_time=0.25, repeats=2):\n    \"\"\"Run the script.\"\"\"\n    # Set JAX config\n    jax.config.update(\"jax_enable_x64\", True)\n\n    algorithms = {\n        r\"Forward-mode\": odejet_via_jvp(),\n        r\"Taylor-mode (scan)\": taylor_mode_scan(),\n        r\"Taylor-mode (unroll)\": taylor_mode_unroll(),\n        r\"Taylor-mode (doubling)\": taylor_mode_doubling(),\n    }\n\n    # Compute a reference solution\n    timeit_fun = timeit_fun_from_args(repeats=repeats)\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in algorithms.items():\n        print(\"\\n\")\n        print(label)\n        results[label] = adaptive_benchmark(\n            algo, timeit_fun=timeit_fun, max_time=max_time\n        )\n\n    fig, (axis_perform, axis_compile) = plt.subplots(\n        ncols=2, figsize=(8, 3), dpi=150, sharex=True, sharey=True\n    )\n\n    for label, wp in results.items():\n        inputs = wp[\"arguments\"]\n        work_compile = wp[\"work_compile\"]\n        work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]\n\n        if \"doubling\" in label:\n            num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))\n            inputs = jnp.arange(1, jnp.amax(inputs) * 1)\n            work_compile = _adaptive_repeat(work_compile, num_repeats)\n            work_mean = _adaptive_repeat(work_mean, num_repeats)\n            work_std = _adaptive_repeat(work_std, num_repeats)\n\n        axis_compile.semilogy(inputs, work_compile, label=label)\n        axis_perform.semilogy(inputs, work_mean, label=label)\n\n    axis_compile.set_title(\"Compilation time\")\n    axis_perform.set_title(\"Evaluation time\")\n    axis_perform.legend(fontsize=\"small\")\n    axis_compile.legend(fontsize=\"small\")\n    axis_compile.set_xlabel(\"Number of Derivatives\")\n    axis_perform.set_xlabel(\"Number of Derivatives\")\n    axis_perform.set_ylabel(\"Wall time (sec)\")\n    axis_perform.grid(linestyle=\"dotted\")\n    axis_compile.grid(linestyle=\"dotted\")\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef _adaptive_repeat(xs, ys):\n    \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"\n    zs = []\n    for x, y in zip(xs, ys):\n        zs.extend([x] * int(y))\n    return jnp.asarray(zs)\n\n\ndef timeit_fun_from_args(*, repeats: int) -&gt; Callable:\n    \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"\n\n    def timer(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=repeats))\n\n    return timer\n\n\ndef taylor_mode_scan() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0,) = _fitzhugh_nagumo()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef taylor_mode_unroll() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0,) = _fitzhugh_nagumo()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_unroll(vf_auto, (u0,), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef taylor_mode_doubling() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0,) = _fitzhugh_nagumo()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_doubling_unroll(vf_auto, (u0,), num_doublings=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef odejet_via_jvp() -&gt; Callable:\n    \"\"\"Forward-mode estimation.\"\"\"\n    vf_auto, (u0,) = _fitzhugh_nagumo()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_via_jvp(vf_auto, (u0,), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef _fitzhugh_nagumo():\n    u0 = jnp.asarray([-1.0, 1.0])\n\n    @jax.jit\n    def vf_probdiffeq(u, a=0.2, b=0.2, c=3.0):\n        \"\"\"FitzHugh--Nagumo model.\"\"\"\n        du1 = c * (u[0] - u[0] ** 3 / 3 + u[1])\n        du2 = -(1.0 / c) * (u[0] - a - b * u[1])\n        return jnp.asarray([du1, du2])\n\n    return vf_probdiffeq, (u0,)\n\n\ndef adaptive_benchmark(fun, *, timeit_fun: Callable, max_time) -&gt; dict:\n    \"\"\"Benchmark a function iteratively until a max-time threshold is exceeded.\"\"\"\n    work_compile = []\n    work_mean = []\n    work_std = []\n    arguments = []\n\n    t0 = time.perf_counter()\n    arg = 1\n    while (elapsed := time.perf_counter() - t0) &lt; max_time:\n        print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")\n        t0 = time.perf_counter()\n        tcoeffs = fun(arg).block_until_ready()\n        t1 = time.perf_counter()\n        time_compile = t1 - t0\n\n        time_execute = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023\n\n        arguments.append(len(tcoeffs))\n        work_compile.append(time_compile)\n        work_mean.append(statistics.mean(time_execute))\n        work_std.append(statistics.stdev(time_execute))\n        arg += 1\n    print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")\n    return {\n        \"work_mean\": jnp.asarray(work_mean),\n        \"work_std\": jnp.asarray(work_std),\n        \"work_compile\": jnp.asarray(work_compile),\n        \"arguments\": jnp.asarray(arguments),\n    }\n\n\nmain()\n</pre> \"\"\"Benchmark the initialisation methods on the FitzHugh-Nagumo problem.\"\"\"  import functools import statistics import time import timeit from collections.abc import Callable  import jax import jax.numpy as jnp import matplotlib.pyplot as plt  from probdiffeq import taylor   def main(max_time=0.25, repeats=2):     \"\"\"Run the script.\"\"\"     # Set JAX config     jax.config.update(\"jax_enable_x64\", True)      algorithms = {         r\"Forward-mode\": odejet_via_jvp(),         r\"Taylor-mode (scan)\": taylor_mode_scan(),         r\"Taylor-mode (unroll)\": taylor_mode_unroll(),         r\"Taylor-mode (doubling)\": taylor_mode_doubling(),     }      # Compute a reference solution     timeit_fun = timeit_fun_from_args(repeats=repeats)      # Compute all work-precision diagrams     results = {}     for label, algo in algorithms.items():         print(\"\\n\")         print(label)         results[label] = adaptive_benchmark(             algo, timeit_fun=timeit_fun, max_time=max_time         )      fig, (axis_perform, axis_compile) = plt.subplots(         ncols=2, figsize=(8, 3), dpi=150, sharex=True, sharey=True     )      for label, wp in results.items():         inputs = wp[\"arguments\"]         work_compile = wp[\"work_compile\"]         work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]          if \"doubling\" in label:             num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))             inputs = jnp.arange(1, jnp.amax(inputs) * 1)             work_compile = _adaptive_repeat(work_compile, num_repeats)             work_mean = _adaptive_repeat(work_mean, num_repeats)             work_std = _adaptive_repeat(work_std, num_repeats)          axis_compile.semilogy(inputs, work_compile, label=label)         axis_perform.semilogy(inputs, work_mean, label=label)      axis_compile.set_title(\"Compilation time\")     axis_perform.set_title(\"Evaluation time\")     axis_perform.legend(fontsize=\"small\")     axis_compile.legend(fontsize=\"small\")     axis_compile.set_xlabel(\"Number of Derivatives\")     axis_perform.set_xlabel(\"Number of Derivatives\")     axis_perform.set_ylabel(\"Wall time (sec)\")     axis_perform.grid(linestyle=\"dotted\")     axis_compile.grid(linestyle=\"dotted\")      plt.tight_layout()     plt.show()   def _adaptive_repeat(xs, ys):     \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"     zs = []     for x, y in zip(xs, ys):         zs.extend([x] * int(y))     return jnp.asarray(zs)   def timeit_fun_from_args(*, repeats: int) -&gt; Callable:     \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"      def timer(fun, /):         return list(timeit.repeat(fun, number=1, repeat=repeats))      return timer   def taylor_mode_scan() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0,) = _fitzhugh_nagumo()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num)         return jnp.asarray(tcoeffs)      return estimate   def taylor_mode_unroll() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0,) = _fitzhugh_nagumo()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_unroll(vf_auto, (u0,), num=num)         return jnp.asarray(tcoeffs)      return estimate   def taylor_mode_doubling() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0,) = _fitzhugh_nagumo()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_doubling_unroll(vf_auto, (u0,), num_doublings=num)         return jnp.asarray(tcoeffs)      return estimate   def odejet_via_jvp() -&gt; Callable:     \"\"\"Forward-mode estimation.\"\"\"     vf_auto, (u0,) = _fitzhugh_nagumo()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_via_jvp(vf_auto, (u0,), num=num)         return jnp.asarray(tcoeffs)      return estimate   def _fitzhugh_nagumo():     u0 = jnp.asarray([-1.0, 1.0])      @jax.jit     def vf_probdiffeq(u, a=0.2, b=0.2, c=3.0):         \"\"\"FitzHugh--Nagumo model.\"\"\"         du1 = c * (u[0] - u[0] ** 3 / 3 + u[1])         du2 = -(1.0 / c) * (u[0] - a - b * u[1])         return jnp.asarray([du1, du2])      return vf_probdiffeq, (u0,)   def adaptive_benchmark(fun, *, timeit_fun: Callable, max_time) -&gt; dict:     \"\"\"Benchmark a function iteratively until a max-time threshold is exceeded.\"\"\"     work_compile = []     work_mean = []     work_std = []     arguments = []      t0 = time.perf_counter()     arg = 1     while (elapsed := time.perf_counter() - t0) &lt; max_time:         print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")         t0 = time.perf_counter()         tcoeffs = fun(arg).block_until_ready()         t1 = time.perf_counter()         time_compile = t1 - t0          time_execute = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023          arguments.append(len(tcoeffs))         work_compile.append(time_compile)         work_mean.append(statistics.mean(time_execute))         work_std.append(statistics.stdev(time_execute))         arg += 1     print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")     return {         \"work_mean\": jnp.asarray(work_mean),         \"work_std\": jnp.asarray(work_std),         \"work_compile\": jnp.asarray(work_compile),         \"arguments\": jnp.asarray(arguments),     }   main() <pre>\n\nForward-mode\nnum = 1 | elapsed = 0.00 | max_time = 0.25\nnum = 2 | elapsed = 0.03 | max_time = 0.25\nnum = 3 | elapsed = 0.02 | max_time = 0.25\nnum = 4 | elapsed = 0.04 | max_time = 0.25\nnum = 5 | elapsed = 0.06 | max_time = 0.25\n</pre> <pre>num = 6 | elapsed = 0.11 | max_time = 0.25\nnum = 7 | elapsed = 0.23 | max_time = 0.25\n</pre> <pre>num = 8 | elapsed = 0.45 | max_time = 0.25\n\n\nTaylor-mode (scan)\nnum = 1 | elapsed = 0.00 | max_time = 0.25\nnum = 2 | elapsed = 0.02 | max_time = 0.25\nnum = 3 | elapsed = 0.12 | max_time = 0.25\n</pre> <pre>num = 4 | elapsed = 0.10 | max_time = 0.25\nnum = 5 | elapsed = 0.12 | max_time = 0.25\n</pre> <pre>num = 6 | elapsed = 0.15 | max_time = 0.25\nnum = 7 | elapsed = 0.20 | max_time = 0.25\n</pre> <pre>num = 8 | elapsed = 0.23 | max_time = 0.25\n</pre> <pre>num = 9 | elapsed = 0.28 | max_time = 0.25\n\n\nTaylor-mode (unroll)\nnum = 1 | elapsed = 0.00 | max_time = 0.25\nnum = 2 | elapsed = 0.02 | max_time = 0.25\nnum = 3 | elapsed = 0.04 | max_time = 0.25\nnum = 4 | elapsed = 0.06 | max_time = 0.25\n</pre> <pre>num = 5 | elapsed = 0.08 | max_time = 0.25\nnum = 6 | elapsed = 0.13 | max_time = 0.25\n</pre> <pre>num = 7 | elapsed = 0.18 | max_time = 0.25\n</pre> <pre>num = 8 | elapsed = 0.24 | max_time = 0.25\n</pre> <pre>num = 9 | elapsed = 0.40 | max_time = 0.25\n\n\nTaylor-mode (doubling)\nnum = 1 | elapsed = 0.00 | max_time = 0.25\nnum = 2 | elapsed = 0.09 | max_time = 0.25\n</pre> <pre>num = 3 | elapsed = 0.46 | max_time = 0.25\n</pre>"},{"location":"examples_benchmarks/taylor-init-fitzhughnagumo/#initialisation-fitzhugh-nagumo","title":"Initialisation: FitzHugh-Nagumo\u00b6","text":""},{"location":"examples_benchmarks/taylor-init-node/","title":"Initialisation: Neural ODE","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark the initialisation methods on a neural ODE problem.\"\"\"\n\nimport functools\nimport statistics\nimport time\nimport timeit\nfrom collections.abc import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom probdiffeq import taylor\n\n\ndef main(max_time=0.55, repeats=2):\n    \"\"\"Run the script.\"\"\"\n    # Set JAX config\n    jax.config.update(\"jax_enable_x64\", True)\n\n    algorithms = {\n        r\"Forward-mode\": odejet_via_jvp(),\n        r\"Taylor-mode (scan)\": taylor_mode_scan(),\n        r\"Taylor-mode (unroll)\": taylor_mode_unroll(),\n        r\"Taylor-mode (doubling)\": taylor_mode_doubling(),\n    }\n\n    # Compute a reference solution\n    timeit_fun = timeit_fun_from_args(repeats=repeats)\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in algorithms.items():\n        print(\"\\n\")\n        print(label)\n        results[label] = adaptive_benchmark(\n            algo, timeit_fun=timeit_fun, max_time=max_time\n        )\n\n    fig, (axis_perform, axis_compile) = plt.subplots(\n        ncols=2, figsize=(8, 3), dpi=150, sharex=True, sharey=True\n    )\n\n    for label, wp in results.items():\n        inputs = wp[\"arguments\"]\n        work_compile = wp[\"work_compile\"]\n        work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]\n\n        if \"doubling\" in label:\n            num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))\n            inputs = jnp.arange(1, jnp.amax(inputs) * 1)\n            work_compile = _adaptive_repeat(work_compile, num_repeats)\n            work_mean = _adaptive_repeat(work_mean, num_repeats)\n            work_std = _adaptive_repeat(work_std, num_repeats)\n\n        axis_compile.semilogy(inputs, work_compile, label=label)\n        axis_perform.semilogy(inputs, work_mean, label=label)\n\n    axis_compile.set_title(\"Compilation time\")\n    axis_perform.set_title(\"Evaluation time\")\n    axis_perform.legend(fontsize=\"small\")\n    axis_compile.legend(fontsize=\"small\")\n    axis_compile.set_xlabel(\"Number of Derivatives\")\n    axis_perform.set_xlabel(\"Number of Derivatives\")\n    axis_perform.set_ylabel(\"Wall time (sec)\")\n    axis_perform.grid(linestyle=\"dotted\")\n    axis_compile.grid(linestyle=\"dotted\")\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef _adaptive_repeat(xs, ys):\n    \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"\n    zs = []\n    for x, y in zip(xs, ys):\n        zs.extend([x] * int(y))\n    return jnp.asarray(zs)\n\n\ndef timeit_fun_from_args(*, repeats: int) -&gt; Callable:\n    \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"\n\n    def timer(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=repeats))\n\n    return timer\n\n\ndef taylor_mode_scan() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0,) = _node()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef taylor_mode_unroll() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0,) = _node()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_unroll(vf_auto, (u0,), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef taylor_mode_doubling() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0,) = _node()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_doubling_unroll(vf_auto, (u0,), num_doublings=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef odejet_via_jvp() -&gt; Callable:\n    \"\"\"Forward-mode estimation.\"\"\"\n    vf_auto, (u0,) = _node()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_via_jvp(vf_auto, (u0,), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef _node():\n    N = 100\n    M = 100\n    num_layers = 2\n\n    key = jax.random.PRNGKey(seed=1)\n    key1, key2, key3, key4 = jax.random.split(key, num=4)\n\n    u0 = jax.random.uniform(key1, shape=(N,))\n\n    weights = jax.random.normal(key2, shape=(num_layers, 2, M, N))\n    biases1 = jax.random.normal(key3, shape=(num_layers, M))\n    biases2 = jax.random.normal(key4, shape=(num_layers, N))\n\n    fun = jnp.tanh\n\n    @jax.jit\n    def vf(x):\n        for (w1, w2), b1, b2 in zip(weights, biases1, biases2):\n            x = fun(w2.T @ fun(w1 @ x + b1) + b2)\n        return x\n\n    return vf, (u0,)\n\n\ndef adaptive_benchmark(fun, *, timeit_fun: Callable, max_time) -&gt; dict:\n    \"\"\"Benchmark a function iteratively until a max-time threshold is exceeded.\"\"\"\n    work_compile = []\n    work_mean = []\n    work_std = []\n    arguments = []\n\n    t0 = time.perf_counter()\n    arg = 1\n    while (elapsed := time.perf_counter() - t0) &lt; max_time:\n        print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")\n        t0 = time.perf_counter()\n        tcoeffs = fun(arg).block_until_ready()\n        t1 = time.perf_counter()\n        time_compile = t1 - t0\n\n        time_execute = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023\n\n        arguments.append(len(tcoeffs))\n        work_compile.append(time_compile)\n        work_mean.append(statistics.mean(time_execute))\n        work_std.append(statistics.stdev(time_execute))\n        arg += 1\n    print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")\n    return {\n        \"work_mean\": jnp.asarray(work_mean),\n        \"work_std\": jnp.asarray(work_std),\n        \"work_compile\": jnp.asarray(work_compile),\n        \"arguments\": jnp.asarray(arguments),\n    }\n\n\nmain()\n</pre> \"\"\"Benchmark the initialisation methods on a neural ODE problem.\"\"\"  import functools import statistics import time import timeit from collections.abc import Callable  import jax import jax.numpy as jnp import matplotlib.pyplot as plt  from probdiffeq import taylor   def main(max_time=0.55, repeats=2):     \"\"\"Run the script.\"\"\"     # Set JAX config     jax.config.update(\"jax_enable_x64\", True)      algorithms = {         r\"Forward-mode\": odejet_via_jvp(),         r\"Taylor-mode (scan)\": taylor_mode_scan(),         r\"Taylor-mode (unroll)\": taylor_mode_unroll(),         r\"Taylor-mode (doubling)\": taylor_mode_doubling(),     }      # Compute a reference solution     timeit_fun = timeit_fun_from_args(repeats=repeats)      # Compute all work-precision diagrams     results = {}     for label, algo in algorithms.items():         print(\"\\n\")         print(label)         results[label] = adaptive_benchmark(             algo, timeit_fun=timeit_fun, max_time=max_time         )      fig, (axis_perform, axis_compile) = plt.subplots(         ncols=2, figsize=(8, 3), dpi=150, sharex=True, sharey=True     )      for label, wp in results.items():         inputs = wp[\"arguments\"]         work_compile = wp[\"work_compile\"]         work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]          if \"doubling\" in label:             num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))             inputs = jnp.arange(1, jnp.amax(inputs) * 1)             work_compile = _adaptive_repeat(work_compile, num_repeats)             work_mean = _adaptive_repeat(work_mean, num_repeats)             work_std = _adaptive_repeat(work_std, num_repeats)          axis_compile.semilogy(inputs, work_compile, label=label)         axis_perform.semilogy(inputs, work_mean, label=label)      axis_compile.set_title(\"Compilation time\")     axis_perform.set_title(\"Evaluation time\")     axis_perform.legend(fontsize=\"small\")     axis_compile.legend(fontsize=\"small\")     axis_compile.set_xlabel(\"Number of Derivatives\")     axis_perform.set_xlabel(\"Number of Derivatives\")     axis_perform.set_ylabel(\"Wall time (sec)\")     axis_perform.grid(linestyle=\"dotted\")     axis_compile.grid(linestyle=\"dotted\")      plt.tight_layout()     plt.show()   def _adaptive_repeat(xs, ys):     \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"     zs = []     for x, y in zip(xs, ys):         zs.extend([x] * int(y))     return jnp.asarray(zs)   def timeit_fun_from_args(*, repeats: int) -&gt; Callable:     \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"      def timer(fun, /):         return list(timeit.repeat(fun, number=1, repeat=repeats))      return timer   def taylor_mode_scan() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0,) = _node()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num)         return jnp.asarray(tcoeffs)      return estimate   def taylor_mode_unroll() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0,) = _node()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_unroll(vf_auto, (u0,), num=num)         return jnp.asarray(tcoeffs)      return estimate   def taylor_mode_doubling() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0,) = _node()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_doubling_unroll(vf_auto, (u0,), num_doublings=num)         return jnp.asarray(tcoeffs)      return estimate   def odejet_via_jvp() -&gt; Callable:     \"\"\"Forward-mode estimation.\"\"\"     vf_auto, (u0,) = _node()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_via_jvp(vf_auto, (u0,), num=num)         return jnp.asarray(tcoeffs)      return estimate   def _node():     N = 100     M = 100     num_layers = 2      key = jax.random.PRNGKey(seed=1)     key1, key2, key3, key4 = jax.random.split(key, num=4)      u0 = jax.random.uniform(key1, shape=(N,))      weights = jax.random.normal(key2, shape=(num_layers, 2, M, N))     biases1 = jax.random.normal(key3, shape=(num_layers, M))     biases2 = jax.random.normal(key4, shape=(num_layers, N))      fun = jnp.tanh      @jax.jit     def vf(x):         for (w1, w2), b1, b2 in zip(weights, biases1, biases2):             x = fun(w2.T @ fun(w1 @ x + b1) + b2)         return x      return vf, (u0,)   def adaptive_benchmark(fun, *, timeit_fun: Callable, max_time) -&gt; dict:     \"\"\"Benchmark a function iteratively until a max-time threshold is exceeded.\"\"\"     work_compile = []     work_mean = []     work_std = []     arguments = []      t0 = time.perf_counter()     arg = 1     while (elapsed := time.perf_counter() - t0) &lt; max_time:         print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")         t0 = time.perf_counter()         tcoeffs = fun(arg).block_until_ready()         t1 = time.perf_counter()         time_compile = t1 - t0          time_execute = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023          arguments.append(len(tcoeffs))         work_compile.append(time_compile)         work_mean.append(statistics.mean(time_execute))         work_std.append(statistics.stdev(time_execute))         arg += 1     print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")     return {         \"work_mean\": jnp.asarray(work_mean),         \"work_std\": jnp.asarray(work_std),         \"work_compile\": jnp.asarray(work_compile),         \"arguments\": jnp.asarray(arguments),     }   main() <pre>\n\nForward-mode\nnum = 1 | elapsed = 0.00 | max_time = 0.55\nnum = 2 | elapsed = 0.03 | max_time = 0.55\nnum = 3 | elapsed = 0.05 | max_time = 0.55\nnum = 4 | elapsed = 0.09 | max_time = 0.55\n</pre> <pre>num = 5 | elapsed = 0.17 | max_time = 0.55\n</pre> <pre>num = 6 | elapsed = 0.36 | max_time = 0.55\n</pre> <pre>num = 7 | elapsed = 0.81 | max_time = 0.55\n\n\nTaylor-mode (scan)\nnum = 1 | elapsed = 0.00 | max_time = 0.55\nnum = 2 | elapsed = 0.02 | max_time = 0.55\nnum = 3 | elapsed = 0.10 | max_time = 0.55\n</pre> <pre>num = 4 | elapsed = 0.25 | max_time = 0.55\n</pre> <pre>num = 5 | elapsed = 0.39 | max_time = 0.55\n</pre> <pre>num = 6 | elapsed = 0.41 | max_time = 0.55\n</pre> <pre>num = 7 | elapsed = 0.52 | max_time = 0.55\n</pre> <pre>num = 8 | elapsed = 0.65 | max_time = 0.55\n\n\nTaylor-mode (unroll)\nnum = 1 | elapsed = 0.00 | max_time = 0.55\nnum = 2 | elapsed = 0.02 | max_time = 0.55\nnum = 3 | elapsed = 0.06 | max_time = 0.55\n</pre> <pre>num = 4 | elapsed = 0.11 | max_time = 0.55\nnum = 5 | elapsed = 0.19 | max_time = 0.55\n</pre> <pre>num = 6 | elapsed = 0.27 | max_time = 0.55\n</pre> <pre>num = 7 | elapsed = 0.40 | max_time = 0.55\n</pre> <pre>num = 8 | elapsed = 0.64 | max_time = 0.55\n\n\nTaylor-mode (doubling)\nnum = 1 | elapsed = 0.00 | max_time = 0.55\nnum = 2 | elapsed = 0.16 | max_time = 0.55\n</pre> <pre>num = 3 | elapsed = 1.12 | max_time = 0.55\n</pre>"},{"location":"examples_benchmarks/taylor-init-node/#initialisation-neural-ode","title":"Initialisation: Neural ODE\u00b6","text":""},{"location":"examples_benchmarks/taylor-init-pleiades/","title":"Initialisation: Pleiades","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark the initialisation methods on the Pleiades problem.\"\"\"\n\nimport functools\nimport statistics\nimport time\nimport timeit\nfrom collections.abc import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom probdiffeq import taylor\n\n\ndef main(max_time=0.5, repeats=2):\n    \"\"\"Run the script.\"\"\"\n    # Set JAX config\n    jax.config.update(\"jax_enable_x64\", True)\n\n    algorithms = {\n        r\"Forward-mode\": odejet_via_jvp(),\n        r\"Taylor-mode (scan)\": taylor_mode_scan(),\n        r\"Taylor-mode (unroll)\": taylor_mode_unroll(),\n    }\n\n    # Compute a reference solution\n    timeit_fun = timeit_fun_from_args(repeats=repeats)\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in algorithms.items():\n        print(\"\\n\")\n        print(label)\n        results[label] = adaptive_benchmark(\n            algo, timeit_fun=timeit_fun, max_time=max_time\n        )\n\n    fig, (axis_perform, axis_compile) = plt.subplots(\n        ncols=2, figsize=(8, 3), dpi=150, sharex=True, sharey=True\n    )\n\n    for label, wp in results.items():\n        inputs = wp[\"arguments\"]\n        work_compile = wp[\"work_compile\"]\n        work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]\n\n        if \"doubling\" in label:\n            num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))\n            inputs = jnp.arange(1, jnp.amax(inputs) * 1)\n            work_compile = _adaptive_repeat(work_compile, num_repeats)\n            work_mean = _adaptive_repeat(work_mean, num_repeats)\n            work_std = _adaptive_repeat(work_std, num_repeats)\n\n        axis_compile.semilogy(inputs, work_compile, label=label)\n        axis_perform.semilogy(inputs, work_mean, label=label)\n\n    axis_compile.set_title(\"Compilation time\")\n    axis_perform.set_title(\"Evaluation time\")\n    axis_perform.legend(fontsize=\"small\")\n    axis_compile.legend(fontsize=\"small\")\n    axis_compile.set_xlabel(\"Number of Derivatives\")\n    axis_perform.set_xlabel(\"Number of Derivatives\")\n    axis_perform.set_ylabel(\"Wall time (sec)\")\n    axis_perform.grid(linestyle=\"dotted\")\n    axis_compile.grid(linestyle=\"dotted\")\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef _adaptive_repeat(xs, ys):\n    \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"\n    zs = []\n    for x, y in zip(xs, ys):\n        zs.extend([x] * int(y))\n    return jnp.asarray(zs)\n\n\ndef timeit_fun_from_args(*, repeats: int) -&gt; Callable:\n    \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"\n\n    def timer(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=repeats))\n\n    return timer\n\n\ndef taylor_mode_scan() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0, du0) = _pleiades()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0, du0), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef taylor_mode_unroll() -&gt; Callable:\n    \"\"\"Taylor-mode estimation.\"\"\"\n    vf_auto, (u0, du0) = _pleiades()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_unroll(vf_auto, (u0, du0), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef odejet_via_jvp() -&gt; Callable:\n    \"\"\"Forward-mode estimation.\"\"\"\n    vf_auto, (u0, du0) = _pleiades()\n\n    @functools.partial(jax.jit, static_argnames=[\"num\"])\n    def estimate(num):\n        tcoeffs = taylor.odejet_via_jvp(vf_auto, (u0, du0), num=num)\n        return jnp.asarray(tcoeffs)\n\n    return estimate\n\n\ndef _pleiades():\n    # fmt: off\n    u0 = jnp.asarray(\n        [\n            3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,\n            3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,\n        ]\n    )\n    du0 = jnp.asarray(\n        [\n            0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,\n            0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,\n        ]\n    )\n    # fmt: on\n    t0 = 0.0\n\n    @jax.jit\n    def vf_probdiffeq(u, du, *, t=t0):  # noqa: ARG001\n        \"\"\"Pleiades problem.\"\"\"\n        x = u[0:7]  # x\n        y = u[7:14]  # y\n        xi, xj = x[:, None], x[None, :]\n        yi, yj = y[:, None], y[None, :]\n        rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)\n        mj = jnp.arange(1, 8)[None, :]\n        ddx = jnp.sum(jnp.nan_to_num(mj * (xj - xi) / rij), axis=1)\n        ddy = jnp.sum(jnp.nan_to_num(mj * (yj - yi) / rij), axis=1)\n        return jnp.concatenate((ddx, ddy))\n\n    return vf_probdiffeq, (u0, du0)\n\n\ndef adaptive_benchmark(fun, *, timeit_fun: Callable, max_time) -&gt; dict:\n    \"\"\"Benchmark a function iteratively until a max-time threshold is exceeded.\"\"\"\n    work_compile = []\n    work_mean = []\n    work_std = []\n    arguments = []\n\n    t0 = time.perf_counter()\n    arg = 1\n    while (elapsed := time.perf_counter() - t0) &lt; max_time:\n        print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")\n        t0 = time.perf_counter()\n        tcoeffs = fun(arg).block_until_ready()\n        t1 = time.perf_counter()\n        time_compile = t1 - t0\n\n        time_execute = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023\n\n        arguments.append(len(tcoeffs))\n        work_compile.append(time_compile)\n        work_mean.append(statistics.mean(time_execute))\n        work_std.append(statistics.stdev(time_execute))\n        arg += 1\n    print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")\n    return {\n        \"work_mean\": jnp.asarray(work_mean),\n        \"work_std\": jnp.asarray(work_std),\n        \"work_compile\": jnp.asarray(work_compile),\n        \"arguments\": jnp.asarray(arguments),\n    }\n\n\nmain()\n</pre> \"\"\"Benchmark the initialisation methods on the Pleiades problem.\"\"\"  import functools import statistics import time import timeit from collections.abc import Callable  import jax import jax.numpy as jnp import matplotlib.pyplot as plt  from probdiffeq import taylor   def main(max_time=0.5, repeats=2):     \"\"\"Run the script.\"\"\"     # Set JAX config     jax.config.update(\"jax_enable_x64\", True)      algorithms = {         r\"Forward-mode\": odejet_via_jvp(),         r\"Taylor-mode (scan)\": taylor_mode_scan(),         r\"Taylor-mode (unroll)\": taylor_mode_unroll(),     }      # Compute a reference solution     timeit_fun = timeit_fun_from_args(repeats=repeats)      # Compute all work-precision diagrams     results = {}     for label, algo in algorithms.items():         print(\"\\n\")         print(label)         results[label] = adaptive_benchmark(             algo, timeit_fun=timeit_fun, max_time=max_time         )      fig, (axis_perform, axis_compile) = plt.subplots(         ncols=2, figsize=(8, 3), dpi=150, sharex=True, sharey=True     )      for label, wp in results.items():         inputs = wp[\"arguments\"]         work_compile = wp[\"work_compile\"]         work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]          if \"doubling\" in label:             num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))             inputs = jnp.arange(1, jnp.amax(inputs) * 1)             work_compile = _adaptive_repeat(work_compile, num_repeats)             work_mean = _adaptive_repeat(work_mean, num_repeats)             work_std = _adaptive_repeat(work_std, num_repeats)          axis_compile.semilogy(inputs, work_compile, label=label)         axis_perform.semilogy(inputs, work_mean, label=label)      axis_compile.set_title(\"Compilation time\")     axis_perform.set_title(\"Evaluation time\")     axis_perform.legend(fontsize=\"small\")     axis_compile.legend(fontsize=\"small\")     axis_compile.set_xlabel(\"Number of Derivatives\")     axis_perform.set_xlabel(\"Number of Derivatives\")     axis_perform.set_ylabel(\"Wall time (sec)\")     axis_perform.grid(linestyle=\"dotted\")     axis_compile.grid(linestyle=\"dotted\")      plt.tight_layout()     plt.show()   def _adaptive_repeat(xs, ys):     \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"     zs = []     for x, y in zip(xs, ys):         zs.extend([x] * int(y))     return jnp.asarray(zs)   def timeit_fun_from_args(*, repeats: int) -&gt; Callable:     \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"      def timer(fun, /):         return list(timeit.repeat(fun, number=1, repeat=repeats))      return timer   def taylor_mode_scan() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0, du0) = _pleiades()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0, du0), num=num)         return jnp.asarray(tcoeffs)      return estimate   def taylor_mode_unroll() -&gt; Callable:     \"\"\"Taylor-mode estimation.\"\"\"     vf_auto, (u0, du0) = _pleiades()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_unroll(vf_auto, (u0, du0), num=num)         return jnp.asarray(tcoeffs)      return estimate   def odejet_via_jvp() -&gt; Callable:     \"\"\"Forward-mode estimation.\"\"\"     vf_auto, (u0, du0) = _pleiades()      @functools.partial(jax.jit, static_argnames=[\"num\"])     def estimate(num):         tcoeffs = taylor.odejet_via_jvp(vf_auto, (u0, du0), num=num)         return jnp.asarray(tcoeffs)      return estimate   def _pleiades():     # fmt: off     u0 = jnp.asarray(         [             3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,             3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,         ]     )     du0 = jnp.asarray(         [             0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,             0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,         ]     )     # fmt: on     t0 = 0.0      @jax.jit     def vf_probdiffeq(u, du, *, t=t0):  # noqa: ARG001         \"\"\"Pleiades problem.\"\"\"         x = u[0:7]  # x         y = u[7:14]  # y         xi, xj = x[:, None], x[None, :]         yi, yj = y[:, None], y[None, :]         rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)         mj = jnp.arange(1, 8)[None, :]         ddx = jnp.sum(jnp.nan_to_num(mj * (xj - xi) / rij), axis=1)         ddy = jnp.sum(jnp.nan_to_num(mj * (yj - yi) / rij), axis=1)         return jnp.concatenate((ddx, ddy))      return vf_probdiffeq, (u0, du0)   def adaptive_benchmark(fun, *, timeit_fun: Callable, max_time) -&gt; dict:     \"\"\"Benchmark a function iteratively until a max-time threshold is exceeded.\"\"\"     work_compile = []     work_mean = []     work_std = []     arguments = []      t0 = time.perf_counter()     arg = 1     while (elapsed := time.perf_counter() - t0) &lt; max_time:         print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")         t0 = time.perf_counter()         tcoeffs = fun(arg).block_until_ready()         t1 = time.perf_counter()         time_compile = t1 - t0          time_execute = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023          arguments.append(len(tcoeffs))         work_compile.append(time_compile)         work_mean.append(statistics.mean(time_execute))         work_std.append(statistics.stdev(time_execute))         arg += 1     print(f\"num = {arg} | elapsed = {elapsed:.2f} | max_time = {max_time}\")     return {         \"work_mean\": jnp.asarray(work_mean),         \"work_std\": jnp.asarray(work_std),         \"work_compile\": jnp.asarray(work_compile),         \"arguments\": jnp.asarray(arguments),     }   main() <pre>\n\nForward-mode\nnum = 1 | elapsed = 0.00 | max_time = 0.5\nnum = 2 | elapsed = 0.08 | max_time = 0.5\nnum = 3 | elapsed = 0.12 | max_time = 0.5\n</pre> <pre>num = 4 | elapsed = 0.21 | max_time = 0.5\n</pre> <pre>num = 5 | elapsed = 0.35 | max_time = 0.5\n</pre> <pre>num = 6 | elapsed = 0.64 | max_time = 0.5\n\n\nTaylor-mode (scan)\nnum = 1 | elapsed = 0.00 | max_time = 0.5\nnum = 2 | elapsed = 0.06 | max_time = 0.5\n</pre> <pre>num = 3 | elapsed = 0.16 | max_time = 0.5\n</pre> <pre>num = 4 | elapsed = 0.23 | max_time = 0.5\n</pre> <pre>num = 5 | elapsed = 0.31 | max_time = 0.5\n</pre> <pre>num = 6 | elapsed = 0.40 | max_time = 0.5\n</pre> <pre>num = 7 | elapsed = 0.58 | max_time = 0.5\n\n\nTaylor-mode (unroll)\nnum = 1 | elapsed = 0.00 | max_time = 0.5\nnum = 2 | elapsed = 0.06 | max_time = 0.5\nnum = 3 | elapsed = 0.12 | max_time = 0.5\n</pre> <pre>num = 4 | elapsed = 0.22 | max_time = 0.5\n</pre> <pre>num = 5 | elapsed = 0.31 | max_time = 0.5\n</pre> <pre>num = 6 | elapsed = 0.42 | max_time = 0.5\n</pre> <pre>num = 7 | elapsed = 0.56 | max_time = 0.5\n</pre>"},{"location":"examples_benchmarks/taylor-init-pleiades/#initialisation-pleiades","title":"Initialisation: Pleiades\u00b6","text":""},{"location":"examples_benchmarks/work-precision-hires/","title":"WP diagram: HIRES","text":"In\u00a0[1]: Copied! <pre>\"\"\"HIRES work-precision diagram.\"\"\"\n\nimport functools\nimport statistics\nimport timeit\nfrom collections.abc import Callable\n\nimport diffrax\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.integrate\nimport tqdm\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\n\ndef main(start=1.0, stop=9.0, step=1.0, repeats=2, use_diffrax: bool = False):\n    \"\"\"Run the script.\"\"\"\n    # Set up all the configs\n    jax.config.update(\"jax_enable_x64\", True)\n\n    # Simulate once to plot the state\n    ts, ys = solve_ivp_once()\n\n    fig, ax = plt.subplots(figsize=(5, 3))\n    ax.plot(ts, ys)\n    ax.set_title(\"Hires problem\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"State\")\n    plt.tight_layout()\n    plt.show()\n\n    # Read configuration from command line\n    tolerances = setup_tolerances(start=start, stop=stop, step=step)\n    timeit_fun = setup_timeit(repeats=repeats)\n\n    # Assemble algorithms\n    algorithms = {\n        r\"ProbDiffEq: TS1($3$)\": solver_probdiffeq(num_derivatives=3),\n        r\"ProbDiffEq: TS1($5$)\": solver_probdiffeq(num_derivatives=5),\n        \"SciPy: 'LSODA'\": solver_scipy(method=\"LSODA\"),\n        \"SciPy: 'Radau'\": solver_scipy(method=\"Radau\"),\n    }\n\n    if use_diffrax:\n        # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0\n        # Revisit in the near future.\n        algorithms[\"Diffrax: Kvaerno3()\"] = solver_diffrax(solver=diffrax.Kvaerno3())\n        algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())\n    else:\n        print(\"\\nSkipped Diffrax.\\n\")\n\n    # Compute a reference solution\n    reference = solver_scipy(method=\"BDF\")(1e-13)\n    precision_fun = rmse_relative(reference)\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in tqdm.tqdm(algorithms.items()):\n        param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)\n        results[label] = param_to_wp(tolerances)\n\n    fig, ax = plt.subplots(figsize=(5, 3))\n    for label, wp in results.items():\n        ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)\n\n    ax.set_title(\"Work-precision diagram\")\n    ax.set_xlabel(\"Precision (relative RMSE)\")\n    ax.set_ylabel(\"Work (avg. wall time)\")\n    ax.grid(linestyle=\"dotted\", which=\"both\")\n    ax.legend(fontsize=\"small\")\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef solve_ivp_once():\n    \"\"\"Compute plotting-values for the IVP.\"\"\"\n\n    def vf_scipy(_t, u):\n        \"\"\"High irradiance response.\"\"\"\n        du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007\n        du2 = 1.71 * u[0] - 8.75 * u[1]\n        du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]\n        du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]\n        du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]\n        du6 = (\n            -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]\n        )\n        du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]\n        du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]\n        return np.asarray([du1, du2, du3, du4, du5, du6, du7, du8])\n\n    u0 = np.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])\n    time_span = np.asarray([0.0, 321.8122])\n\n    tol = 1e-12\n    solution = scipy.integrate.solve_ivp(\n        vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"\n    )\n    return solution.t, solution.y.T\n\n\ndef setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:\n    \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"\n    return 0.1 ** jnp.arange(start, stop, step=step)\n\n\ndef setup_timeit(*, repeats: int) -&gt; Callable:\n    \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"\n\n    def timer(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=repeats))\n\n    return timer\n\n\ndef solver_probdiffeq(*, num_derivatives: int) -&gt; Callable:\n    \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"\n\n    @jax.jit\n    def vf_probdiffeq(u, *, t):  # noqa: ARG001\n        \"\"\"High irradiance response.\"\"\"\n        du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007\n        du2 = 1.71 * u[0] - 8.75 * u[1]\n        du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]\n        du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]\n        du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]\n        du6 = (\n            -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]\n        )\n        du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]\n        du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]\n        return jnp.asarray([du1, du2, du3, du4, du5, du6, du7, du8])\n\n    u0 = jnp.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])\n    t0, t1 = jnp.asarray([0.0, 321.8122])\n\n    @jax.jit\n    def param_to_solution(tol):\n        # Build a solver\n        vf_auto = functools.partial(vf_probdiffeq, t=t0)\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num_derivatives)\n        init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")\n        ts1 = ivpsolvers.correction_ts1(vf_probdiffeq, ssm=ssm)\n        strategy = ivpsolvers.strategy_filter(ssm=ssm)\n        solver = ivpsolvers.solver_dynamic(strategy, prior=ibm, correction=ts1, ssm=ssm)\n        control = ivpsolvers.control_proportional_integral()\n        adaptive_solver = ivpsolvers.adaptive(\n            solver, atol=1e-2 * tol, rtol=tol, control=control, ssm=ssm, clip_dt=True\n        )\n\n        # Solve\n        dt0 = ivpsolve.dt0(vf_auto, (u0,))\n        solution = ivpsolve.solve_adaptive_terminal_values(\n            init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n\n        # Return the terminal value\n        return jax.block_until_ready(solution.u[0])\n\n    return param_to_solution\n\n\ndef solver_diffrax(*, solver) -&gt; Callable:\n    \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"\n\n    @diffrax.ODETerm\n    @jax.jit\n    def vf_diffrax(_t, u, _args):\n        \"\"\"High irradiance response.\"\"\"\n        du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007\n        du2 = 1.71 * u[0] - 8.75 * u[1]\n        du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]\n        du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]\n        du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]\n        du6 = (\n            -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]\n        )\n        du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]\n        du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]\n        return jnp.asarray([du1, du2, du3, du4, du5, du6, du7, du8])\n\n    u0 = jnp.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])\n    t0, t1 = jnp.asarray([0.0, 321.8122])\n\n    @jax.jit\n    def param_to_solution(tol):\n        controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)\n        saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)\n        solution = diffrax.diffeqsolve(\n            vf_diffrax,\n            y0=u0,\n            t0=t0,\n            t1=t1,\n            saveat=saveat,\n            stepsize_controller=controller,\n            dt0=None,\n            max_steps=10_000,\n            solver=solver,\n        )\n        return jax.block_until_ready(solution.ys[0, :])\n\n    return param_to_solution\n\n\ndef solver_scipy(*, method: str) -&gt; Callable:\n    \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"\n\n    def vf_scipy(_t, u):\n        \"\"\"High irradiance response.\"\"\"\n        du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007\n        du2 = 1.71 * u[0] - 8.75 * u[1]\n        du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]\n        du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]\n        du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]\n        du6 = (\n            -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]\n        )\n        du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]\n        du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]\n        return np.asarray([du1, du2, du3, du4, du5, du6, du7, du8])\n\n    u0 = np.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])\n    time_span = np.asarray([0.0, 321.8122])\n\n    def param_to_solution(tol):\n        solution = scipy.integrate.solve_ivp(\n            vf_scipy,\n            y0=u0,\n            t_span=time_span,\n            t_eval=time_span,\n            atol=1e-3 * tol,\n            rtol=tol,\n            method=method,\n        )\n        return jnp.asarray(solution.y[:, -1])\n\n    return param_to_solution\n\n\ndef rmse_relative(expected: jax.Array, *, nugget=1e-5) -&gt; Callable:\n    \"\"\"Compute the relative RMSE.\"\"\"\n    expected = jnp.asarray(expected)\n\n    def rmse(received):\n        received = jnp.asarray(received)\n        error_absolute = jnp.abs(expected - received)\n        error_relative = error_absolute / jnp.abs(nugget + expected)\n        return jnp.linalg.norm(error_relative) / jnp.sqrt(error_relative.size)\n\n    return rmse\n\n\ndef workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:\n    \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"\n\n    def parameter_list_to_workprecision(list_of_args, /):\n        works_mean = []\n        works_std = []\n        precisions = []\n        for arg in list_of_args:\n            precision = precision_fun(fun(arg).block_until_ready())\n            times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023\n\n            precisions.append(precision)\n            works_mean.append(statistics.mean(times))\n            works_std.append(statistics.stdev(times))\n        return {\n            \"work_mean\": jnp.asarray(works_mean),\n            \"work_std\": jnp.asarray(works_std),\n            \"precision\": jnp.asarray(precisions),\n        }\n\n    return parameter_list_to_workprecision\n\n\nmain()\n</pre> \"\"\"HIRES work-precision diagram.\"\"\"  import functools import statistics import timeit from collections.abc import Callable  import diffrax import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import scipy.integrate import tqdm  from probdiffeq import ivpsolve, ivpsolvers, taylor   def main(start=1.0, stop=9.0, step=1.0, repeats=2, use_diffrax: bool = False):     \"\"\"Run the script.\"\"\"     # Set up all the configs     jax.config.update(\"jax_enable_x64\", True)      # Simulate once to plot the state     ts, ys = solve_ivp_once()      fig, ax = plt.subplots(figsize=(5, 3))     ax.plot(ts, ys)     ax.set_title(\"Hires problem\")     ax.set_xlabel(\"Time\")     ax.set_ylabel(\"State\")     plt.tight_layout()     plt.show()      # Read configuration from command line     tolerances = setup_tolerances(start=start, stop=stop, step=step)     timeit_fun = setup_timeit(repeats=repeats)      # Assemble algorithms     algorithms = {         r\"ProbDiffEq: TS1($3$)\": solver_probdiffeq(num_derivatives=3),         r\"ProbDiffEq: TS1($5$)\": solver_probdiffeq(num_derivatives=5),         \"SciPy: 'LSODA'\": solver_scipy(method=\"LSODA\"),         \"SciPy: 'Radau'\": solver_scipy(method=\"Radau\"),     }      if use_diffrax:         # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0         # Revisit in the near future.         algorithms[\"Diffrax: Kvaerno3()\"] = solver_diffrax(solver=diffrax.Kvaerno3())         algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())     else:         print(\"\\nSkipped Diffrax.\\n\")      # Compute a reference solution     reference = solver_scipy(method=\"BDF\")(1e-13)     precision_fun = rmse_relative(reference)      # Compute all work-precision diagrams     results = {}     for label, algo in tqdm.tqdm(algorithms.items()):         param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)         results[label] = param_to_wp(tolerances)      fig, ax = plt.subplots(figsize=(5, 3))     for label, wp in results.items():         ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)      ax.set_title(\"Work-precision diagram\")     ax.set_xlabel(\"Precision (relative RMSE)\")     ax.set_ylabel(\"Work (avg. wall time)\")     ax.grid(linestyle=\"dotted\", which=\"both\")     ax.legend(fontsize=\"small\")      plt.tight_layout()     plt.show()   def solve_ivp_once():     \"\"\"Compute plotting-values for the IVP.\"\"\"      def vf_scipy(_t, u):         \"\"\"High irradiance response.\"\"\"         du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007         du2 = 1.71 * u[0] - 8.75 * u[1]         du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]         du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]         du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]         du6 = (             -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]         )         du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]         du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]         return np.asarray([du1, du2, du3, du4, du5, du6, du7, du8])      u0 = np.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])     time_span = np.asarray([0.0, 321.8122])      tol = 1e-12     solution = scipy.integrate.solve_ivp(         vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"     )     return solution.t, solution.y.T   def setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:     \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"     return 0.1 ** jnp.arange(start, stop, step=step)   def setup_timeit(*, repeats: int) -&gt; Callable:     \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"      def timer(fun, /):         return list(timeit.repeat(fun, number=1, repeat=repeats))      return timer   def solver_probdiffeq(*, num_derivatives: int) -&gt; Callable:     \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"      @jax.jit     def vf_probdiffeq(u, *, t):  # noqa: ARG001         \"\"\"High irradiance response.\"\"\"         du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007         du2 = 1.71 * u[0] - 8.75 * u[1]         du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]         du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]         du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]         du6 = (             -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]         )         du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]         du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]         return jnp.asarray([du1, du2, du3, du4, du5, du6, du7, du8])      u0 = jnp.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])     t0, t1 = jnp.asarray([0.0, 321.8122])      @jax.jit     def param_to_solution(tol):         # Build a solver         vf_auto = functools.partial(vf_probdiffeq, t=t0)         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num_derivatives)         init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")         ts1 = ivpsolvers.correction_ts1(vf_probdiffeq, ssm=ssm)         strategy = ivpsolvers.strategy_filter(ssm=ssm)         solver = ivpsolvers.solver_dynamic(strategy, prior=ibm, correction=ts1, ssm=ssm)         control = ivpsolvers.control_proportional_integral()         adaptive_solver = ivpsolvers.adaptive(             solver, atol=1e-2 * tol, rtol=tol, control=control, ssm=ssm, clip_dt=True         )          # Solve         dt0 = ivpsolve.dt0(vf_auto, (u0,))         solution = ivpsolve.solve_adaptive_terminal_values(             init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm         )          # Return the terminal value         return jax.block_until_ready(solution.u[0])      return param_to_solution   def solver_diffrax(*, solver) -&gt; Callable:     \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"      @diffrax.ODETerm     @jax.jit     def vf_diffrax(_t, u, _args):         \"\"\"High irradiance response.\"\"\"         du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007         du2 = 1.71 * u[0] - 8.75 * u[1]         du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]         du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]         du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]         du6 = (             -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]         )         du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]         du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]         return jnp.asarray([du1, du2, du3, du4, du5, du6, du7, du8])      u0 = jnp.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])     t0, t1 = jnp.asarray([0.0, 321.8122])      @jax.jit     def param_to_solution(tol):         controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)         saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)         solution = diffrax.diffeqsolve(             vf_diffrax,             y0=u0,             t0=t0,             t1=t1,             saveat=saveat,             stepsize_controller=controller,             dt0=None,             max_steps=10_000,             solver=solver,         )         return jax.block_until_ready(solution.ys[0, :])      return param_to_solution   def solver_scipy(*, method: str) -&gt; Callable:     \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"      def vf_scipy(_t, u):         \"\"\"High irradiance response.\"\"\"         du1 = -1.71 * u[0] + 0.43 * u[1] + 8.32 * u[2] + 0.0007         du2 = 1.71 * u[0] - 8.75 * u[1]         du3 = -10.03 * u[2] + 0.43 * u[3] + 0.035 * u[4]         du4 = 8.32 * u[1] + 1.71 * u[2] - 1.12 * u[3]         du5 = -1.745 * u[4] + 0.43 * u[5] + 0.43 * u[6]         du6 = (             -280.0 * u[5] * u[7] + 0.69 * u[3] + 1.71 * u[4] - 0.43 * u[5] + 0.69 * u[6]         )         du7 = 280.0 * u[5] * u[7] - 1.81 * u[6]         du8 = -280.0 * u[5] * u[7] + 1.81 * u[6]         return np.asarray([du1, du2, du3, du4, du5, du6, du7, du8])      u0 = np.asarray([1.0, 0.0, 0.0, 0, 0, 0, 0, 0.0057])     time_span = np.asarray([0.0, 321.8122])      def param_to_solution(tol):         solution = scipy.integrate.solve_ivp(             vf_scipy,             y0=u0,             t_span=time_span,             t_eval=time_span,             atol=1e-3 * tol,             rtol=tol,             method=method,         )         return jnp.asarray(solution.y[:, -1])      return param_to_solution   def rmse_relative(expected: jax.Array, *, nugget=1e-5) -&gt; Callable:     \"\"\"Compute the relative RMSE.\"\"\"     expected = jnp.asarray(expected)      def rmse(received):         received = jnp.asarray(received)         error_absolute = jnp.abs(expected - received)         error_relative = error_absolute / jnp.abs(nugget + expected)         return jnp.linalg.norm(error_relative) / jnp.sqrt(error_relative.size)      return rmse   def workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:     \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"      def parameter_list_to_workprecision(list_of_args, /):         works_mean = []         works_std = []         precisions = []         for arg in list_of_args:             precision = precision_fun(fun(arg).block_until_ready())             times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023              precisions.append(precision)             works_mean.append(statistics.mean(times))             works_std.append(statistics.stdev(times))         return {             \"work_mean\": jnp.asarray(works_mean),             \"work_std\": jnp.asarray(works_std),             \"precision\": jnp.asarray(precisions),         }      return parameter_list_to_workprecision   main() <pre>\nSkipped Diffrax.\n\n</pre> <pre>\r  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>\r 25%|\u2588\u2588\u258c       | 1/4 [00:02&lt;00:06,  2.29s/it]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:04&lt;00:04,  2.21s/it]</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:04&lt;00:01,  1.34s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:08&lt;00:00,  2.17s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:08&lt;00:00,  2.05s/it]</pre> <pre>\n</pre>"},{"location":"examples_benchmarks/work-precision-hires/#wp-diagram-hires","title":"WP diagram: HIRES\u00b6","text":""},{"location":"examples_benchmarks/work-precision-lotka-volterra/","title":"WP diagram: Lotka-Volterra","text":"In\u00a0[1]: Copied! <pre>\"\"\"Lotka-Volterra work-precision diagram.\"\"\"\n\nimport functools\nimport statistics\nimport timeit\nfrom collections.abc import Callable\n\nimport diffrax\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.integrate\nimport tqdm\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\n\ndef main(start=3.0, stop=12.0, step=1.0, repeats=2, use_diffrax: bool = False):\n    \"\"\"Run the script.\"\"\"\n    # Set up all the configs\n    jax.config.update(\"jax_enable_x64\", True)\n\n    # Simulate once to plot the state\n    ts, ys = solve_ivp_once()\n\n    fig, ax = plt.subplots(figsize=(5, 3))\n    ax.plot(ts, ys)\n    ax.set_title(\"Lotka-Volterra problem\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"State\")\n    plt.tight_layout()\n    plt.show()\n\n    # Read configuration from command line\n    tolerances = setup_tolerances(start=start, stop=stop, step=step)\n    timeit_fun = setup_timeit(repeats=repeats)\n\n    # Assemble algorithms\n    ts0, ts1 = ivpsolvers.correction_ts0, ivpsolvers.correction_ts1\n    ts0_iso = solver_probdiffeq(5, correction=ts0, implementation=\"isotropic\")\n    ts0_bd = solver_probdiffeq(5, correction=ts0, implementation=\"blockdiag\")\n    ts1_dense = solver_probdiffeq(8, correction=ts1, implementation=\"dense\")\n    algorithms = {\n        r\"ProbDiffEq: TS0($5$, isotropic)\": ts0_iso,\n        r\"ProbDiffEq: TS0($5$, blockdiag)\": ts0_bd,\n        r\"ProbDiffEq: TS1($8$, dense)\": ts1_dense,\n        \"Diffrax: Tsit5()\": solver_diffrax(solver=diffrax.Tsit5()),\n        \"Diffrax: Dopri8()\": solver_diffrax(solver=diffrax.Dopri8()),\n        \"SciPy: 'RK45'\": solver_scipy(method=\"RK45\"),\n        \"SciPy: 'DOP853'\": solver_scipy(method=\"DOP853\"),\n    }\n\n    if use_diffrax:\n        # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0\n        # Revisit in the near future.\n        algorithms[\"Diffrax: Kvaerno3()\"] = solver_diffrax(solver=diffrax.Kvaerno3())\n        algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())\n    else:\n        print(\"\\nSkipped Diffrax.\\n\")\n\n    # Compute a reference solution\n    reference = solver_scipy(method=\"BDF\")(1e-13)\n    precision_fun = rmse_relative(reference)\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in tqdm.tqdm(algorithms.items()):\n        param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)\n        results[label] = param_to_wp(tolerances)\n\n    fig, ax = plt.subplots(figsize=(7, 3))\n    for label, wp in results.items():\n        ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)\n\n    ax.set_title(\"Work-precision diagram\")\n    ax.set_xlabel(\"Precision (relative RMSE)\")\n    ax.set_ylabel(\"Work (avg. wall time)\")\n    ax.grid(linestyle=\"dotted\", which=\"both\")\n    ax.legend(fontsize=\"small\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef solve_ivp_once():\n    \"\"\"Compute plotting-values for the IVP.\"\"\"\n\n    def vf_scipy(_t, y):\n        \"\"\"Lotka--Volterra dynamics.\"\"\"\n        dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]\n        dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]\n        return np.asarray([dy1, dy2])\n\n    u0 = jnp.asarray((20.0, 20.0))\n    time_span = np.asarray([0.0, 50.0])\n    tol = 1e-12\n    solution = scipy.integrate.solve_ivp(\n        vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"\n    )\n    return solution.t, solution.y.T\n\n\ndef setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:\n    \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"\n    return 0.1 ** jnp.arange(start, stop, step=step)\n\n\ndef setup_timeit(*, repeats: int) -&gt; Callable:\n    \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"\n\n    def timer(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=repeats))\n\n    return timer\n\n\ndef solver_probdiffeq(num_derivatives: int, implementation, correction) -&gt; Callable:\n    \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"\n\n    @jax.jit\n    def vf_probdiffeq(y, *, t):  # noqa: ARG001\n        \"\"\"Lotka--Volterra dynamics.\"\"\"\n        dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]\n        dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]\n        return jnp.asarray([dy1, dy2])\n\n    u0 = jnp.asarray((20.0, 20.0))\n    t0, t1 = (0.0, 50.0)\n\n    @jax.jit\n    def param_to_solution(tol):\n        # Build a solver\n        vf_auto = functools.partial(vf_probdiffeq, t=t0)\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num_derivatives)\n\n        init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n            tcoeffs, ssm_fact=implementation\n        )\n        strategy = ivpsolvers.strategy_filter(ssm=ssm)\n        corr = correction(vf_probdiffeq, ssm=ssm)\n        solver = ivpsolvers.solver_mle(strategy, prior=ibm, correction=corr, ssm=ssm)\n        control = ivpsolvers.control_proportional_integral()\n        adaptive_solver = ivpsolvers.adaptive(\n            solver, atol=1e-2 * tol, rtol=tol, control=control, ssm=ssm\n        )\n\n        # Solve\n        dt0 = ivpsolve.dt0(vf_auto, (u0,))\n        solution = ivpsolve.solve_adaptive_terminal_values(\n            init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n\n        # Return the terminal value\n        return jax.block_until_ready(solution.u[0])\n\n    return param_to_solution\n\n\ndef solver_diffrax(*, solver) -&gt; Callable:\n    \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"\n\n    @diffrax.ODETerm\n    @jax.jit\n    def vf_diffrax(_t, y, _args):\n        \"\"\"Lotka--Volterra dynamics.\"\"\"\n        dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]\n        dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]\n        return jnp.asarray([dy1, dy2])\n\n    u0 = jnp.asarray((20.0, 20.0))\n    t0, t1 = (0.0, 50.0)\n\n    @jax.jit\n    def param_to_solution(tol):\n        controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)\n        saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)\n        solution = diffrax.diffeqsolve(\n            vf_diffrax,\n            y0=u0,\n            t0=t0,\n            t1=t1,\n            saveat=saveat,\n            stepsize_controller=controller,\n            dt0=None,\n            max_steps=10_000,\n            solver=solver,\n        )\n        return jax.block_until_ready(solution.ys[0, :])\n\n    return param_to_solution\n\n\ndef solver_scipy(*, method: str) -&gt; Callable:\n    \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"\n\n    def vf_scipy(_t, y):\n        \"\"\"Lotka--Volterra dynamics.\"\"\"\n        dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]\n        dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]\n        return np.asarray([dy1, dy2])\n\n    u0 = jnp.asarray((20.0, 20.0))\n    time_span = np.asarray([0.0, 50.0])\n\n    def param_to_solution(tol):\n        solution = scipy.integrate.solve_ivp(\n            vf_scipy,\n            y0=u0,\n            t_span=time_span,\n            t_eval=time_span,\n            atol=1e-3 * tol,\n            rtol=tol,\n            method=method,\n        )\n        return jnp.asarray(solution.y[:, -1])\n\n    return param_to_solution\n\n\ndef rmse_relative(expected: jax.Array, *, nugget=1e-5) -&gt; Callable:\n    \"\"\"Compute the relative RMSE.\"\"\"\n    expected = jnp.asarray(expected)\n\n    def rmse(received):\n        received = jnp.asarray(received)\n        error_absolute = jnp.abs(expected - received)\n        error_relative = error_absolute / jnp.abs(nugget + expected)\n        return jnp.linalg.norm(error_relative) / jnp.sqrt(error_relative.size)\n\n    return rmse\n\n\ndef workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:\n    \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"\n\n    def parameter_list_to_workprecision(list_of_args, /):\n        works_mean = []\n        works_std = []\n        precisions = []\n        for arg in list_of_args:\n            precision = precision_fun(fun(arg).block_until_ready())\n            times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023\n\n            precisions.append(precision)\n            works_mean.append(statistics.mean(times))\n            works_std.append(statistics.stdev(times))\n        return {\n            \"work_mean\": jnp.asarray(works_mean),\n            \"work_std\": jnp.asarray(works_std),\n            \"precision\": jnp.asarray(precisions),\n        }\n\n    return parameter_list_to_workprecision\n\n\nmain()\n</pre> \"\"\"Lotka-Volterra work-precision diagram.\"\"\"  import functools import statistics import timeit from collections.abc import Callable  import diffrax import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import scipy.integrate import tqdm  from probdiffeq import ivpsolve, ivpsolvers, taylor   def main(start=3.0, stop=12.0, step=1.0, repeats=2, use_diffrax: bool = False):     \"\"\"Run the script.\"\"\"     # Set up all the configs     jax.config.update(\"jax_enable_x64\", True)      # Simulate once to plot the state     ts, ys = solve_ivp_once()      fig, ax = plt.subplots(figsize=(5, 3))     ax.plot(ts, ys)     ax.set_title(\"Lotka-Volterra problem\")     ax.set_xlabel(\"Time\")     ax.set_ylabel(\"State\")     plt.tight_layout()     plt.show()      # Read configuration from command line     tolerances = setup_tolerances(start=start, stop=stop, step=step)     timeit_fun = setup_timeit(repeats=repeats)      # Assemble algorithms     ts0, ts1 = ivpsolvers.correction_ts0, ivpsolvers.correction_ts1     ts0_iso = solver_probdiffeq(5, correction=ts0, implementation=\"isotropic\")     ts0_bd = solver_probdiffeq(5, correction=ts0, implementation=\"blockdiag\")     ts1_dense = solver_probdiffeq(8, correction=ts1, implementation=\"dense\")     algorithms = {         r\"ProbDiffEq: TS0($5$, isotropic)\": ts0_iso,         r\"ProbDiffEq: TS0($5$, blockdiag)\": ts0_bd,         r\"ProbDiffEq: TS1($8$, dense)\": ts1_dense,         \"Diffrax: Tsit5()\": solver_diffrax(solver=diffrax.Tsit5()),         \"Diffrax: Dopri8()\": solver_diffrax(solver=diffrax.Dopri8()),         \"SciPy: 'RK45'\": solver_scipy(method=\"RK45\"),         \"SciPy: 'DOP853'\": solver_scipy(method=\"DOP853\"),     }      if use_diffrax:         # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0         # Revisit in the near future.         algorithms[\"Diffrax: Kvaerno3()\"] = solver_diffrax(solver=diffrax.Kvaerno3())         algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())     else:         print(\"\\nSkipped Diffrax.\\n\")      # Compute a reference solution     reference = solver_scipy(method=\"BDF\")(1e-13)     precision_fun = rmse_relative(reference)      # Compute all work-precision diagrams     results = {}     for label, algo in tqdm.tqdm(algorithms.items()):         param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)         results[label] = param_to_wp(tolerances)      fig, ax = plt.subplots(figsize=(7, 3))     for label, wp in results.items():         ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)      ax.set_title(\"Work-precision diagram\")     ax.set_xlabel(\"Precision (relative RMSE)\")     ax.set_ylabel(\"Work (avg. wall time)\")     ax.grid(linestyle=\"dotted\", which=\"both\")     ax.legend(fontsize=\"small\", loc=\"center left\", bbox_to_anchor=(1, 0.5))      plt.tight_layout()     plt.show()   def solve_ivp_once():     \"\"\"Compute plotting-values for the IVP.\"\"\"      def vf_scipy(_t, y):         \"\"\"Lotka--Volterra dynamics.\"\"\"         dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]         dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]         return np.asarray([dy1, dy2])      u0 = jnp.asarray((20.0, 20.0))     time_span = np.asarray([0.0, 50.0])     tol = 1e-12     solution = scipy.integrate.solve_ivp(         vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"     )     return solution.t, solution.y.T   def setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:     \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"     return 0.1 ** jnp.arange(start, stop, step=step)   def setup_timeit(*, repeats: int) -&gt; Callable:     \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"      def timer(fun, /):         return list(timeit.repeat(fun, number=1, repeat=repeats))      return timer   def solver_probdiffeq(num_derivatives: int, implementation, correction) -&gt; Callable:     \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"      @jax.jit     def vf_probdiffeq(y, *, t):  # noqa: ARG001         \"\"\"Lotka--Volterra dynamics.\"\"\"         dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]         dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]         return jnp.asarray([dy1, dy2])      u0 = jnp.asarray((20.0, 20.0))     t0, t1 = (0.0, 50.0)      @jax.jit     def param_to_solution(tol):         # Build a solver         vf_auto = functools.partial(vf_probdiffeq, t=t0)         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0,), num=num_derivatives)          init, ibm, ssm = ivpsolvers.prior_wiener_integrated(             tcoeffs, ssm_fact=implementation         )         strategy = ivpsolvers.strategy_filter(ssm=ssm)         corr = correction(vf_probdiffeq, ssm=ssm)         solver = ivpsolvers.solver_mle(strategy, prior=ibm, correction=corr, ssm=ssm)         control = ivpsolvers.control_proportional_integral()         adaptive_solver = ivpsolvers.adaptive(             solver, atol=1e-2 * tol, rtol=tol, control=control, ssm=ssm         )          # Solve         dt0 = ivpsolve.dt0(vf_auto, (u0,))         solution = ivpsolve.solve_adaptive_terminal_values(             init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm         )          # Return the terminal value         return jax.block_until_ready(solution.u[0])      return param_to_solution   def solver_diffrax(*, solver) -&gt; Callable:     \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"      @diffrax.ODETerm     @jax.jit     def vf_diffrax(_t, y, _args):         \"\"\"Lotka--Volterra dynamics.\"\"\"         dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]         dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]         return jnp.asarray([dy1, dy2])      u0 = jnp.asarray((20.0, 20.0))     t0, t1 = (0.0, 50.0)      @jax.jit     def param_to_solution(tol):         controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)         saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)         solution = diffrax.diffeqsolve(             vf_diffrax,             y0=u0,             t0=t0,             t1=t1,             saveat=saveat,             stepsize_controller=controller,             dt0=None,             max_steps=10_000,             solver=solver,         )         return jax.block_until_ready(solution.ys[0, :])      return param_to_solution   def solver_scipy(*, method: str) -&gt; Callable:     \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"      def vf_scipy(_t, y):         \"\"\"Lotka--Volterra dynamics.\"\"\"         dy1 = 0.5 * y[0] - 0.05 * y[0] * y[1]         dy2 = -0.5 * y[1] + 0.05 * y[0] * y[1]         return np.asarray([dy1, dy2])      u0 = jnp.asarray((20.0, 20.0))     time_span = np.asarray([0.0, 50.0])      def param_to_solution(tol):         solution = scipy.integrate.solve_ivp(             vf_scipy,             y0=u0,             t_span=time_span,             t_eval=time_span,             atol=1e-3 * tol,             rtol=tol,             method=method,         )         return jnp.asarray(solution.y[:, -1])      return param_to_solution   def rmse_relative(expected: jax.Array, *, nugget=1e-5) -&gt; Callable:     \"\"\"Compute the relative RMSE.\"\"\"     expected = jnp.asarray(expected)      def rmse(received):         received = jnp.asarray(received)         error_absolute = jnp.abs(expected - received)         error_relative = error_absolute / jnp.abs(nugget + expected)         return jnp.linalg.norm(error_relative) / jnp.sqrt(error_relative.size)      return rmse   def workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:     \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"      def parameter_list_to_workprecision(list_of_args, /):         works_mean = []         works_std = []         precisions = []         for arg in list_of_args:             precision = precision_fun(fun(arg).block_until_ready())             times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023              precisions.append(precision)             works_mean.append(statistics.mean(times))             works_std.append(statistics.stdev(times))         return {             \"work_mean\": jnp.asarray(works_mean),             \"work_std\": jnp.asarray(works_std),             \"precision\": jnp.asarray(precisions),         }      return parameter_list_to_workprecision   main() <pre>\nSkipped Diffrax.\n\n</pre> <pre>\r  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> <pre>\r 14%|\u2588\u258d        | 1/7 [00:01&lt;00:08,  1.34s/it]</pre> <pre>\r 29%|\u2588\u2588\u258a       | 2/7 [00:02&lt;00:05,  1.19s/it]</pre> <pre>\r 43%|\u2588\u2588\u2588\u2588\u258e     | 3/7 [00:03&lt;00:05,  1.31s/it]</pre> <pre>\r 57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4/7 [00:04&lt;00:03,  1.00s/it]</pre> <pre>\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 5/7 [00:04&lt;00:01,  1.30it/s]</pre> <pre>\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:06&lt;00:00,  1.07it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:06&lt;00:00,  1.26it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:06&lt;00:00,  1.07it/s]</pre> <pre>\n</pre>"},{"location":"examples_benchmarks/work-precision-lotka-volterra/#wp-diagram-lotka-volterra","title":"WP diagram: Lotka-Volterra\u00b6","text":""},{"location":"examples_benchmarks/work-precision-pleiades/","title":"WP diagram: Pleiades","text":"In\u00a0[1]: Copied! <pre>\"\"\"Pleiades work-precision diagram.\"\"\"\n\nimport functools\nimport statistics\nimport timeit\nfrom collections.abc import Callable\n\nimport diffrax\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numba\nimport numpy as np\nimport scipy.integrate\nimport tqdm\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\n\ndef main(start=3.0, stop=11.0, step=1.0, repeats=2, use_diffrax: bool = False):\n    \"\"\"Run the script.\"\"\"\n    # Set up all the configs\n    jax.config.update(\"jax_enable_x64\", True)\n\n    # Simulate once to plot the state\n    ts, ys = solve_ivp_once()\n\n    fig, ax = plt.subplots(figsize=(5, 3))\n    ax.plot(ys[:, :7], ys[:, 7:14], linestyle=\"solid\", marker=\"None\")\n    ax.plot(ys[0, :7], ys[0, 7:14], linestyle=\"None\", marker=\".\", markersize=4)\n    ax.plot(ys[-1, :7], ys[-1, 7:14], linestyle=\"None\", marker=\"*\", markersize=8)\n\n    ax.set_title(\"Pleiades problem\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"State\")\n    plt.tight_layout()\n    plt.show()\n\n    # Read configuration from command line\n    tolerances = setup_tolerances(start=start, stop=stop, step=step)\n    timeit_fun = setup_timeit(repeats=repeats)\n\n    # Assemble algorithms\n    algorithms = {\n        r\"ProbDiffEq: TS0($5$)\": solver_probdiffeq(\n            num_derivatives=5, correction_fun=ivpsolvers.correction_ts0\n        ),\n        r\"ProbDiffEq: TS0($8$)\": solver_probdiffeq(\n            num_derivatives=8, correction_fun=ivpsolvers.correction_ts0\n        ),\n        \"SciPy: 'RK45'\": solver_scipy(method=\"RK45\", use_numba=False),\n        \"SciPy: 'DOP853'\": solver_scipy(method=\"DOP853\", use_numba=False),\n        \"SciPy: 'RK45' (+numba)\": solver_scipy(method=\"RK45\", use_numba=True),\n        \"SciPy: 'DOP853' (+numba)\": solver_scipy(method=\"DOP853\", use_numba=True),\n        \"Diffrax: Tsit5()\": solver_diffrax(solver=diffrax.Tsit5()),\n        \"Diffrax: Dopri8()\": solver_diffrax(solver=diffrax.Dopri8()),\n    }\n\n    if use_diffrax:\n        # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0\n        # Revisit in the near future.\n        algorithms[\"Diffrax: Kvaerno3()\"] = solver_diffrax(solver=diffrax.Kvaerno3())\n        algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())\n    else:\n        print(\"\\nSkipped Diffrax.\\n\")\n\n    # Compute a reference solution\n    reference = solver_scipy(method=\"LSODA\", use_numba=False)(1e-14)\n    precision_fun = rmse_absolute(reference)\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in tqdm.tqdm(algorithms.items()):\n        param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)\n        results[label] = param_to_wp(tolerances)\n\n    fig, ax = plt.subplots(figsize=(7, 3))\n    for label, wp in results.items():\n        ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)\n\n    ax.set_title(\"Work-precision diagram\")\n    ax.set_xlabel(\"Precision (relative RMSE)\")\n    ax.set_ylabel(\"Work (avg. wall time)\")\n    ax.grid(linestyle=\"dotted\", which=\"both\")\n    ax.legend(fontsize=\"small\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef solve_ivp_once():\n    \"\"\"Compute plotting-values for the IVP.\"\"\"\n    # fmt: off\n    u0 = np.asarray(\n        [\n            3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,\n            3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,\n            0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,\n            0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,\n        ]\n    )\n    # fmt: on\n\n    def vf_scipy(_t, u):\n        \"\"\"Pleiades problem.\"\"\"\n        x = u[0:7]  # x\n        y = u[7:14]  # y\n        xi, xj = x[:, None], x[None, :]\n        yi, yj = y[:, None], y[None, :]\n        rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)\n        mj = np.arange(1, 8)[None, :]\n\n        # Explicitly avoid dividing by zero for scipy's solver\n        # The JAX solvers divide by zero and turn the NaNs to zeros.\n        rij = np.where(rij == 0.0, 1.0, rij)\n        ddx = np.sum((mj * (xj - xi) / rij), axis=1)\n        ddy = np.sum((mj * (yj - yi) / rij), axis=1)\n        return np.concatenate((u[14:21], u[21:28], ddx, ddy))\n\n    time_span = np.asarray([0.0, 3.0])\n\n    tol = 1e-12\n    solution = scipy.integrate.solve_ivp(\n        vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"\n    )\n\n    return solution.t, solution.y.T\n\n\ndef setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:\n    \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"\n    return 0.1 ** jnp.arange(start, stop, step=step)\n\n\ndef setup_timeit(*, repeats: int) -&gt; Callable:\n    \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"\n\n    def timer(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=repeats))\n\n    return timer\n\n\ndef solver_probdiffeq(*, num_derivatives: int, correction_fun) -&gt; Callable:\n    \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"\n    # fmt: off\n    u0 = jnp.asarray(\n        [\n            3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,\n            3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,\n        ]\n    )\n    du0 = jnp.asarray(\n        [\n            0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,\n            0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,\n        ]\n    )\n    # fmt: on\n\n    @jax.jit\n    def vf_probdiffeq(u, du, *, t):  # noqa: ARG001\n        \"\"\"Pleiades problem.\"\"\"\n        x = u[0:7]  # x\n        y = u[7:14]  # y\n        xi, xj = x[:, None], x[None, :]\n        yi, yj = y[:, None], y[None, :]\n        rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)\n        mj = jnp.arange(1, 8)[None, :]\n        ddx = jnp.sum(jnp.nan_to_num(mj * (xj - xi) / rij), axis=1)\n        ddy = jnp.sum(jnp.nan_to_num(mj * (yj - yi) / rij), axis=1)\n        return jnp.concatenate((ddx, ddy))\n\n    t0, t1 = 0.0, 3.0\n\n    @jax.jit\n    def param_to_solution(tol):\n        # Build a solver\n        vf_auto = functools.partial(vf_probdiffeq, t=t0)\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0, du0), num=num_derivatives - 1)\n\n        init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n            tcoeffs, ssm_fact=\"isotropic\"\n        )\n        ts0_or_ts1 = correction_fun(vf_probdiffeq, ssm=ssm, ode_order=2)\n        strategy = ivpsolvers.strategy_filter(ssm=ssm)\n        solver = ivpsolvers.solver_dynamic(\n            strategy, prior=ibm, correction=ts0_or_ts1, ssm=ssm\n        )\n        control = ivpsolvers.control_proportional_integral()\n        adaptive_solver = ivpsolvers.adaptive(\n            solver, atol=1e-3 * tol, rtol=tol, control=control, ssm=ssm\n        )\n\n        # Solve\n        dt0 = ivpsolve.dt0(vf_auto, (u0, du0))\n        solution = ivpsolve.solve_adaptive_terminal_values(\n            init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n\n        # Return the terminal value\n        return jax.block_until_ready(solution.u[0])\n\n    return param_to_solution\n\n\ndef solver_diffrax(*, solver) -&gt; Callable:\n    \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"\n    # fmt: off\n    u0 = jnp.asarray(\n        [\n            3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,\n            3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,\n            0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,\n            0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,\n        ]\n    )\n    # fmt: on\n\n    @diffrax.ODETerm\n    @jax.jit\n    def vf_diffrax(_t, u, _args):\n        \"\"\"Pleiades problem.\"\"\"\n        x = u[0:7]  # x\n        y = u[7:14]  # y\n        xi, xj = x[:, None], x[None, :]\n        yi, yj = y[:, None], y[None, :]\n        rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)\n        mj = jnp.arange(1, 8)[None, :]\n        ddx = jnp.sum(jnp.nan_to_num(mj * (xj - xi) / rij), axis=1)\n        ddy = jnp.sum(jnp.nan_to_num(mj * (yj - yi) / rij), axis=1)\n        return jnp.concatenate((u[14:21], u[21:28], ddx, ddy))\n\n    t0, t1 = 0.0, 3.0\n\n    @jax.jit\n    def param_to_solution(tol):\n        controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)\n        saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)\n        solution = diffrax.diffeqsolve(\n            vf_diffrax,\n            y0=u0,\n            t0=t0,\n            t1=t1,\n            saveat=saveat,\n            stepsize_controller=controller,\n            dt0=None,\n            max_steps=10_000,\n            solver=solver,\n        )\n        return jax.block_until_ready(solution.ys[0, :14])\n\n    return param_to_solution\n\n\ndef solver_scipy(*, method: str, use_numba: bool) -&gt; Callable:\n    \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"\n    # fmt: off\n    u0 = np.asarray(\n        [\n            3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,\n            3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,\n            0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,\n            0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,\n        ]\n    )\n    # fmt: on\n\n    def vf_scipy(_t, u):\n        \"\"\"Pleiades problem.\"\"\"\n        x = u[0:7]  # x\n        y = u[7:14]  # y\n        xi, xj = x[:, None], x[None, :]\n        yi, yj = y[:, None], y[None, :]\n        rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)\n        mj = np.arange(1, 8)[None, :]\n\n        # Explicitly avoid dividing by zero for scipy's solver\n        # The JAX solvers divide by zero and turn the NaNs to zeros.\n        rij = np.where(rij == 0.0, 1.0, rij)\n        ddx = np.sum((mj * (xj - xi) / rij), axis=1)\n        ddy = np.sum((mj * (yj - yi) / rij), axis=1)\n        return np.concatenate((u[14:21], u[21:28], ddx, ddy))\n\n    if use_numba:\n        vf_scipy = numba.jit(nopython=True)(vf_scipy)\n\n    time_span = np.asarray([0.0, 3.0])\n\n    def param_to_solution(tol):\n        solution = scipy.integrate.solve_ivp(\n            vf_scipy,\n            y0=u0,\n            t_span=time_span,\n            t_eval=time_span,\n            atol=1e-3 * tol,\n            rtol=tol,\n            method=method,\n        )\n        return jnp.asarray(solution.y[:14, -1])\n\n    return param_to_solution\n\n\ndef rmse_absolute(expected: jax.Array) -&gt; Callable:\n    \"\"\"Compute the absolute RMSE.\"\"\"\n    expected = jnp.asarray(expected)\n\n    def rmse(received):\n        received = jnp.asarray(received)\n        error_absolute = jnp.abs(expected - received)\n        return jnp.linalg.norm(error_absolute) / jnp.sqrt(error_absolute.size)\n\n    return rmse\n\n\ndef workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:\n    \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"\n\n    def parameter_list_to_workprecision(list_of_args, /):\n        works_mean = []\n        works_std = []\n        precisions = []\n        for arg in list_of_args:\n            precision = precision_fun(fun(arg).block_until_ready())\n            times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023\n\n            precisions.append(precision)\n            works_mean.append(statistics.mean(times))\n            works_std.append(statistics.stdev(times))\n        return {\n            \"work_mean\": jnp.asarray(works_mean),\n            \"work_std\": jnp.asarray(works_std),\n            \"precision\": jnp.asarray(precisions),\n        }\n\n    return parameter_list_to_workprecision\n\n\nmain()\n</pre> \"\"\"Pleiades work-precision diagram.\"\"\"  import functools import statistics import timeit from collections.abc import Callable  import diffrax import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numba import numpy as np import scipy.integrate import tqdm  from probdiffeq import ivpsolve, ivpsolvers, taylor   def main(start=3.0, stop=11.0, step=1.0, repeats=2, use_diffrax: bool = False):     \"\"\"Run the script.\"\"\"     # Set up all the configs     jax.config.update(\"jax_enable_x64\", True)      # Simulate once to plot the state     ts, ys = solve_ivp_once()      fig, ax = plt.subplots(figsize=(5, 3))     ax.plot(ys[:, :7], ys[:, 7:14], linestyle=\"solid\", marker=\"None\")     ax.plot(ys[0, :7], ys[0, 7:14], linestyle=\"None\", marker=\".\", markersize=4)     ax.plot(ys[-1, :7], ys[-1, 7:14], linestyle=\"None\", marker=\"*\", markersize=8)      ax.set_title(\"Pleiades problem\")     ax.set_xlabel(\"Time\")     ax.set_ylabel(\"State\")     plt.tight_layout()     plt.show()      # Read configuration from command line     tolerances = setup_tolerances(start=start, stop=stop, step=step)     timeit_fun = setup_timeit(repeats=repeats)      # Assemble algorithms     algorithms = {         r\"ProbDiffEq: TS0($5$)\": solver_probdiffeq(             num_derivatives=5, correction_fun=ivpsolvers.correction_ts0         ),         r\"ProbDiffEq: TS0($8$)\": solver_probdiffeq(             num_derivatives=8, correction_fun=ivpsolvers.correction_ts0         ),         \"SciPy: 'RK45'\": solver_scipy(method=\"RK45\", use_numba=False),         \"SciPy: 'DOP853'\": solver_scipy(method=\"DOP853\", use_numba=False),         \"SciPy: 'RK45' (+numba)\": solver_scipy(method=\"RK45\", use_numba=True),         \"SciPy: 'DOP853' (+numba)\": solver_scipy(method=\"DOP853\", use_numba=True),         \"Diffrax: Tsit5()\": solver_diffrax(solver=diffrax.Tsit5()),         \"Diffrax: Dopri8()\": solver_diffrax(solver=diffrax.Dopri8()),     }      if use_diffrax:         # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0         # Revisit in the near future.         algorithms[\"Diffrax: Kvaerno3()\"] = solver_diffrax(solver=diffrax.Kvaerno3())         algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())     else:         print(\"\\nSkipped Diffrax.\\n\")      # Compute a reference solution     reference = solver_scipy(method=\"LSODA\", use_numba=False)(1e-14)     precision_fun = rmse_absolute(reference)      # Compute all work-precision diagrams     results = {}     for label, algo in tqdm.tqdm(algorithms.items()):         param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)         results[label] = param_to_wp(tolerances)      fig, ax = plt.subplots(figsize=(7, 3))     for label, wp in results.items():         ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)      ax.set_title(\"Work-precision diagram\")     ax.set_xlabel(\"Precision (relative RMSE)\")     ax.set_ylabel(\"Work (avg. wall time)\")     ax.grid(linestyle=\"dotted\", which=\"both\")     ax.legend(fontsize=\"small\", loc=\"center left\", bbox_to_anchor=(1, 0.5))      plt.tight_layout()     plt.show()   def solve_ivp_once():     \"\"\"Compute plotting-values for the IVP.\"\"\"     # fmt: off     u0 = np.asarray(         [             3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,             3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,             0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,             0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,         ]     )     # fmt: on      def vf_scipy(_t, u):         \"\"\"Pleiades problem.\"\"\"         x = u[0:7]  # x         y = u[7:14]  # y         xi, xj = x[:, None], x[None, :]         yi, yj = y[:, None], y[None, :]         rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)         mj = np.arange(1, 8)[None, :]          # Explicitly avoid dividing by zero for scipy's solver         # The JAX solvers divide by zero and turn the NaNs to zeros.         rij = np.where(rij == 0.0, 1.0, rij)         ddx = np.sum((mj * (xj - xi) / rij), axis=1)         ddy = np.sum((mj * (yj - yi) / rij), axis=1)         return np.concatenate((u[14:21], u[21:28], ddx, ddy))      time_span = np.asarray([0.0, 3.0])      tol = 1e-12     solution = scipy.integrate.solve_ivp(         vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"     )      return solution.t, solution.y.T   def setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:     \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"     return 0.1 ** jnp.arange(start, stop, step=step)   def setup_timeit(*, repeats: int) -&gt; Callable:     \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"      def timer(fun, /):         return list(timeit.repeat(fun, number=1, repeat=repeats))      return timer   def solver_probdiffeq(*, num_derivatives: int, correction_fun) -&gt; Callable:     \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"     # fmt: off     u0 = jnp.asarray(         [             3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,             3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,         ]     )     du0 = jnp.asarray(         [             0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,             0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,         ]     )     # fmt: on      @jax.jit     def vf_probdiffeq(u, du, *, t):  # noqa: ARG001         \"\"\"Pleiades problem.\"\"\"         x = u[0:7]  # x         y = u[7:14]  # y         xi, xj = x[:, None], x[None, :]         yi, yj = y[:, None], y[None, :]         rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)         mj = jnp.arange(1, 8)[None, :]         ddx = jnp.sum(jnp.nan_to_num(mj * (xj - xi) / rij), axis=1)         ddy = jnp.sum(jnp.nan_to_num(mj * (yj - yi) / rij), axis=1)         return jnp.concatenate((ddx, ddy))      t0, t1 = 0.0, 3.0      @jax.jit     def param_to_solution(tol):         # Build a solver         vf_auto = functools.partial(vf_probdiffeq, t=t0)         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0, du0), num=num_derivatives - 1)          init, ibm, ssm = ivpsolvers.prior_wiener_integrated(             tcoeffs, ssm_fact=\"isotropic\"         )         ts0_or_ts1 = correction_fun(vf_probdiffeq, ssm=ssm, ode_order=2)         strategy = ivpsolvers.strategy_filter(ssm=ssm)         solver = ivpsolvers.solver_dynamic(             strategy, prior=ibm, correction=ts0_or_ts1, ssm=ssm         )         control = ivpsolvers.control_proportional_integral()         adaptive_solver = ivpsolvers.adaptive(             solver, atol=1e-3 * tol, rtol=tol, control=control, ssm=ssm         )          # Solve         dt0 = ivpsolve.dt0(vf_auto, (u0, du0))         solution = ivpsolve.solve_adaptive_terminal_values(             init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm         )          # Return the terminal value         return jax.block_until_ready(solution.u[0])      return param_to_solution   def solver_diffrax(*, solver) -&gt; Callable:     \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"     # fmt: off     u0 = jnp.asarray(         [             3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,             3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,             0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,             0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,         ]     )     # fmt: on      @diffrax.ODETerm     @jax.jit     def vf_diffrax(_t, u, _args):         \"\"\"Pleiades problem.\"\"\"         x = u[0:7]  # x         y = u[7:14]  # y         xi, xj = x[:, None], x[None, :]         yi, yj = y[:, None], y[None, :]         rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)         mj = jnp.arange(1, 8)[None, :]         ddx = jnp.sum(jnp.nan_to_num(mj * (xj - xi) / rij), axis=1)         ddy = jnp.sum(jnp.nan_to_num(mj * (yj - yi) / rij), axis=1)         return jnp.concatenate((u[14:21], u[21:28], ddx, ddy))      t0, t1 = 0.0, 3.0      @jax.jit     def param_to_solution(tol):         controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)         saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)         solution = diffrax.diffeqsolve(             vf_diffrax,             y0=u0,             t0=t0,             t1=t1,             saveat=saveat,             stepsize_controller=controller,             dt0=None,             max_steps=10_000,             solver=solver,         )         return jax.block_until_ready(solution.ys[0, :14])      return param_to_solution   def solver_scipy(*, method: str, use_numba: bool) -&gt; Callable:     \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"     # fmt: off     u0 = np.asarray(         [             3.0,  3.0, -1.0, -3.00, 2.0, -2.00,  2.0,             3.0, -3.0,  2.0,  0.00, 0.0, -4.00,  4.0,             0.0,  0.0,  0.0,  0.00, 0.0,  1.75, -1.5,             0.0,  0.0,  0.0, -1.25, 1.0,  0.00,  0.0,         ]     )     # fmt: on      def vf_scipy(_t, u):         \"\"\"Pleiades problem.\"\"\"         x = u[0:7]  # x         y = u[7:14]  # y         xi, xj = x[:, None], x[None, :]         yi, yj = y[:, None], y[None, :]         rij = ((xi - xj) ** 2 + (yi - yj) ** 2) ** (3 / 2)         mj = np.arange(1, 8)[None, :]          # Explicitly avoid dividing by zero for scipy's solver         # The JAX solvers divide by zero and turn the NaNs to zeros.         rij = np.where(rij == 0.0, 1.0, rij)         ddx = np.sum((mj * (xj - xi) / rij), axis=1)         ddy = np.sum((mj * (yj - yi) / rij), axis=1)         return np.concatenate((u[14:21], u[21:28], ddx, ddy))      if use_numba:         vf_scipy = numba.jit(nopython=True)(vf_scipy)      time_span = np.asarray([0.0, 3.0])      def param_to_solution(tol):         solution = scipy.integrate.solve_ivp(             vf_scipy,             y0=u0,             t_span=time_span,             t_eval=time_span,             atol=1e-3 * tol,             rtol=tol,             method=method,         )         return jnp.asarray(solution.y[:14, -1])      return param_to_solution   def rmse_absolute(expected: jax.Array) -&gt; Callable:     \"\"\"Compute the absolute RMSE.\"\"\"     expected = jnp.asarray(expected)      def rmse(received):         received = jnp.asarray(received)         error_absolute = jnp.abs(expected - received)         return jnp.linalg.norm(error_absolute) / jnp.sqrt(error_absolute.size)      return rmse   def workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:     \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"      def parameter_list_to_workprecision(list_of_args, /):         works_mean = []         works_std = []         precisions = []         for arg in list_of_args:             precision = precision_fun(fun(arg).block_until_ready())             times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023              precisions.append(precision)             works_mean.append(statistics.mean(times))             works_std.append(statistics.stdev(times))         return {             \"work_mean\": jnp.asarray(works_mean),             \"work_std\": jnp.asarray(works_std),             \"precision\": jnp.asarray(precisions),         }      return parameter_list_to_workprecision   main() <pre>\nSkipped Diffrax.\n\n</pre> <pre>/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scipy/integrate/_ivp/ivp.py:621: UserWarning: At least one element of `rtol` is too small. Setting `rtol = np.maximum(rtol, 2.220446049250313e-14)`.\n  solver = method(fun, t0, y0, tf, vectorized=vectorized, **options)\n</pre> <pre>\r  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>\r 12%|\u2588\u258e        | 1/8 [00:01&lt;00:13,  1.89s/it]</pre> <pre>\r 25%|\u2588\u2588\u258c       | 2/8 [00:03&lt;00:10,  1.79s/it]</pre> <pre>\r 38%|\u2588\u2588\u2588\u258a      | 3/8 [00:07&lt;00:12,  2.58s/it]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:09&lt;00:09,  2.45s/it]</pre> <pre>\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:14&lt;00:10,  3.58s/it]</pre> <pre>\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:17&lt;00:06,  3.20s/it]</pre> <pre>\r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:18&lt;00:02,  2.38s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:18&lt;00:00,  1.78s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:18&lt;00:00,  2.32s/it]</pre> <pre>\n</pre>"},{"location":"examples_benchmarks/work-precision-pleiades/#wp-diagram-pleiades","title":"WP diagram: Pleiades\u00b6","text":""},{"location":"examples_benchmarks/work-precision-vanderpol/","title":"WP diagram: Stiff van-der-Pol","text":"In\u00a0[1]: Copied! <pre>\"\"\"Stiff-van-der-Pol work-precision diagram.\"\"\"\n\nimport functools\nimport statistics\nimport timeit\nfrom collections.abc import Callable\n\nimport diffrax\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.integrate\nimport tqdm\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\n\ndef main(start=2.0, stop=8.0, step=1.0, repeats=2, use_diffrax: bool = False):\n    \"\"\"Run the script.\"\"\"\n    # Set up all the configs\n    jax.config.update(\"jax_enable_x64\", True)\n\n    # Simulate once to plot the state\n    ts, ys = solve_ivp_once()\n\n    fig, ax = plt.subplots(figsize=(5, 3))\n    ax.plot(ts, ys)\n    ax.set_ylim((-6, 6))\n    ax.set_title(\"Van-der-Pol problem\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"State\")\n    plt.tight_layout()\n    plt.show()\n\n    # Read configuration from command line\n    tolerances = setup_tolerances(start=start, stop=stop, step=step)\n    timeit_fun = setup_timeit(repeats=repeats)\n\n    # Assemble algorithms\n    algorithms = {\n        \"SciPy: 'Radau'\": solver_scipy(method=\"Radau\"),\n        \"SciPy: 'LSODA'\": solver_scipy(method=\"LSODA\"),\n        r\"ProbDiffEq: TS1($3$)\": solver_probdiffeq(num_derivatives=3),\n        r\"ProbDiffEq: TS1($4$)\": solver_probdiffeq(num_derivatives=4),\n        r\"ProbDiffEq: TS1($5$)\": solver_probdiffeq(num_derivatives=5),\n    }\n    if use_diffrax:\n        # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0\n        # Revisit in the near future.\n        algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())\n    else:\n        print(\"\\nSkipped Diffrax.\\n\")\n\n    # Compute a reference solution\n    reference = solver_probdiffeq(num_derivatives=4)(1e-10)\n    precision_fun = rmse_absolute(reference)\n\n    # Compute all work-precision diagrams\n    results = {}\n    for label, algo in tqdm.tqdm(algorithms.items()):\n        param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)\n        results[label] = param_to_wp(tolerances)\n\n    fig, ax = plt.subplots(figsize=(7, 3))\n    for label, wp in results.items():\n        ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)\n\n    ax.set_title(\"Work-precision diagram\")\n    ax.set_xlabel(\"Precision (relative RMSE)\")\n    ax.set_ylabel(\"Work (avg. wall time)\")\n    ax.grid(linestyle=\"dotted\", which=\"both\")\n    ax.legend(fontsize=\"small\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef solve_ivp_once():\n    \"\"\"Compute plotting-values for the IVP.\"\"\"\n\n    def vf_scipy(_t, u):\n        \"\"\"Van-der-Pol dynamics as a first-order differential equation.\"\"\"\n        return np.asarray([u[1], 1e5 * ((1.0 - u[0] ** 2) * u[1] - u[0])])\n\n    u0 = np.concatenate((np.atleast_1d(2.0), np.atleast_1d(0.0)))\n    time_span = np.asarray((0.0, 6.3))\n\n    tol = 1e-12\n    solution = scipy.integrate.solve_ivp(\n        vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"\n    )\n    return solution.t, solution.y.T\n\n\ndef setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:\n    \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"\n    return 0.1 ** jnp.arange(start, stop, step=step)\n\n\ndef setup_timeit(*, repeats: int) -&gt; Callable:\n    \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"\n\n    def timer(fun, /):\n        return list(timeit.repeat(fun, number=1, repeat=repeats))\n\n    return timer\n\n\ndef solver_probdiffeq(*, num_derivatives: int) -&gt; Callable:\n    \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"\n\n    @jax.jit\n    def vf_probdiffeq(u, du, *, t):  # noqa: ARG001\n        \"\"\"Van-der-Pol dynamics as a second-order differential equation.\"\"\"\n        return 1e5 * ((1.0 - u**2) * du - u)\n\n    t0, t1 = 0.0, 3.0\n    u0, du0 = (jnp.atleast_1d(2.0), jnp.atleast_1d(0.0))\n    t0, t1 = (0.0, 6.3)\n\n    @jax.jit\n    def param_to_solution(tol):\n        # Build a solver\n        vf_auto = functools.partial(vf_probdiffeq, t=t0)\n        tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0, du0), num=num_derivatives - 1)\n\n        init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")\n        ts0_or_ts1 = ivpsolvers.correction_ts1(vf_probdiffeq, ode_order=2, ssm=ssm)\n        strategy = ivpsolvers.strategy_filter(ssm=ssm)\n\n        solver = ivpsolvers.solver_dynamic(\n            strategy, prior=ibm, correction=ts0_or_ts1, ssm=ssm\n        )\n        control = ivpsolvers.control_proportional_integral()\n        adaptive_solver = ivpsolvers.adaptive(\n            solver, atol=1e-3 * tol, rtol=tol, control=control, ssm=ssm, clip_dt=True\n        )\n\n        # Solve\n        dt0 = ivpsolve.dt0(vf_auto, (u0, du0))\n        solution = ivpsolve.solve_adaptive_terminal_values(\n            init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n\n        # Return the terminal value\n        return jax.block_until_ready(solution.u[0])\n\n    return param_to_solution\n\n\ndef solver_diffrax(*, solver) -&gt; Callable:\n    \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"\n\n    @diffrax.ODETerm\n    @jax.jit\n    def vf_diffrax(_t, u, _args):\n        \"\"\"Van-der-Pol dynamics as a first-order differential equation.\"\"\"\n        return jnp.asarray([u[1], 1e5 * ((1.0 - u[0] ** 2) * u[1] - u[0])])\n\n    t0, t1 = 0.0, 3.0\n    u0 = jnp.concatenate((jnp.atleast_1d(2.0), jnp.atleast_1d(0.0)))\n    t0, t1 = (0.0, 6.3)\n\n    @jax.jit\n    def param_to_solution(tol):\n        controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)\n        saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)\n        solution = diffrax.diffeqsolve(\n            vf_diffrax,\n            y0=u0,\n            t0=t0,\n            t1=t1,\n            saveat=saveat,\n            stepsize_controller=controller,\n            dt0=None,\n            max_steps=10_000,\n            solver=solver,\n        )\n        return jax.block_until_ready(solution.ys[0, 0])\n\n    return param_to_solution\n\n\ndef solver_scipy(method: str) -&gt; Callable:\n    \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"\n\n    def vf_scipy(_t, u):\n        \"\"\"Van-der-Pol dynamics as a first-order differential equation.\"\"\"\n        return np.asarray([u[1], 1e5 * ((1.0 - u[0] ** 2) * u[1] - u[0])])\n\n    u0 = np.concatenate((np.atleast_1d(2.0), np.atleast_1d(0.0)))\n    time_span = np.asarray((0.0, 6.3))\n\n    def param_to_solution(tol):\n        solution = scipy.integrate.solve_ivp(\n            vf_scipy,\n            y0=u0,\n            t_span=time_span,\n            t_eval=time_span,\n            atol=1e-3 * tol,\n            rtol=tol,\n            method=method,\n        )\n        return jnp.asarray(solution.y[0, -1])\n\n    return param_to_solution\n\n\ndef rmse_absolute(expected: jax.Array) -&gt; Callable:\n    \"\"\"Compute the absolute RMSE.\"\"\"\n    expected = jnp.asarray(expected)\n\n    def rmse(received):\n        received = jnp.asarray(received)\n        error_absolute = jnp.abs(expected - received)\n        return jnp.linalg.norm(error_absolute) / jnp.sqrt(error_absolute.size)\n\n    return rmse\n\n\ndef workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:\n    \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"\n\n    def parameter_list_to_workprecision(list_of_args, /):\n        works_mean = []\n        works_std = []\n        precisions = []\n        for arg in list_of_args:\n            precision = precision_fun(fun(arg).block_until_ready())\n            times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023\n\n            precisions.append(precision)\n            works_mean.append(statistics.mean(times))\n            works_std.append(statistics.stdev(times))\n        return {\n            \"work_mean\": jnp.asarray(works_mean),\n            \"work_std\": jnp.asarray(works_std),\n            \"precision\": jnp.asarray(precisions),\n        }\n\n    return parameter_list_to_workprecision\n\n\nmain()\n</pre> \"\"\"Stiff-van-der-Pol work-precision diagram.\"\"\"  import functools import statistics import timeit from collections.abc import Callable  import diffrax import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import scipy.integrate import tqdm  from probdiffeq import ivpsolve, ivpsolvers, taylor   def main(start=2.0, stop=8.0, step=1.0, repeats=2, use_diffrax: bool = False):     \"\"\"Run the script.\"\"\"     # Set up all the configs     jax.config.update(\"jax_enable_x64\", True)      # Simulate once to plot the state     ts, ys = solve_ivp_once()      fig, ax = plt.subplots(figsize=(5, 3))     ax.plot(ts, ys)     ax.set_ylim((-6, 6))     ax.set_title(\"Van-der-Pol problem\")     ax.set_xlabel(\"Time\")     ax.set_ylabel(\"State\")     plt.tight_layout()     plt.show()      # Read configuration from command line     tolerances = setup_tolerances(start=start, stop=stop, step=step)     timeit_fun = setup_timeit(repeats=repeats)      # Assemble algorithms     algorithms = {         \"SciPy: 'Radau'\": solver_scipy(method=\"Radau\"),         \"SciPy: 'LSODA'\": solver_scipy(method=\"LSODA\"),         r\"ProbDiffEq: TS1($3$)\": solver_probdiffeq(num_derivatives=3),         r\"ProbDiffEq: TS1($4$)\": solver_probdiffeq(num_derivatives=4),         r\"ProbDiffEq: TS1($5$)\": solver_probdiffeq(num_derivatives=5),     }     if use_diffrax:         # TODO: this is a temporary fix because Diffrax doesn't work with JAX &gt;= 0.7.0         # Revisit in the near future.         algorithms[\"Diffrax: Kvaerno5()\"] = solver_diffrax(solver=diffrax.Kvaerno5())     else:         print(\"\\nSkipped Diffrax.\\n\")      # Compute a reference solution     reference = solver_probdiffeq(num_derivatives=4)(1e-10)     precision_fun = rmse_absolute(reference)      # Compute all work-precision diagrams     results = {}     for label, algo in tqdm.tqdm(algorithms.items()):         param_to_wp = workprec(algo, precision_fun=precision_fun, timeit_fun=timeit_fun)         results[label] = param_to_wp(tolerances)      fig, ax = plt.subplots(figsize=(7, 3))     for label, wp in results.items():         ax.loglog(wp[\"precision\"], wp[\"work_mean\"], label=label)      ax.set_title(\"Work-precision diagram\")     ax.set_xlabel(\"Precision (relative RMSE)\")     ax.set_ylabel(\"Work (avg. wall time)\")     ax.grid(linestyle=\"dotted\", which=\"both\")     ax.legend(fontsize=\"small\", loc=\"center left\", bbox_to_anchor=(1, 0.5))      plt.tight_layout()     plt.show()   def solve_ivp_once():     \"\"\"Compute plotting-values for the IVP.\"\"\"      def vf_scipy(_t, u):         \"\"\"Van-der-Pol dynamics as a first-order differential equation.\"\"\"         return np.asarray([u[1], 1e5 * ((1.0 - u[0] ** 2) * u[1] - u[0])])      u0 = np.concatenate((np.atleast_1d(2.0), np.atleast_1d(0.0)))     time_span = np.asarray((0.0, 6.3))      tol = 1e-12     solution = scipy.integrate.solve_ivp(         vf_scipy, y0=u0, t_span=time_span, atol=1e-3 * tol, rtol=tol, method=\"LSODA\"     )     return solution.t, solution.y.T   def setup_tolerances(*, start: float, stop: float, step: float) -&gt; jax.Array:     \"\"\"Choose vector of tolerances from the command-line arguments.\"\"\"     return 0.1 ** jnp.arange(start, stop, step=step)   def setup_timeit(*, repeats: int) -&gt; Callable:     \"\"\"Construct a timeit-function from the command-line arguments.\"\"\"      def timer(fun, /):         return list(timeit.repeat(fun, number=1, repeat=repeats))      return timer   def solver_probdiffeq(*, num_derivatives: int) -&gt; Callable:     \"\"\"Construct a solver that wraps ProbDiffEq's solution routines.\"\"\"      @jax.jit     def vf_probdiffeq(u, du, *, t):  # noqa: ARG001         \"\"\"Van-der-Pol dynamics as a second-order differential equation.\"\"\"         return 1e5 * ((1.0 - u**2) * du - u)      t0, t1 = 0.0, 3.0     u0, du0 = (jnp.atleast_1d(2.0), jnp.atleast_1d(0.0))     t0, t1 = (0.0, 6.3)      @jax.jit     def param_to_solution(tol):         # Build a solver         vf_auto = functools.partial(vf_probdiffeq, t=t0)         tcoeffs = taylor.odejet_padded_scan(vf_auto, (u0, du0), num=num_derivatives - 1)          init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")         ts0_or_ts1 = ivpsolvers.correction_ts1(vf_probdiffeq, ode_order=2, ssm=ssm)         strategy = ivpsolvers.strategy_filter(ssm=ssm)          solver = ivpsolvers.solver_dynamic(             strategy, prior=ibm, correction=ts0_or_ts1, ssm=ssm         )         control = ivpsolvers.control_proportional_integral()         adaptive_solver = ivpsolvers.adaptive(             solver, atol=1e-3 * tol, rtol=tol, control=control, ssm=ssm, clip_dt=True         )          # Solve         dt0 = ivpsolve.dt0(vf_auto, (u0, du0))         solution = ivpsolve.solve_adaptive_terminal_values(             init, t0=t0, t1=t1, dt0=dt0, adaptive_solver=adaptive_solver, ssm=ssm         )          # Return the terminal value         return jax.block_until_ready(solution.u[0])      return param_to_solution   def solver_diffrax(*, solver) -&gt; Callable:     \"\"\"Construct a solver that wraps Diffrax' solution routines.\"\"\"      @diffrax.ODETerm     @jax.jit     def vf_diffrax(_t, u, _args):         \"\"\"Van-der-Pol dynamics as a first-order differential equation.\"\"\"         return jnp.asarray([u[1], 1e5 * ((1.0 - u[0] ** 2) * u[1] - u[0])])      t0, t1 = 0.0, 3.0     u0 = jnp.concatenate((jnp.atleast_1d(2.0), jnp.atleast_1d(0.0)))     t0, t1 = (0.0, 6.3)      @jax.jit     def param_to_solution(tol):         controller = diffrax.PIDController(atol=1e-3 * tol, rtol=tol)         saveat = diffrax.SaveAt(t0=False, t1=True, ts=None)         solution = diffrax.diffeqsolve(             vf_diffrax,             y0=u0,             t0=t0,             t1=t1,             saveat=saveat,             stepsize_controller=controller,             dt0=None,             max_steps=10_000,             solver=solver,         )         return jax.block_until_ready(solution.ys[0, 0])      return param_to_solution   def solver_scipy(method: str) -&gt; Callable:     \"\"\"Construct a solver that wraps SciPy's solution routines.\"\"\"      def vf_scipy(_t, u):         \"\"\"Van-der-Pol dynamics as a first-order differential equation.\"\"\"         return np.asarray([u[1], 1e5 * ((1.0 - u[0] ** 2) * u[1] - u[0])])      u0 = np.concatenate((np.atleast_1d(2.0), np.atleast_1d(0.0)))     time_span = np.asarray((0.0, 6.3))      def param_to_solution(tol):         solution = scipy.integrate.solve_ivp(             vf_scipy,             y0=u0,             t_span=time_span,             t_eval=time_span,             atol=1e-3 * tol,             rtol=tol,             method=method,         )         return jnp.asarray(solution.y[0, -1])      return param_to_solution   def rmse_absolute(expected: jax.Array) -&gt; Callable:     \"\"\"Compute the absolute RMSE.\"\"\"     expected = jnp.asarray(expected)      def rmse(received):         received = jnp.asarray(received)         error_absolute = jnp.abs(expected - received)         return jnp.linalg.norm(error_absolute) / jnp.sqrt(error_absolute.size)      return rmse   def workprec(fun, *, precision_fun: Callable, timeit_fun: Callable) -&gt; Callable:     \"\"\"Turn a parameter-to-solution function into parameter-to-workprecision.\"\"\"      def parameter_list_to_workprecision(list_of_args, /):         works_mean = []         works_std = []         precisions = []         for arg in list_of_args:             precision = precision_fun(fun(arg).block_until_ready())             times = timeit_fun(lambda: fun(arg).block_until_ready())  # noqa: B023              precisions.append(precision)             works_mean.append(statistics.mean(times))             works_std.append(statistics.stdev(times))         return {             \"work_mean\": jnp.asarray(works_mean),             \"work_std\": jnp.asarray(works_std),             \"precision\": jnp.asarray(precisions),         }      return parameter_list_to_workprecision   main() <pre>\nSkipped Diffrax.\n\n</pre> <pre>\r  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\r 20%|\u2588\u2588        | 1/5 [00:27&lt;01:48, 27.11s/it]</pre> <pre>\r 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:34, 11.61s/it]</pre> <pre>\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:46&lt;00:29, 14.76s/it]</pre> <pre>\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:50&lt;00:10, 10.63s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:52&lt;00:00,  7.62s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:52&lt;00:00, 10.59s/it]</pre> <pre>\n</pre>"},{"location":"examples_benchmarks/work-precision-vanderpol/#wp-diagram-stiff-van-der-pol","title":"WP diagram: Stiff van-der-Pol\u00b6","text":""},{"location":"examples_quickstart/quickstart/","title":"Quickstart","text":"In\u00a0[1]: Copied! <pre>\"\"\"Solve the logistic equation.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\n# Define a differential equation\n\n\n@jax.jit\ndef vf(y, *, t):  # noqa: ARG001\n    \"\"\"Evaluate the dynamics of the logistic ODE.\"\"\"\n    return 2 * y * (1 - y)\n\n\nu0 = jnp.asarray([0.1])\nt0, t1 = 0.0, 5.0\n\n\n# Set up a state-space model\ntcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")\n\n\n# Build a solver\nts = ivpsolvers.correction_ts1(vf, ssm=ssm, ode_order=1)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\nsolver = ivpsolvers.solver_mle(ssm=ssm, strategy=strategy, prior=ibm, correction=ts)\nadaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n\n\n# Solve the ODE\n# To all users: Try different solution routines.\nsolution = ivpsolve.solve_adaptive_save_every_step(\n    init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n)\n\n# Look at the solution\nprint(f\"\\ninitial = {jax.tree.map(jnp.shape, init)}\")\nprint(f\"\\nsolution = {jax.tree.map(jnp.shape, solution)}\")\n</pre> \"\"\"Solve the logistic equation.\"\"\"  import jax import jax.numpy as jnp  from probdiffeq import ivpsolve, ivpsolvers, taylor  # Define a differential equation   @jax.jit def vf(y, *, t):  # noqa: ARG001     \"\"\"Evaluate the dynamics of the logistic ODE.\"\"\"     return 2 * y * (1 - y)   u0 = jnp.asarray([0.1]) t0, t1 = 0.0, 5.0   # Set up a state-space model tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")   # Build a solver ts = ivpsolvers.correction_ts1(vf, ssm=ssm, ode_order=1) strategy = ivpsolvers.strategy_filter(ssm=ssm) solver = ivpsolvers.solver_mle(ssm=ssm, strategy=strategy, prior=ibm, correction=ts) adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)   # Solve the ODE # To all users: Try different solution routines. solution = ivpsolve.solve_adaptive_save_every_step(     init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm )  # Look at the solution print(f\"\\ninitial = {jax.tree.map(jnp.shape, init)}\") print(f\"\\nsolution = {jax.tree.map(jnp.shape, solution)}\") <pre>\ninitial = Normal(mean=(2,), cholesky=(2, 2))\n\nsolution = IVPSolution(t=(38,), u=[(38, 1), (38, 1)], u_std=[(38, 1), (38, 1)], output_scale=(37,), marginals=Normal(mean=(38, 2), cholesky=(38, 2, 2)), posterior=Normal(mean=(38, 2), cholesky=(38, 2, 2)), num_steps=(37,), ssm=FactImpl(name='dense', prototypes=&lt;probdiffeq.impl._prototypes.DensePrototype object at 0x7feb8c5a33e0&gt;, normal=&lt;probdiffeq.impl._normal.DenseNormal object at 0x7feb8c5ec170&gt;, stats=&lt;probdiffeq.impl._stats.DenseStats object at 0x7feb8c5ec050&gt;, linearise=&lt;probdiffeq.impl._linearise.DenseLinearisation object at 0x7feb8c5ec140&gt;, conditional=&lt;probdiffeq.impl._conditional.DenseConditional object at 0x7feb8c5ec0b0&gt;, num_derivatives=1, unravel=&lt;jax._src.util.HashablePartial object at 0x7feb8f3082c0&gt;))\n</pre>"},{"location":"examples_quickstart/quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>Let's have a look at an easy example.</p>"}]}