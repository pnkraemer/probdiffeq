{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"probdiffeq","text":""},{"location":"#probabilistic-ode-solvers-in-jax","title":"Probabilistic ODE solvers in JAX","text":"<p>Probdiffeq implements adaptive probabilistic numerical solvers for ordinary differential equations (ODEs). It builds on JAX, thus inheriting automatic differentiation, vectorisation, and GPU acceleration.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\u26a1 Calibration and step-size adaptation  </li> <li>\u26a1 Stable implementations of filtering, smoothing, and other estimation strategies  </li> <li>\u26a1 Custom information operators, dense output, and posterior sampling  </li> <li>\u26a1 State-space model factorisations  </li> <li>\u26a1 Parameter estimation</li> <li>\u26a1 Taylor-series estimation with and without Jets  </li> <li>\u26a1 Seamless interoperability with Optax, BlackJAX, and other JAX-based libraries  </li> <li>\u26a1 Numerous tutorials (basic and advanced) -- see the documentation </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest release from PyPI:</p> <pre><code>pip install probdiffeq\n</code></pre> <p>This assumes JAX is already installed.  </p> <p>To install with JAX (CPU backend):  </p> <pre><code>pip install probdiffeq[cpu]\n</code></pre> <p>\u26a0\ufe0f Note: This is an active research project. Expect rough edges and breaking API changes.</p>"},{"location":"#benchmarks","title":"Benchmarks","text":"<p>We maintain benchmarks comparing Probdiffeq against other solvers and libraries, including SciPy, JAX, and Diffrax.</p> <p>Run benchmarks locally:</p> <pre><code>pip install .[example,test]\nmake benchmarks-run\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome! - Browse open issues (look for \u201cgood first issue\u201d). - Check the developer documentation. - Open an issue for feature requests or ideas.  </p>"},{"location":"#citing","title":"Citing","text":"<p>If you use Probdiffeq in your research, please cite:</p> <p><pre><code>@phdthesis{kramer2024implementing,\n  title={Implementing probabilistic numerical solvers for differential equations},\n  author={Kr{\"a}mer, Peter Nicholas},\n  year={2024},\n  school={Universit{\"a}t T{\"u}bingen}\n}\n</code></pre> The PDF explains the mathematics and algorithms behind this library.  </p> <p>For the solve-and-save-at functionality, cite:</p> <p><pre><code>@InProceedings{kramer2024adaptive,\n  title     = {Adaptive Probabilistic ODE Solvers Without Adaptive Memory Requirements},\n  author    = {Kr\"{a}mer, Nicholas},\n  booktitle = {Proceedings of the First International Conference on Probabilistic Numerics},\n  pages     = {12--24},\n  year      = {2025},\n  editor    = {Kanagawa, Motonobu and Cockayne, Jon and Gessner, Alexandra and Hennig, Philipp},\n  volume    = {271},\n  series    = {Proceedings of Machine Learning Research},\n  publisher = {PMLR},\n  url       = {https://proceedings.mlr.press/v271/kramer25a.html}\n}\n</code></pre> Link to the paper: PDF.</p> <p>Link to the experiments:  Code for experiments.  </p> <p>\ud83d\udccc Algorithms in Probdiffeq are based on multiple research papers. If you\u2019re unsure which to cite, feel free to reach out.  </p>"},{"location":"#versioning","title":"Versioning","text":"<p>Probdiffeq follows 0.MINOR.PATCH until its first stable release: - PATCH \u2192 bugfixes &amp; new features - MINOR \u2192 breaking changes  </p> <p>See semantic versioning.</p>"},{"location":"#related-projects","title":"Related projects","text":"<ul> <li>Tornadox </li> <li>ProbNumDiffEq.jl </li> <li>ProbNum </li> </ul> <p>The docs include guidance on migrating from these packages. Missing something? Open an issue or pull request!</p>"},{"location":"#you-might-also-like","title":"You might also like","text":"<ul> <li>diffeqzoo \u2014 reference implementations of differential equations in NumPy and JAX  </li> <li>probfindiff \u2014 probabilistic finite-difference methods in JAX  </li> </ul>"},{"location":"choosing_a_solver/","title":"Choosing a solver","text":"<p>Good solvers are problem-dependent. Nevertheless, some guidelines exist:</p>"},{"location":"choosing_a_solver/#state-space-model-factorisation","title":"State-space model factorisation","text":"<ul> <li>If your problem is scalar-valued (<code>shape=()</code>), use a <code>scalar</code> implementation. Of course, you are always welcome to transform your problem into one with shape <code>(1,)</code> and use a vector-valued solver (not all features are implemented for scalar models).</li> <li>If your problem is vector-valued, be aware that different implementation choices imply different modelling choices.</li> </ul> <p>If you don't care about modelling choices:</p> <ul> <li>If your problem is high-dimensional, use a <code>blockdiag</code> or <code>isotropic</code> implementation.</li> <li>If your problem is medium-dimensional, use any implementations.    <code>isotropic</code> factorisations tend to be the fastest with the worst UQ and worst stability,    <code>dense</code> factorisations tend to be the slowest with the best UQ and best stability,    <code>blockdiag</code> factorisations are somewhere in between.</li> </ul>"},{"location":"choosing_a_solver/#stiffness","title":"Stiffness","text":"<p>If your problem is stiff, use a a <code>dense</code> implementation in combination with a correction scheme that employs first-order linearisation;  for instance, <code>ts1</code> or <code>slr1</code>. Zeroth-order approximation and too-aggressive state-space model factorisation  will likely fail.</p> <p>If your problem is stiff and high-dimensional: try first-order linearisation with a block-diagonal factorisation.  If that does not work: let me know what you come up with...</p>"},{"location":"choosing_a_solver/#filters-vs-smoothers","title":"Filters vs smoothers","text":"<p>Almost always, use a <code>ivpsolvers.strategy_filter</code> strategy for <code>simulate_terminal_values</code>,  a <code>ivpsolvers.strategy_smoother</code> strategy for <code>solve_adaptive_save_every_step</code>, and a <code>ivpsolvers.strategy_fixedpoint</code> strategy for <code>solve_adaptive_save_at</code>. Use either a filter (if you must) or a smoother (recommended) for <code>solve_fixed_step</code>. Other combinations are possible, but rather rare  (and require some understanding of the underlying statistical concepts).</p>"},{"location":"choosing_a_solver/#calibration","title":"Calibration","text":"<p>Use a <code>solvers.solver_dynamic</code> solver if you expect that the output scale of your IVP solution varies greatly. Otherwise, use an <code>solvers.solver_mle</code> solver. Try a <code>solvers.solver</code> for parameter-estimation.</p>"},{"location":"choosing_a_solver/#miscellaneous","title":"Miscellaneous","text":"<p>If you use a <code>ts0</code>, choose an <code>isotropic</code> factorisation instead of a <code>dense</code> factorisation. They do the same, but the <code>isotropic</code> factorisation is cheaper.</p> <p>These guidelines are a work in progress and may change soon. If you have any input, let me know!</p>"},{"location":"migration_guide/","title":"Migration guide","text":"<p>Here is how you get started with ProbDiffEq for solving ordinary differential equations (ODEs)  if you already have experience with other (probabilistic) ODE solver packages in Python and Julia.</p> <p>The most similar packages to ProbDiffEq are</p> <ul> <li>Tornadox</li> <li>ProbNumDiffEq.jl</li> <li>ProbNum</li> </ul> <p>We will explain the differences below.</p> <p>We will also cover differences to  </p> <ul> <li>Diffrax</li> <li>Other non-probabilistic IVP solver libraries, e.g., SciPy. </li> </ul> <p>Before starting, credit is due: ProbDiffEq draws much inspiration from those code bases; without them, it wouldn't exist.</p>"},{"location":"migration_guide/#transitioning-from-tornadox","title":"Transitioning from Tornadox","text":"<p>Tornadox is a package that contains JAX implementations of probabilistic IVP solvers. It has been used, for instance, to solve million-dimensional differential equations.</p> <p>ProbDiffEq is (more or less) a successor of Tornadox: it can do almost everything that Tornadox can do, but is generally faster (compiling entire solver loops instead of only single steps), offers more solvers, and provides more features built ``around'' IVP solutions: e.g. dense output or posterior sampling. ProbDiffEq is also more thoroughly tested and documented, and has a few features that are not yet (that is, at the time of writing this document) implemented in Tornadox.</p> <p>Recently, the development of Tornadox has not been very active, so its API is relatively stable. In contrast, APIs in ProbDiffEq may change. Other than that, you may safely replace Tornadox with ProbDiffEq,  because it is faster, more thoroughly documented, and has more features.</p>"},{"location":"migration_guide/#transitioning-from-probnumdiffeqjl","title":"Transitioning from ProbNumDiffEq.jl","text":"<p>ProbNumDiffEq.jl has been around for a while now (and successfully so), embeds into the <code>SciML</code> ecosystem, and can do a few things we have yet to achieve. It might be the most performant probabilistic-IVP-solver library to date. But there are some neat little features that ProbDiffEq provides that are unavailable in ProbNumDiffEq.jl.</p> <p>Ignoring that ProbNumDiffEq.jl is written in Julia and that ProbDiffEq builds on JAX, the two packages are very similar, and  both packages should be more or less equally efficient. </p> <p>The most apparent feature differences between ProbNumDiffEq.jl and ProbDiffEq are (at the time of this writing):</p> <ul> <li>ProbNumDiffEq.jl can solve mass-matrix problems, which ProbDiffEq does not yet do.</li> <li>ProbNumDiffEq.jl allows callbacks, which ProbDiffEq does not yet do.</li> <li>ProbDiffEq has a few methods that are not in ProbNumDiffEq.jl, e.g., statistical linearisation solvers.</li> </ul> <p>Both packages are still evolving, so this list may not remain up-to-date. When in doubt, consult each package's API documentation.</p> <p>To translate between the two packages, consider the following:</p> <ul> <li>Everything termed <code>EK0</code> or <code>EK1</code> in ProbNumDiffEq.jl is <code>ts0</code> or <code>ts1</code> in ProbDiffEq (\"ts\" stands for \"Taylor series\" linearisation and stands in contrast to \"slr\", i.e. \"statistical linear regression\").</li> <li>ProbNumDiffEq.jl calibrates output scales via <code>DynamicDiffusion</code>, <code>FixedDiffusion</code>, <code>DynamicMVDiffusion</code>, or <code>FixedMVDiffusion</code>.   Their equivalents in ProbDiffEq are <code>ivpsolvers.solver_dynamic()</code> or <code>ivpsolvers.solver_mle()</code>. Feed them with any strategies (Filters/Smoothers) and any state-space model implementations. Use a block-diagonal implementation (e.g. <code>impl.choose(\"blockdiag\")</code>) for multivariate output scales (\"<code>MVDiffusion</code>\"). Try the <code>ivpsolvers.solver()</code> with a manual (gradient-based?) calibration if the other routines are unsatisfactory.</li> <li>ProbNumDiffEq.jl refers to <code>prior_ibm(output_scale=x)</code> as <code>IWP(diffusion=x^2)</code>. They are the same processes. </li> <li>ProbNumDiffEq.jl switches between filtering and smoothing with a <code>smooth=true/false</code> flag. ProbDiffEq uses different strategies to distinguish these strategies and offers a third one (fixedpoint-smoothing). </li> <li>Initialisation schemes like those in <code>ProbNumDiffEq</code> are in <code>probdiffeq.taylor</code>. ProbDiffEq offers some rules for high-order differential equations and some unique methods (e.g. doubling). But the feature lists are relatively similar.</li> </ul>"},{"location":"migration_guide/#transitioning-from-probnum","title":"Transitioning from ProbNum","text":"<p>ProbNum is a probabilistic numerics library in Python, just like ProbDiffEq. ProbNum collects probabilistic solvers for many problems, not just ordinary differential equations. That said, ProbDiffEq specialises in pure JAX and state-space-model-based IVP solvers, which leads to significant efficiency gains. Probdiffeq implements more solvers than ProbNum.</p>"},{"location":"migration_guide/#transitioning-from-diffrax","title":"Transitioning from Diffrax","text":"<p>Diffrax is a JAX-based library offering numerical differential equation solvers. One of its big selling points is that it unifies implementations of solvers for SDEs, CDEs, and ODEs.</p> <p>The main difference between ProbDiffEq and Diffrax is that Diffrax provides non-probabilistic ODE solvers whereas ProbDiffEq provides probabilistic solvers. Both solve differential equations, but they can only be compared to a certain extent:</p> <p>Yes, both classes of algorithms solve differential equations and can (and will) be part of the same benchmarks. But the sets of methods provided by each package are completely disjoint, and the choice between both toolboxes reduces to the choice between non-probabilistic and probabilistic algorithms. (When to choose which one is a subject for another document; some selling points of probabilistic solvers are discussed in the example notebooks.)</p> <p>The main API differences between the packages are the following:</p> <ul> <li>To build a solver in Diffrax, it usually suffices to call, e.g. <code>diffrax.Tsit5()</code>.    In ProbDiffEq, constructing a solver is more involved (which is not necessarily a drawback; check the quickstart). </li> <li>The vector fields in Diffrax are <code>diffrax.ODETerm()</code>s (presumably, because of the joint treatment of ODEs/SDEs);    in ProbDiffEq, we pass plain functions with signature <code>(*ys, t)</code>.</li> <li>Diffrax offers multiple modes of differentiating the IVP solver. For probabilistic solvers, all but what JAX natively provides is a work in progress.</li> </ul> <p>To roughly translate the Diffrax IVP solvers to ProbDiffEq solvers, consider the following selection of solvers:</p> If you use the following solvers in Diffrax: You might like the following solvers in ProbDiffEq: Comments <code>Heun()</code>, <code>Midpoint()</code>, <code>Ralston()</code>, <code>LeapfrogMidpoint()</code> e.g. <code>prior_ibm(num_derivatives=1)</code>, <code>ts0()</code> with an <code>isotropic</code> or <code>blockdiag</code> state-space model Use a block-diagonal state-space model if the ODE dimensions have greatly different scales <code>Bosh3()</code> increase <code>num_derivatives</code> to <code>num_derivatives=2</code> in the above See above. <code>Tsit5()</code>, <code>Dopri5()</code> increase <code>num_derivatives</code> to <code>num_derivatives=4</code> in the above See above. <code>Dopri8()</code> increase <code>num_derivatives</code> to <code>num_derivatives={5,6,7}</code> in the above. If this is inefficient, try a <code>ts1()</code> correction. See above. <code>Kvaerno3()</code> use <code>num_derivatives=2</code> and a <code>ts1()</code> correction. See above. <code>Kvaerno4()</code> use <code>num_derivatives=3</code> and a <code>ts1()</code> correction. See above. <code>Kvaerno5()</code> use <code>num_derivatives=4</code> and a <code>ts1()</code> correction. See above. Symplectic methods Work in progress. Reversible methods Work in progress."},{"location":"migration_guide/#general-divergences-from-other-non-probabilistic-solver-libraries-eg-jaxodeint-or-scipy","title":"General divergences from other non-probabilistic solver libraries (e.g. jax.odeint or SciPy)","text":"<p>Most of the divergences from Diffrax apply.  Additionally:</p> <ul> <li>Solution objects in ProbDiffEq are random processes (posterior distributions). Random variable types replace most vectors and matrices. This statistical description is richer than a point estimate but needs to be calibrated and demands a non-trivial interaction with the solution (e.g. via sampling from it instead of simply plotting the point-estimate)</li> <li>ProbDiffEq offers different solution methods: <code>simulate_terminal_values()</code>, <code>solve_adaptive_save_every_step()</code>, or <code>solve_adaptive_save_at()</code>. Many conventional ODE solver suites expose this functionality through flags in a single <code>solve</code> function.  Expressing different modes of solving differential equations in different functions almost exclusively affects the source-code simplicity; but it also allows matching the solver to the solving mode (e.g., terminal values vs save-at). For example, <code>simulate_terminal_values()</code> is best combined with a filter.</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#general-troubleshooting","title":"General troubleshooting","text":"<p>If you encounter unexpected issues, please ensure you have the latest version of JAX installed.  If you're not already using virtual environments, now might be a good time to start, as they can help manage dependencies more effectively.</p> <p>With these points covered, try to execute some of the examples in Probdiffeq's documentation, for example the easy example. If these examples work \\(-\\) great! If not, reach out. </p> <p>Unlike many other JAX-based scientific computing libraries, probdiffeq works best with double precision.  This is because, during solver initialization, it computes the Cholesky factor of a Hilbert matrix (with somewhere between 2-12 rows), which needs high precision.</p>"},{"location":"troubleshooting/#long-compilation-times","title":"Long compilation times","text":"<p>If a solution routine takes an unexpectedly long time to compile but runs quickly afterward, the issue might be related to how Taylor coefficients are computed.  Some functions in <code>probdiffeq.taylor</code> unroll a small loop, which can slow down compilation. To avoid this, try using the padded scan, which replaces loop unrolling with a scan. If the problem persists, consider:  </p> <ul> <li>Reducing the number of derivatives (if appropriate for your problem).  </li> <li>Switching to a different Taylor-coefficient routine, such as a Runge-Kutta starter.</li> </ul> <p>For \\(\\nu &lt; 5\\), using a Runge-Kutta starter should maintain solver performance. However, for higher-order methods (e.g., \\(\\nu = 9\\)), Taylor-mode (\"jets\") appears to be the best choice.  </p>"},{"location":"troubleshooting/#taylor-derivative-routines-yield-nans","title":"Taylor-derivative routines yield NaNs","text":"<p>If you encounter unexpected NaNs while estimating Taylor derivative routines, the issue might come from the vector field itself! For instance, in the Pleiades problem, there's a term like \\(\\|x\\|^2 / (\\|x\\|^2 + \\|y\\|^2)\\), which can have differentiability issues near zero, depending on how it's implemented.  See this issue (external) for more details. In some cases, the fix is as simple as wrapping the quotient in <code>jax.numpy.nan_to_num</code>.  You can also check out Probdiffeq's Pleiades benchmark for a concrete example.</p>"},{"location":"troubleshooting/#other-problems","title":"Other problems","text":"<p>Your problem is not discussed here? Feel free to reach out \\(-\\) opening an issue is a great way to get help!</p>"},{"location":"api_docs/impl/","title":"impl","text":"<p>State-space model implementations.</p>"},{"location":"api_docs/ivpsolve/","title":"ivpsolve","text":"<p>Routines for estimating solutions of initial value problems.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution","title":"<code>IVPSolution</code>","text":"<p>The probabilistic numerical solution of an initial value problem (IVP).</p> <p>This class stores the computed solution, its uncertainty estimates, and details of the probabilistic model used in probabilistic numerical integration.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>@containers.dataclass\nclass IVPSolution:\n    \"\"\"The probabilistic numerical solution of an initial value problem (IVP).\n\n    This class stores the computed solution,\n    its uncertainty estimates, and details of the probabilistic model\n    used in probabilistic numerical integration.\n    \"\"\"\n\n    t: Array\n    \"\"\"Time points at which the IVP solution has been computed.\"\"\"\n\n    u: Array\n    \"\"\"The mean of the IVP solution at each computed time point.\"\"\"\n\n    u_std: Array\n    \"\"\"The standard deviation of the IVP solution, indicating uncertainty.\"\"\"\n\n    output_scale: Array\n    \"\"\"The calibrated output scale of the probabilistic model.\"\"\"\n\n    marginals: Any\n    \"\"\"Marginal distributions for each time point in the posterior distribution.\"\"\"\n\n    posterior: Any\n    \"\"\"A the full posterior distribution of the probabilistic numerical solution.\n\n    Typically, a backward factorisation of the posterior.\n    \"\"\"\n\n    num_steps: Array\n    \"\"\"The number of solver steps taken at each time point.\"\"\"\n\n    ssm: Any\n    \"\"\"State-space model implementation used by the solver.\"\"\"\n\n    @staticmethod\n    def _register_pytree_node():\n        def _sol_flatten(sol):\n            children = (\n                sol.t,\n                sol.u,\n                sol.u_std,\n                sol.marginals,\n                sol.posterior,\n                sol.output_scale,\n                sol.num_steps,\n            )\n            aux = (sol.ssm,)\n            return children, aux\n\n        def _sol_unflatten(aux, children):\n            (ssm,) = aux\n            t, u, u_std, marginals, posterior, output_scale, n = children\n            return IVPSolution(\n                t=t,\n                u=u,\n                u_std=u_std,\n                marginals=marginals,\n                posterior=posterior,\n                output_scale=output_scale,\n                num_steps=n,\n                ssm=ssm,\n            )\n\n        tree_util.register_pytree_node(IVPSolution, _sol_flatten, _sol_unflatten)\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.marginals","title":"<code>marginals: Any</code>  <code>instance-attribute</code>","text":"<p>Marginal distributions for each time point in the posterior distribution.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.num_steps","title":"<code>num_steps: Array</code>  <code>instance-attribute</code>","text":"<p>The number of solver steps taken at each time point.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.output_scale","title":"<code>output_scale: Array</code>  <code>instance-attribute</code>","text":"<p>The calibrated output scale of the probabilistic model.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.posterior","title":"<code>posterior: Any</code>  <code>instance-attribute</code>","text":"<p>A the full posterior distribution of the probabilistic numerical solution.</p> <p>Typically, a backward factorisation of the posterior.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.ssm","title":"<code>ssm: Any</code>  <code>instance-attribute</code>","text":"<p>State-space model implementation used by the solver.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.t","title":"<code>t: Array</code>  <code>instance-attribute</code>","text":"<p>Time points at which the IVP solution has been computed.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.u","title":"<code>u: Array</code>  <code>instance-attribute</code>","text":"<p>The mean of the IVP solution at each computed time point.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.IVPSolution.u_std","title":"<code>u_std: Array</code>  <code>instance-attribute</code>","text":"<p>The standard deviation of the IVP solution, indicating uncertainty.</p>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.dt0","title":"<code>dt0(vf_autonomous, initial_values, /, scale=0.01, nugget=1e-05)</code>","text":"<p>Propose an initial time-step.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def dt0(vf_autonomous, initial_values, /, scale=0.01, nugget=1e-5):\n    \"\"\"Propose an initial time-step.\"\"\"\n    u0, *_ = initial_values\n    f0 = vf_autonomous(*initial_values)\n\n    u0, _ = tree_util.ravel_pytree(u0)\n    f0, _ = tree_util.ravel_pytree(f0)\n\n    norm_y0 = linalg.vector_norm(u0)\n    norm_dy0 = linalg.vector_norm(f0) + nugget\n\n    return scale * norm_y0 / norm_dy0\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.dt0_adaptive","title":"<code>dt0_adaptive(vf, initial_values, /, t0, *, error_contraction_rate, rtol, atol)</code>","text":"<p>Propose an initial time-step as a function of the tolerances.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def dt0_adaptive(vf, initial_values, /, t0, *, error_contraction_rate, rtol, atol):\n    \"\"\"Propose an initial time-step as a function of the tolerances.\"\"\"\n    # Algorithm from:\n    # E. Hairer, S. P. Norsett G. Wanner,\n    # Solving Ordinary Differential Equations I: Nonstiff Problems, Sec. II.4.\n    # Implementation mostly copied from\n    #\n    # https://github.com/google/jax/blob/main/jax/experimental/ode.py\n    #\n\n    if len(initial_values) &gt; 1:\n        raise ValueError\n    y0 = initial_values[0]\n\n    f0 = vf(y0, t=t0)\n\n    y0, unravel = tree_util.ravel_pytree(y0)\n    f0, _ = tree_util.ravel_pytree(f0)\n\n    scale = atol + np.abs(y0) * rtol\n    d0, d1 = linalg.vector_norm(y0), linalg.vector_norm(f0)\n\n    dt0 = np.where((d0 &lt; 1e-5) | (d1 &lt; 1e-5), 1e-6, 0.01 * d0 / d1)\n\n    y1 = y0 + dt0 * f0\n    f1 = vf(unravel(y1), t=t0 + dt0)\n    f1, _ = tree_util.ravel_pytree(f1)\n    d2 = linalg.vector_norm((f1 - f0) / scale) / dt0\n\n    dt1 = np.where(\n        (d1 &lt;= 1e-15) &amp; (d2 &lt;= 1e-15),\n        np.maximum(1e-6, dt0 * 1e-3),\n        (0.01 / np.maximum(d1, d2)) ** (1.0 / (error_contraction_rate + 1.0)),\n    )\n    return np.minimum(100.0 * dt0, dt1)\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_adaptive_save_at","title":"<code>solve_adaptive_save_at(ssm_init, /, *, save_at, adaptive_solver, dt0, ssm, warn=True) -&gt; IVPSolution</code>","text":"<p>Solve an initial value problem and return the solution at a pre-determined grid.</p> <p>This algorithm implements the method by Kr\u00e4mer (2024). Please consider citing it if you use it for your research. A PDF is available here and Kr\u00e4mer's (2024) experiments are available here.</p> BibTex for Kr\u00e4mer (2024) <pre><code>@InProceedings{kramer2024adaptive,\n    title     = {Adaptive Probabilistic ODE Solvers Without Adaptive Memory\n                Requirements},\n    author    = {Kr\\\"{a}mer, Nicholas},\n    booktitle = {Proceedings of the First International Conference on\n                Probabilistic Numerics},\n    pages     = {12--24},\n    year      = {2025},\n    editor    = {Kanagawa, Motonobu and Cockayne, Jon and Gessner, Alexandra\n                and Hennig, Philipp},\n    volume    = {271},\n    series    = {Proceedings of Machine Learning Research},\n    publisher = {PMLR},\n    url       = {https://proceedings.mlr.press/v271/kramer25a.html}\n}\n</code></pre> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_adaptive_save_at(\n    ssm_init, /, *, save_at, adaptive_solver, dt0, ssm, warn=True\n) -&gt; IVPSolution:\n    r\"\"\"Solve an initial value problem and return the solution at a pre-determined grid.\n\n    This algorithm implements the method by Kr\u00e4mer (2024). Please consider citing it\n    if you use it for your research. A PDF is available\n    [here](https://arxiv.org/abs/2410.10530) and Kr\u00e4mer's (2024) experiments are\n    available [here](https://github.com/pnkraemer/code-adaptive-prob-ode-solvers).\n\n    ??? note \"BibTex for Kr\u00e4mer (2024)\"\n        ```bibtex\n        @InProceedings{kramer2024adaptive,\n            title     = {Adaptive Probabilistic ODE Solvers Without Adaptive Memory\n                        Requirements},\n            author    = {Kr\\\"{a}mer, Nicholas},\n            booktitle = {Proceedings of the First International Conference on\n                        Probabilistic Numerics},\n            pages     = {12--24},\n            year      = {2025},\n            editor    = {Kanagawa, Motonobu and Cockayne, Jon and Gessner, Alexandra\n                        and Hennig, Philipp},\n            volume    = {271},\n            series    = {Proceedings of Machine Learning Research},\n            publisher = {PMLR},\n            url       = {https://proceedings.mlr.press/v271/kramer25a.html}\n        }\n        ```\n    \"\"\"\n    if not adaptive_solver.solver.is_suitable_for_save_at and warn:\n        msg = (\n            f\"Strategy {adaptive_solver.solver} should not \"\n            f\"be used in solve_adaptive_save_at. \"\n        )\n        warnings.warn(msg, stacklevel=1)\n\n    (_t, solution_save_at), _, num_steps = _solve_adaptive_save_at(\n        save_at[0],\n        ssm_init,\n        save_at=save_at[1:],\n        adaptive_solver=adaptive_solver,\n        dt0=dt0,\n    )\n\n    # I think the user expects the initial condition to be part of the state\n    # (as well as marginals), so we compute those things here\n    posterior_save_at, output_scale = solution_save_at\n    _tmp = _userfriendly_output(posterior=posterior_save_at, ssm_init=ssm_init, ssm=ssm)\n    marginals, posterior = _tmp\n    u = ssm.stats.qoi_from_sample(marginals.mean)\n    std = ssm.stats.standard_deviation(marginals)\n    u_std = ssm.stats.qoi_from_sample(std)\n    return IVPSolution(\n        t=save_at,\n        u=u,\n        u_std=u_std,\n        marginals=marginals,\n        posterior=posterior,\n        output_scale=output_scale,\n        num_steps=num_steps,\n        ssm=ssm,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_adaptive_save_every_step","title":"<code>solve_adaptive_save_every_step(ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm) -&gt; IVPSolution</code>","text":"<p>Solve an initial value problem and save every step.</p> <p>This function uses a native-Python while loop.</p> <p>Warning</p> <p>Not JITable, not reverse-mode-differentiable.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_adaptive_save_every_step(\n    ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm\n) -&gt; IVPSolution:\n    \"\"\"Solve an initial value problem and save every step.\n\n    This function uses a native-Python while loop.\n\n    !!! warning\n        Not JITable, not reverse-mode-differentiable.\n    \"\"\"\n    if not adaptive_solver.solver.is_suitable_for_save_every_step:\n        msg = (\n            f\"Strategy {adaptive_solver.solver} should not \"\n            f\"be used in solve_adaptive_save_every_step.\"\n        )\n        warnings.warn(msg, stacklevel=1)\n\n    generator = _solution_generator(\n        t0, ssm_init, t1=t1, adaptive_solver=adaptive_solver, dt0=dt0\n    )\n    tmp = tree_array_util.tree_stack(list(generator))\n    (t, solution_every_step), _dt, num_steps = tmp\n\n    # I think the user expects the initial time-point to be part of the grid\n    # (Even though t0 is not computed by this function)\n    t = np.concatenate((np.atleast_1d(t0), t))\n\n    # I think the user expects marginals, so we compute them here\n    posterior, output_scale = solution_every_step\n    _tmp = _userfriendly_output(posterior=posterior, ssm_init=ssm_init, ssm=ssm)\n    marginals, posterior = _tmp\n\n    u = ssm.stats.qoi_from_sample(marginals.mean)\n    std = ssm.stats.standard_deviation(marginals)\n    u_std = ssm.stats.qoi_from_sample(std)\n    return IVPSolution(\n        t=t,\n        u=u,\n        u_std=u_std,\n        ssm=ssm,\n        marginals=marginals,\n        posterior=posterior,\n        output_scale=output_scale,\n        num_steps=num_steps,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_adaptive_terminal_values","title":"<code>solve_adaptive_terminal_values(ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm) -&gt; IVPSolution</code>","text":"<p>Simulate the terminal values of an initial value problem.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_adaptive_terminal_values(\n    ssm_init, /, *, t0, t1, adaptive_solver, dt0, ssm\n) -&gt; IVPSolution:\n    \"\"\"Simulate the terminal values of an initial value problem.\"\"\"\n    save_at = np.asarray([t0, t1])\n    solution = solve_adaptive_save_at(\n        ssm_init,\n        save_at=save_at,\n        adaptive_solver=adaptive_solver,\n        dt0=dt0,\n        ssm=ssm,\n        warn=False,  # Turn off warnings because any solver goes for terminal values\n    )\n    return tree_util.tree_map(lambda s: s[-1], solution)\n</code></pre>"},{"location":"api_docs/ivpsolve/#probdiffeq.ivpsolve.solve_fixed_grid","title":"<code>solve_fixed_grid(ssm_init, /, *, grid, solver, ssm) -&gt; IVPSolution</code>","text":"<p>Solve an initial value problem on a fixed, pre-determined grid.</p> Source code in <code>probdiffeq/ivpsolve.py</code> <pre><code>def solve_fixed_grid(ssm_init, /, *, grid, solver, ssm) -&gt; IVPSolution:\n    \"\"\"Solve an initial value problem on a fixed, pre-determined grid.\"\"\"\n    # Compute the solution\n\n    def body_fn(s, dt):\n        _error, s_new = solver.step(state=s, dt=dt)\n        return s_new, s_new\n\n    t0 = grid[0]\n    state0 = solver.init(t0, ssm_init)\n    _, result_state = control_flow.scan(body_fn, init=state0, xs=np.diff(grid))\n    _t, (posterior, output_scale) = solver.extract(result_state)\n\n    # I think the user expects marginals, so we compute them here\n    _tmp = _userfriendly_output(posterior=posterior, ssm_init=ssm_init, ssm=ssm)\n    marginals, posterior = _tmp\n\n    u = ssm.stats.qoi_from_sample(marginals.mean)\n    std = ssm.stats.standard_deviation(marginals)\n    u_std = ssm.stats.qoi_from_sample(std)\n    return IVPSolution(\n        t=grid,\n        u=u,\n        u_std=u_std,\n        ssm=ssm,\n        marginals=marginals,\n        posterior=posterior,\n        output_scale=output_scale,\n        num_steps=np.arange(1.0, len(grid)),\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/","title":"ivpsolvers","text":"<p>Probabilistic IVP solvers.</p>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.adaptive","title":"<code>adaptive(slvr, /, *, ssm, atol=0.0001, rtol=0.01, control=None, norm_ord=None, clip_dt: bool = False, eps: float | None = None)</code>","text":"<p>Make an IVP solver adaptive.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def adaptive(\n    slvr,\n    /,\n    *,\n    ssm,\n    atol=1e-4,\n    rtol=1e-2,\n    control=None,\n    norm_ord=None,\n    clip_dt: bool = False,\n    eps: float | None = None,\n):\n    \"\"\"Make an IVP solver adaptive.\"\"\"\n    if control is None:\n        control = control_proportional_integral()\n    if eps is None:\n        eps = 10 * np.finfo_eps(float)\n    return _AdaSolver(\n        slvr,\n        ssm=ssm,\n        atol=atol,\n        rtol=rtol,\n        control=control,\n        norm_ord=norm_ord,\n        clip_dt=clip_dt,\n        eps=eps,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.control_integral","title":"<code>control_integral(*, safety=0.95, factor_min=0.2, factor_max=10.0) -&gt; _Controller[None]</code>","text":"<p>Construct an integral-controller.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def control_integral(\n    *, safety=0.95, factor_min=0.2, factor_max=10.0\n) -&gt; _Controller[None]:\n    \"\"\"Construct an integral-controller.\"\"\"\n\n    def init(_dt, /) -&gt; None:\n        return None\n\n    def apply(dt, _state, /, *, error_power):\n        # error_power = error_norm ** (-1.0 / error_contraction_rate)\n        scale_factor_unclipped = safety * error_power\n\n        scale_factor_clipped_min = np.minimum(scale_factor_unclipped, factor_max)\n        scale_factor = np.maximum(factor_min, scale_factor_clipped_min)\n        return scale_factor * dt, None\n\n    return _Controller(init=init, apply=apply)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.control_proportional_integral","title":"<code>control_proportional_integral(*, safety=0.95, factor_min=0.2, factor_max=10.0, power_integral_unscaled=0.3, power_proportional_unscaled=0.4) -&gt; _Controller[float]</code>","text":"<p>Construct a proportional-integral-controller with time-clipping.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def control_proportional_integral(\n    *,\n    safety=0.95,\n    factor_min=0.2,\n    factor_max=10.0,\n    power_integral_unscaled=0.3,\n    power_proportional_unscaled=0.4,\n) -&gt; _Controller[float]:\n    \"\"\"Construct a proportional-integral-controller with time-clipping.\"\"\"\n\n    def init(_dt: float, /) -&gt; float:\n        return 1.0\n\n    def apply(dt: float, error_power_prev: float, /, *, error_power):\n        # Equivalent: error_power = error_norm ** (-1.0 / error_contraction_rate)\n        a1 = error_power**power_integral_unscaled\n        a2 = (error_power / error_power_prev) ** power_proportional_unscaled\n        scale_factor_unclipped = safety * a1 * a2\n\n        scale_factor_clipped_min = np.minimum(scale_factor_unclipped, factor_max)\n        scale_factor = np.maximum(factor_min, scale_factor_clipped_min)\n\n        # &gt;= 1.0 because error_power is 1/scaled_error_norm\n        error_power_prev = np.where(error_power &gt;= 1.0, error_power, error_power_prev)\n\n        dt_proposed = scale_factor * dt\n        return dt_proposed, error_power_prev\n\n    return _Controller(init=init, apply=apply)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_slr0","title":"<code>correction_slr0(vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0) -&gt; _Correction</code>","text":"<p>Zeroth-order statistical linear regression.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_slr0(\n    vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0\n) -&gt; _Correction:\n    \"\"\"Zeroth-order statistical linear regression.\"\"\"\n    linearize = ssm.linearise.ode_statistical_0th(cubature_fun, damp=damp)\n    return _Correction(\n        ssm=ssm,\n        vector_field=vector_field,\n        ode_order=1,\n        linearize=linearize,\n        name=\"SLR0\",\n        re_linearize=True,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_slr1","title":"<code>correction_slr1(vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0) -&gt; _Correction</code>","text":"<p>First-order statistical linear regression.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_slr1(\n    vector_field, *, ssm, cubature_fun=cubature_third_order_spherical, damp: float = 0.0\n) -&gt; _Correction:\n    \"\"\"First-order statistical linear regression.\"\"\"\n    linearize = ssm.linearise.ode_statistical_1st(cubature_fun, damp=damp)\n    return _Correction(\n        ssm=ssm,\n        vector_field=vector_field,\n        ode_order=1,\n        linearize=linearize,\n        name=\"SLR1\",\n        re_linearize=True,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_ts0","title":"<code>correction_ts0(vector_field, *, ssm, ode_order=1, damp: float = 0.0) -&gt; _Correction</code>","text":"<p>Zeroth-order Taylor linearisation.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_ts0(vector_field, *, ssm, ode_order=1, damp: float = 0.0) -&gt; _Correction:\n    \"\"\"Zeroth-order Taylor linearisation.\"\"\"\n    linearize = ssm.linearise.ode_taylor_0th(ode_order=ode_order, damp=damp)\n    return _Correction(\n        name=\"TS0\",\n        vector_field=vector_field,\n        ode_order=ode_order,\n        ssm=ssm,\n        linearize=linearize,\n        re_linearize=False,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.correction_ts1","title":"<code>correction_ts1(vector_field, *, ssm, ode_order=1, damp: float = 0.0, jvp_probes=10, jvp_probes_seed=1) -&gt; _Correction</code>","text":"<p>First-order Taylor linearisation.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def correction_ts1(\n    vector_field,\n    *,\n    ssm,\n    ode_order=1,\n    damp: float = 0.0,\n    jvp_probes=10,\n    jvp_probes_seed=1,\n) -&gt; _Correction:\n    \"\"\"First-order Taylor linearisation.\"\"\"\n    assert jvp_probes &gt; 0\n    linearize = ssm.linearise.ode_taylor_1st(\n        ode_order=ode_order,\n        damp=damp,\n        jvp_probes=jvp_probes,\n        jvp_probes_seed=jvp_probes_seed,\n    )\n    return _Correction(\n        name=\"TS1\",\n        vector_field=vector_field,\n        ode_order=ode_order,\n        ssm=ssm,\n        linearize=linearize,\n        re_linearize=False,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.cubature_gauss_hermite","title":"<code>cubature_gauss_hermite(input_shape, degree=5) -&gt; _PositiveCubatureRule</code>","text":"<p>(Statistician's) Gauss-Hermite cubature.</p> <p>The number of cubature points is <code>prod(input_shape)**degree</code>.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def cubature_gauss_hermite(input_shape, degree=5) -&gt; _PositiveCubatureRule:\n    \"\"\"(Statistician's) Gauss-Hermite cubature.\n\n    The number of cubature points is `prod(input_shape)**degree`.\n    \"\"\"\n    assert len(input_shape) == 1\n    (dim,) = input_shape\n\n    # Roots of the probabilist/statistician's Hermite polynomials (in Numpy...)\n    _roots = special.roots_hermitenorm(n=degree, mu=True)\n    pts, weights, sum_of_weights = _roots\n    weights = weights / sum_of_weights\n\n    # Transform into jax arrays and take square root of weights\n    pts = np.asarray(pts)\n    weights_sqrtm = np.sqrt(np.asarray(weights))\n\n    # Build a tensor grid and return class\n    tensor_pts = _tensor_points(pts, d=dim)\n    tensor_weights_sqrtm = _tensor_weights(weights_sqrtm, d=dim)\n    return _PositiveCubatureRule(points=tensor_pts, weights_sqrtm=tensor_weights_sqrtm)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.cubature_third_order_spherical","title":"<code>cubature_third_order_spherical(input_shape) -&gt; _PositiveCubatureRule</code>","text":"<p>Third-order spherical cubature integration.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def cubature_third_order_spherical(input_shape) -&gt; _PositiveCubatureRule:\n    \"\"\"Third-order spherical cubature integration.\"\"\"\n    assert len(input_shape) &lt;= 1\n    if len(input_shape) == 1:\n        (d,) = input_shape\n        points_mat, weights_sqrtm = _third_order_spherical_params(d=d)\n        return _PositiveCubatureRule(points=points_mat, weights_sqrtm=weights_sqrtm)\n\n    # If input_shape == (), compute weights via input_shape=(1,)\n    # and 'squeeze' the points.\n    points_mat, weights_sqrtm = _third_order_spherical_params(d=1)\n    (S, _) = points_mat.shape\n    points = np.reshape(points_mat, (S,))\n    return _PositiveCubatureRule(points=points, weights_sqrtm=weights_sqrtm)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.cubature_unscented_transform","title":"<code>cubature_unscented_transform(input_shape, r=1.0) -&gt; _PositiveCubatureRule</code>","text":"<p>Unscented transform.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def cubature_unscented_transform(input_shape, r=1.0) -&gt; _PositiveCubatureRule:\n    \"\"\"Unscented transform.\"\"\"\n    assert len(input_shape) &lt;= 1\n    if len(input_shape) == 1:\n        (d,) = input_shape\n        points_mat, weights_sqrtm = _unscented_transform_params(d=d, r=r)\n        return _PositiveCubatureRule(points=points_mat, weights_sqrtm=weights_sqrtm)\n\n    # If input_shape == (), compute weights via input_shape=(1,)\n    # and 'squeeze' the points.\n    points_mat, weights_sqrtm = _unscented_transform_params(d=1, r=r)\n    (S, _) = points_mat.shape\n    points = np.reshape(points_mat, (S,))\n    return _PositiveCubatureRule(points=points, weights_sqrtm=weights_sqrtm)\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.prior_wiener_integrated","title":"<code>prior_wiener_integrated(tcoeffs, *, ssm_fact: str, output_scale: ArrayLike | None = None, damp: float = 0.0)</code>","text":"<p>Construct an adaptive(/continuous-time), multiply-integrated Wiener process.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def prior_wiener_integrated(\n    tcoeffs, *, ssm_fact: str, output_scale: ArrayLike | None = None, damp: float = 0.0\n):\n    \"\"\"Construct an adaptive(/continuous-time), multiply-integrated Wiener process.\"\"\"\n    ssm = impl.choose(ssm_fact, tcoeffs_like=tcoeffs)\n\n    # TODO: should the output_scale be an argument to solve()?\n    # TODO: should the output scale (and all 'damp'-like factors)\n    #       mirror the pytree structure of 'tcoeffs'?\n    if output_scale is None:\n        output_scale = np.ones_like(ssm.prototypes.output_scale())\n\n    discretize = ssm.conditional.ibm_transitions(base_scale=output_scale)\n\n    # Increase damping to get visually more pleasing uncertainties\n    #  and more numerical robustness for\n    #  high-order solvers in low precision arithmetic\n    init = ssm.normal.from_tcoeffs(tcoeffs, damp=damp)\n    return init, discretize, ssm\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.prior_wiener_integrated_discrete","title":"<code>prior_wiener_integrated_discrete(ts, *args, **kwargs)</code>","text":"<p>Compute a time-discretized, multiply-integrated Wiener process.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def prior_wiener_integrated_discrete(ts, *args, **kwargs):\n    \"\"\"Compute a time-discretized, multiply-integrated Wiener process.\"\"\"\n    init, discretize, ssm = prior_wiener_integrated(*args, **kwargs)\n    scales = np.ones_like(ssm.prototypes.output_scale())\n    discretize_vmap = functools.vmap(discretize, in_axes=(0, None))\n    conditionals = discretize_vmap(np.diff(ts), scales)\n    return init, conditionals, ssm\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.solver","title":"<code>solver(strategy, *, correction, prior, ssm)</code>","text":"<p>Create a solver that does not calibrate the output scale automatically.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def solver(strategy, *, correction, prior, ssm):\n    \"\"\"Create a solver that does not calibrate the output scale automatically.\"\"\"\n\n    def step(state: _State, *, dt, calibration):\n        del calibration  # unused\n\n        u_step_from = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n\n        # Estimate the error\n        transition = prior(dt, state.output_scale)\n        mean = ssm.stats.mean(state.rv)\n        hidden = ssm.conditional.apply(mean, transition)\n        t = state.t + dt\n        error, _, correction_state = correction.estimate_error(\n            hidden, state.correction_state, t=t\n        )\n\n        # Do the full extrapolation step (reuse the transition)\n        hidden, extra = strategy.extrapolate(\n            state.rv, state.strategy_state, transition=transition\n        )\n\n        # Do the full correction step\n        hidden, _, correction_state = correction.correct(hidden, correction_state, t=t)\n        state = _State(\n            t=t,\n            rv=hidden,\n            strategy_state=extra,\n            correction_state=correction_state,\n            output_scale=state.output_scale,\n        )\n\n        # Normalise the error\n        u_proposed = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n        reference = np.maximum(np.abs(u_proposed), np.abs(u_step_from))\n        error = _ErrorEstimate(dt * error, reference=reference)\n        return error, state\n\n    return _ProbabilisticSolver(\n        ssm=ssm,\n        prior=prior,\n        strategy=strategy,\n        correction=correction,\n        calibration=_calibration_none(ssm=ssm),\n        step_implementation=step,\n        name=\"Probabilistic solver\",\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.solver_dynamic","title":"<code>solver_dynamic(strategy, *, correction, prior, ssm)</code>","text":"<p>Create a solver that calibrates the output scale dynamically.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def solver_dynamic(strategy, *, correction, prior, ssm):\n    \"\"\"Create a solver that calibrates the output scale dynamically.\"\"\"\n\n    def step_dynamic(state, /, *, dt, calibration):\n        u_step_from = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n\n        # Estimate error and calibrate the output scale\n        ones = np.ones_like(ssm.prototypes.output_scale())\n        transition = prior(dt, ones)\n        mean = ssm.stats.mean(state.rv)\n        hidden = ssm.conditional.apply(mean, transition)\n\n        t = state.t + dt\n        error, observed, correction_state = correction.estimate_error(\n            hidden, state.correction_state, t=t\n        )\n        output_scale = calibration.update(state.output_scale, observed=observed)\n\n        # Do the full extrapolation with the calibrated output scale\n        scale, _ = calibration.extract(output_scale)\n        transition = prior(dt, scale)\n        hidden, extra = strategy.extrapolate(\n            state.rv, state.strategy_state, transition=transition\n        )\n\n        # Do the full correction step\n        hidden, _, correction_state = correction.correct(hidden, correction_state, t=t)\n\n        # Return solution\n        state = _State(\n            t=t,\n            rv=hidden,\n            strategy_state=extra,\n            correction_state=correction_state,\n            output_scale=output_scale,\n        )\n\n        # Normalise the error\n        u_proposed = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n        reference = np.maximum(np.abs(u_proposed), np.abs(u_step_from))\n        error = _ErrorEstimate(dt * error, reference=reference)\n        return error, state\n\n    return _ProbabilisticSolver(\n        prior=prior,\n        ssm=ssm,\n        strategy=strategy,\n        correction=correction,\n        calibration=_calibration_most_recent(ssm=ssm),\n        name=\"Dynamic probabilistic solver\",\n        step_implementation=step_dynamic,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.solver_mle","title":"<code>solver_mle(strategy, *, correction, prior, ssm)</code>","text":"<p>Create a solver that calibrates the output scale via maximum-likelihood.</p> <p>Warning: needs to be combined with a call to stats.calibrate() after solving if the MLE-calibration shall be used.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def solver_mle(strategy, *, correction, prior, ssm):\n    \"\"\"Create a solver that calibrates the output scale via maximum-likelihood.\n\n    Warning: needs to be combined with a call to stats.calibrate()\n    after solving if the MLE-calibration shall be *used*.\n    \"\"\"\n\n    def step_mle(state, /, *, dt, calibration):\n        u_step_from = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n\n        # Estimate the error\n        output_scale_prior, _calibrated = calibration.extract(state.output_scale)\n        transition = prior(dt, output_scale_prior)\n        mean = ssm.stats.mean(state.rv)\n        mean_extra = ssm.conditional.apply(mean, transition)\n        t = state.t + dt\n        error, _, correction_state = correction.estimate_error(\n            mean_extra, state.correction_state, t=t\n        )\n\n        # Do the full prediction step (reuse previous discretisation)\n        hidden, extra = strategy.extrapolate(\n            state.rv, state.strategy_state, transition=transition\n        )\n\n        # Do the full correction step\n        hidden, observed, corr_state = correction.correct(hidden, correction_state, t=t)\n\n        # Calibrate the output scale\n        output_scale = calibration.update(state.output_scale, observed=observed)\n\n        # Normalise the error\n\n        state = _State(\n            t=t,\n            rv=hidden,\n            strategy_state=extra,\n            correction_state=corr_state,\n            output_scale=output_scale,\n        )\n        u_proposed = tree_util.ravel_pytree(ssm.unravel(state.rv.mean)[0])[0]\n        reference = np.maximum(np.abs(u_proposed), np.abs(u_step_from))\n        error = _ErrorEstimate(dt * error, reference=reference)\n        return error, state\n\n    return _ProbabilisticSolver(\n        ssm=ssm,\n        name=\"Probabilistic solver with MLE calibration\",\n        prior=prior,\n        calibration=_calibration_running_mean(ssm=ssm),\n        step_implementation=step_mle,\n        strategy=strategy,\n        correction=correction,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.strategy_filter","title":"<code>strategy_filter(*, ssm) -&gt; _Strategy</code>","text":"<p>Construct a filter.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def strategy_filter(*, ssm) -&gt; _Strategy:\n    \"\"\"Construct a filter.\"\"\"\n\n    @containers.dataclass\n    class Filter(_Strategy):\n        def init(self, sol, /):\n            return sol, None\n\n        def extrapolate(self, rv, aux, /, *, transition):\n            del aux\n            rv = self.ssm.conditional.marginalise(rv, transition)\n\n            return rv, None\n\n        def extract(self, hidden_state, _extra, /):\n            return hidden_state\n\n        def interpolate(self, state_t0, state_t1, dt0, dt1, output_scale, *, prior):\n            # todo: by ditching marginal_t1 and dt1, this function _extrapolates\n            #  (no *inter*polation happening)\n            del dt1\n            marginal_t1, _ = state_t1\n\n            hidden, extra = state_t0\n            prior0 = prior(dt0, output_scale)\n            hidden, extra = self.extrapolate(hidden, extra, transition=prior0)\n\n            # Consistent state-types in interpolation result.\n            interp = (hidden, extra)\n            step_from = (marginal_t1, None)\n            return _InterpRes(\n                step_from=step_from, interpolated=interp, interp_from=interp\n            )\n\n        def interpolate_at_t1(\n            self, state_t0, state_t1, dt0, dt1, output_scale, *, prior\n        ):\n            del prior\n            del state_t0\n            del dt0\n            del dt1\n            del output_scale\n            rv, extra = state_t1\n            return _InterpRes((rv, extra), (rv, extra), (rv, extra))\n\n    return Filter(\n        ssm=ssm,\n        is_suitable_for_save_at=True,\n        is_suitable_for_save_every_step=True,\n        is_suitable_for_offgrid_marginals=True,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.strategy_fixedpoint","title":"<code>strategy_fixedpoint(*, ssm) -&gt; _Strategy</code>","text":"<p>Construct a fixedpoint-smoother.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def strategy_fixedpoint(*, ssm) -&gt; _Strategy:\n    \"\"\"Construct a fixedpoint-smoother.\"\"\"\n\n    @containers.dataclass\n    class FixedPoint(_Strategy):\n        def init(self, sol, /):\n            cond = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            return sol, cond\n\n        def extrapolate(self, rv, bw0, /, *, transition):\n            extrapolated, cond = self.ssm.conditional.revert(rv, transition)\n            cond = self.ssm.conditional.merge(bw0, cond)\n            return extrapolated, cond\n\n        def extract(self, hidden_state, extra, /):\n            return stats.MarkovSeq(init=hidden_state, conditional=extra)\n\n        def interpolate_at_t1(\n            self, state_t0, state_t1, *, dt0, dt1, output_scale, prior\n        ):\n            del prior\n            del state_t0\n            del dt0\n            del dt1\n            del output_scale\n            rv, extra = state_t1\n            cond_identity = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            return _InterpRes((rv, cond_identity), (rv, extra), (rv, cond_identity))\n\n        def interpolate(self, state_t0, state_t1, *, dt0, dt1, output_scale, prior):\n            \"\"\"Interpolate.\n\n            A fixed-point smoother interpolates by\n\n            * Extrapolating from t0 to t, which gives the \"filtering\" marginal\n            and the backward transition from t to t0.\n            * Extrapolating from t to t1, which gives another \"filtering\" marginal\n            and the backward transition from t1 to t.\n            * Applying the t1-to-t backward transition\n            to compute the interpolation result.\n            This intermediate result is informed about its \"right-hand side\" datum.\n\n            The difference to smoother-interpolation is quite subtle:\n\n            * The backward transition of the solution at 't'\n            is merged with that at 't0'.\n            The reason is that the backward transition at 't0' knows\n            \"how to get to the quantity of interest\",\n            and this is precisely what we want to interpolate.\n            * Subsequent interpolations do not continue from the value at 't', but\n            from a very similar value where the backward transition\n            is replaced with an identity. The reason is that the interpolated solution\n            becomes the new quantity of interest, and subsequent interpolations\n            need to learn how to get here.\n            * Subsequent solver steps do not continue from the value at 't1',\n            but the value at 't1' where the backward model is replaced by\n            the 't1-to-t' backward model. The reason is similar to the above:\n            future steps need to know \"how to get back to the quantity of interest\",\n            which is the interpolated solution.\n\n            These distinctions are precisely why we need three fields\n            in every interpolation result:\n                the solution,\n                the continue-interpolation-from-here,\n                and the continue-stepping-from-here.\n            All three are different for fixed point smoothers.\n            (Really, I try removing one of them monthly and\n            then don't understand why tests fail.)\n            \"\"\"\n            marginal_t1, _ = state_t1\n            # Extrapolate from t0 to t, and from t to t1.\n            # This yields all building blocks.\n            prior0 = prior(dt0, output_scale)\n            extrapolated_t = self.extrapolate(*state_t0, transition=prior0)\n            conditional_id = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            previous_new = (extrapolated_t[0], conditional_id)\n\n            prior1 = prior(dt1, output_scale)\n            extrapolated_t1 = self.extrapolate(*previous_new, transition=prior1)\n\n            # Marginalise from t1 to t to obtain the interpolated solution.\n            conditional_t1_to_t = extrapolated_t1[1]\n            rv_at_t = self.ssm.conditional.marginalise(marginal_t1, conditional_t1_to_t)\n\n            # Return the right combination of marginals and conditionals.\n            return _InterpRes(\n                step_from=(marginal_t1, conditional_t1_to_t),\n                interpolated=(rv_at_t, extrapolated_t[1]),\n                interp_from=previous_new,\n            )\n\n    return FixedPoint(\n        ssm=ssm,\n        is_suitable_for_save_at=True,\n        is_suitable_for_save_every_step=False,\n        is_suitable_for_offgrid_marginals=False,\n    )\n</code></pre>"},{"location":"api_docs/ivpsolvers/#probdiffeq.ivpsolvers.strategy_smoother","title":"<code>strategy_smoother(*, ssm) -&gt; _Strategy</code>","text":"<p>Construct a smoother.</p> Source code in <code>probdiffeq/ivpsolvers.py</code> <pre><code>def strategy_smoother(*, ssm) -&gt; _Strategy:\n    \"\"\"Construct a smoother.\"\"\"\n\n    @containers.dataclass\n    class Smoother(_Strategy):\n        def init(self, sol, /):\n            # Special case for implementing offgrid-marginals...\n            if isinstance(sol, stats.MarkovSeq):\n                rv = sol.init\n                cond = sol.conditional\n            else:\n                rv = sol\n                cond = self.ssm.conditional.identity(ssm.num_derivatives + 1)\n            return rv, cond\n\n        def extrapolate(self, rv, aux, /, *, transition):\n            del aux\n            return self.ssm.conditional.revert(rv, transition)\n\n        def extract(self, hidden_state, extra, /):\n            return stats.MarkovSeq(init=hidden_state, conditional=extra)\n\n        def interpolate(self, state_t0, state_t1, *, dt0, dt1, output_scale, prior):\n            \"\"\"Interpolate.\n\n            A smoother interpolates by_\n            * Extrapolating from t0 to t, which gives the \"filtering\" marginal\n            and the backward transition from t to t0.\n            * Extrapolating from t to t1, which gives another \"filtering\" marginal\n            and the backward transition from t1 to t.\n            * Applying the new t1-to-t backward transition to compute the interpolation.\n            This intermediate result is informed about its \"right-hand side\" datum.\n\n            Subsequent interpolations continue from the value at 't'.\n            Subsequent IVP solver steps continue from the value at 't1'.\n            \"\"\"\n            # TODO: if we pass prior1 and prior2, then\n            #       we don't have to pass dt0, dt1, output_scale, and prior...\n\n            # Extrapolate from t0 to t, and from t to t1.\n            prior0 = prior(dt0, output_scale)\n            extrapolated_t = self.extrapolate(*state_t0, transition=prior0)\n            prior1 = prior(dt1, output_scale)\n            extrapolated_t1 = self.extrapolate(*extrapolated_t, transition=prior1)\n\n            # Marginalise from t1 to t to obtain the interpolated solution.\n            marginal_t1, _ = state_t1\n            conditional_t1_to_t = extrapolated_t1[1]\n            rv_at_t = self.ssm.conditional.marginalise(marginal_t1, conditional_t1_to_t)\n            solution_at_t = (rv_at_t, extrapolated_t[1])\n\n            # The state at t1 gets a new backward model;\n            # (it must remember how to get back to t, not to t0).\n            solution_at_t1 = (marginal_t1, conditional_t1_to_t)\n            return _InterpRes(\n                step_from=solution_at_t1,\n                interpolated=solution_at_t,\n                interp_from=solution_at_t,\n            )\n\n        def interpolate_at_t1(\n            self, state_t0, state_t1, *, dt0, dt1, output_scale, prior\n        ):\n            del prior\n            del state_t0\n            del dt0\n            del dt1\n            del output_scale\n            return _InterpRes(state_t1, state_t1, state_t1)\n\n    return Smoother(\n        ssm=ssm,\n        is_suitable_for_save_at=False,\n        is_suitable_for_save_every_step=True,\n        is_suitable_for_offgrid_marginals=True,\n    )\n</code></pre>"},{"location":"api_docs/stats/","title":"stats","text":"<p>Interact with IVP solutions.</p> <p>For example, this module contains functionality to compute off-grid marginals, or to evaluate marginal likelihoods of observations of the solutions.</p>"},{"location":"api_docs/stats/#probdiffeq.stats.MarkovSeq","title":"<code>MarkovSeq</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Markov sequence.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>class MarkovSeq(containers.NamedTuple):\n    \"\"\"Markov sequence.\"\"\"\n\n    init: Any\n    conditional: Any\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.calibrate","title":"<code>calibrate(x, /, output_scale, *, ssm)</code>","text":"<p>Calibrated a posterior distribution of an IVP solution.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def calibrate(x, /, output_scale, *, ssm):\n    \"\"\"Calibrated a posterior distribution of an IVP solution.\"\"\"\n    if np.ndim(output_scale) &gt; np.ndim(ssm.prototypes.output_scale()):\n        output_scale = output_scale[-1]\n    if isinstance(x, MarkovSeq):\n        return _markov_rescale_cholesky(x, output_scale, ssm=ssm)\n    return ssm.stats.rescale_cholesky(x, output_scale)\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.log_marginal_likelihood","title":"<code>log_marginal_likelihood(u, /, *, standard_deviation, posterior, ssm)</code>","text":"<p>Compute the log-marginal-likelihood of observations of the IVP solution.</p> <p>Note</p> <p>Use <code>log_marginal_likelihood_terminal_values</code> to compute the log-likelihood at the terminal values.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <p>Observation. Expected to match the ODE's type/shape.</p> required <code>standard_deviation</code> <p>Standard deviation of the observation. Expected to match 'u's Pytree structure, but every leaf must be a scalar.</p> required <code>posterior</code> <p>Posterior distribution. Expected to correspond to a solution of an ODE with shape (d,).</p> required Source code in <code>probdiffeq/stats.py</code> <pre><code>def log_marginal_likelihood(u, /, *, standard_deviation, posterior, ssm):\n    \"\"\"Compute the log-marginal-likelihood of observations of the IVP solution.\n\n    !!! note\n        Use `log_marginal_likelihood_terminal_values`\n        to compute the log-likelihood at the terminal values.\n\n    Parameters\n    ----------\n    u\n        Observation. Expected to match the ODE's type/shape.\n    standard_deviation\n        Standard deviation of the observation. Expected to match 'u's\n        Pytree structure, but every leaf must be a scalar.\n    posterior\n        Posterior distribution.\n        Expected to correspond to a solution of an ODE with shape (d,).\n    \"\"\"\n    [u_leaves], u_structure = tree_util.tree_flatten(u)\n    [std_leaves], std_structure = tree_util.tree_flatten(standard_deviation)\n\n    if u_structure != std_structure:\n        msg = (\n            f\"Observation-noise tree structure {std_structure} \"\n            f\"does not match the observation structure {u_structure}. \"\n        )\n        raise ValueError(msg)\n\n    qoi_flat, _ = tree_util.ravel_pytree(ssm.prototypes.qoi())\n    if np.ndim(std_leaves) &lt; 1 or np.ndim(u_leaves) != np.ndim(qoi_flat) + 1:\n        msg = (\n            f\"Time-series solution expected. \"\n            f\"ndim={np.ndim(u_leaves)}, shape={np.shape(u_leaves)} received.\"\n        )\n        raise ValueError(msg)\n\n    if len(u_leaves) != len(np.asarray(std_leaves)):\n        msg = (\n            f\"Observation-noise shape {np.shape(std_leaves)} \"\n            f\"does not match the observation shape {np.shape(u_leaves)}. \"\n        )\n        raise ValueError(msg)\n\n    if not isinstance(posterior, MarkovSeq):\n        msg1 = \"Time-series marginal likelihoods \"\n        msg2 = \"cannot be computed with a filtering solution.\"\n        raise TypeError(msg1 + msg2)\n\n    # Generate an observation-model for the QOI\n\n    model_fun = functools.vmap(ssm.conditional.to_derivative, in_axes=(None, 0, 0))\n    models = model_fun(0, u, standard_deviation)\n\n    # Select the terminal variable\n    rv = tree_util.tree_map(lambda s: s[-1, ...], posterior.init)\n\n    # Run the reverse Kalman filter\n    estimator = filter_util.kalmanfilter_with_marginal_likelihood(ssm=ssm)\n    (_corrected, _num_data, logpdf), _ = filter_util.estimate_rev(\n        np.zeros_like(u_leaves),\n        init=rv,\n        prior_transitions=posterior.conditional,\n        observation_model=models,\n        estimator=estimator,\n    )\n\n    # Return only the logpdf\n    return logpdf\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.log_marginal_likelihood_terminal_values","title":"<code>log_marginal_likelihood_terminal_values(u, /, *, standard_deviation, posterior, ssm)</code>","text":"<p>Compute the log-marginal-likelihood at the terminal value.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <p>Observation. Expected to have shape (d,) for an ODE with shape (d,).</p> required <code>standard_deviation</code> <p>Standard deviation of the observation. Expected to be a scalar.</p> required <code>posterior</code> <p>Posterior distribution. Expected to correspond to a solution of an ODE with shape (d,).</p> required Source code in <code>probdiffeq/stats.py</code> <pre><code>def log_marginal_likelihood_terminal_values(\n    u, /, *, standard_deviation, posterior, ssm\n):\n    \"\"\"Compute the log-marginal-likelihood at the terminal value.\n\n    Parameters\n    ----------\n    u\n        Observation. Expected to have shape (d,) for an ODE with shape (d,).\n    standard_deviation\n        Standard deviation of the observation. Expected to be a scalar.\n    posterior\n        Posterior distribution.\n        Expected to correspond to a solution of an ODE with shape (d,).\n    \"\"\"\n    [u_leaves], u_structure = tree_util.tree_flatten(u)\n    [std_leaves], std_structure = tree_util.tree_flatten(standard_deviation)\n\n    if u_structure != std_structure:\n        msg = (\n            f\"Observation-noise tree structure {std_structure} \"\n            f\"does not match the observation structure {u_structure}. \"\n        )\n        raise ValueError(msg)\n\n    # Generate an observation-model for the QOI\n    model = ssm.conditional.to_derivative(0, u, standard_deviation)\n    rv = posterior.init if isinstance(posterior, MarkovSeq) else posterior\n\n    data = np.zeros_like(u_leaves)  # 'u' is baked into the observation model\n    _corrected, logpdf = _condition_and_logpdf(rv, data, model, ssm=ssm)\n    return logpdf\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.markov_marginals","title":"<code>markov_marginals(markov_seq: MarkovSeq, *, reverse, ssm)</code>","text":"<p>Extract the (time-)marginals from a Markov sequence.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def markov_marginals(markov_seq: MarkovSeq, *, reverse, ssm):\n    \"\"\"Extract the (time-)marginals from a Markov sequence.\"\"\"\n    _assert_filtering_solution_removed(markov_seq)\n\n    def step(x, cond):\n        extrapolated = ssm.conditional.marginalise(x, cond)\n        return extrapolated, extrapolated\n\n    init, xs = markov_seq.init, markov_seq.conditional\n    _, marg = control_flow.scan(step, init=init, xs=xs, reverse=reverse)\n    return marg\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.markov_sample","title":"<code>markov_sample(key, markov_seq: MarkovSeq, *, reverse, ssm, shape=())</code>","text":"<p>Sample from a Markov sequence.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def markov_sample(key, markov_seq: MarkovSeq, *, reverse, ssm, shape=()):\n    \"\"\"Sample from a Markov sequence.\"\"\"\n    _assert_filtering_solution_removed(markov_seq)\n    # A smoother samples on the grid by sampling i.i.d values\n    # from the terminal RV x_N and the backward noises z_(1:N)\n    # and then combining them backwards as\n    # x_(n-1) = l_n @ x_n + z_n, for n=1,...,N.\n    markov_seq_shape = _sample_shape(markov_seq, ssm=ssm)\n    base_samples = random.normal(key, shape=shape + markov_seq_shape)\n    return _transform_unit_sample(markov_seq, base_samples, reverse=reverse, ssm=ssm)\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.markov_select_terminal","title":"<code>markov_select_terminal(markov_seq: MarkovSeq) -&gt; MarkovSeq</code>","text":"<p>Discard all intermediate filtering solutions from a Markov sequence.</p> <p>This function is useful to convert a smoothing-solution into a Markov sequence that is compatible with sampling or marginalisation.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def markov_select_terminal(markov_seq: MarkovSeq) -&gt; MarkovSeq:\n    \"\"\"Discard all intermediate filtering solutions from a Markov sequence.\n\n    This function is useful to convert a smoothing-solution into a Markov sequence\n    that is compatible with sampling or marginalisation.\n    \"\"\"\n    init = tree_util.tree_map(lambda x: x[-1, ...], markov_seq.init)\n    return MarkovSeq(init, markov_seq.conditional)\n</code></pre>"},{"location":"api_docs/stats/#probdiffeq.stats.offgrid_marginals_searchsorted","title":"<code>offgrid_marginals_searchsorted(*, ts, solution, solver)</code>","text":"<p>Compute off-grid marginals on a dense grid via jax.numpy.searchsorted.</p> <p>Warning</p> <p>The elements in ts and the elements in the solution grid must be disjoint. Otherwise, anything can happen and the solution will be incorrect. At the moment, we do not check this.</p> <p>Warning</p> <p>The elements in ts must be strictly in (t0, t1). They must not lie outside the interval, and they must not coincide with the interval boundaries. At the moment, we do not check this.</p> Source code in <code>probdiffeq/stats.py</code> <pre><code>def offgrid_marginals_searchsorted(*, ts, solution, solver):\n    \"\"\"Compute off-grid marginals on a dense grid via jax.numpy.searchsorted.\n\n    !!! warning\n        The elements in ts and the elements in the solution grid must be disjoint.\n        Otherwise, anything can happen and the solution will be incorrect.\n        At the moment, we do not check this.\n\n    !!! warning\n        The elements in ts must be strictly in (t0, t1).\n        They must not lie outside the interval, and they must not coincide\n        with the interval boundaries.\n        At the moment, we do not check this.\n    \"\"\"\n    offgrid_marginals_vmap = functools.vmap(_offgrid_marginals, in_axes=(0, None, None))\n    return offgrid_marginals_vmap(ts, solution, solver)\n</code></pre>"},{"location":"api_docs/taylor/","title":"taylor","text":"<p>Taylor-expand the solution of an initial value problem (IVP).</p>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_affine","title":"<code>odejet_affine(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Evaluate the Taylor series of an affine differential equation.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop of length <code>num</code>.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_affine(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Evaluate the Taylor series of an affine differential equation.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop of length `num`.\n\n    \"\"\"\n    if num == 0:\n        return inits\n\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_affine(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    fx, jvp_fn = functools.linearize(vf, *inits)\n\n    tmp = fx\n    fx_evaluations = [tmp := jvp_fn(tmp) for _ in range(num - 1)]\n    return [*inits, fx, *fx_evaluations]\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_doubling_unroll","title":"<code>odejet_doubling_unroll(vf: Callable, inits: Sequence[Array], /, num_doublings: int)</code>","text":"<p>Combine Taylor-mode differentiation and Newton's doubling.</p> <p>Warning: highly EXPERIMENTAL feature!</p> <p>Support for Newton's doubling is highly experimental. There is no guarantee that it works correctly. It might be deleted tomorrow and without any deprecation policy.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_doubling_unroll(vf: Callable, inits: Sequence[Array], /, num_doublings: int):\n    \"\"\"Combine Taylor-mode differentiation and Newton's doubling.\n\n    !!! warning \"Warning: highly EXPERIMENTAL feature!\"\n        Support for Newton's doubling is highly experimental.\n        There is no guarantee that it works correctly.\n        It might be deleted tomorrow\n        and without any deprecation policy.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop.\n\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_doubling_unroll(\n            vf_wrapped, inits_flat, num_doublings=num_doublings\n        )\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    (u0,) = inits\n    zeros = np.zeros_like(u0)\n\n    def jet_embedded(*c, degree):\n        \"\"\"Call a modified jet().\n\n        The modifications include:\n        * We merge \"primals\" and \"series\" into a single set of coefficients\n        * We expect and return _normalised_ Taylor coefficients.\n\n        The reason for the latter is that the doubling-recursion\n        simplifies drastically for normalised coefficients\n        (compared to unnormalised coefficients).\n        \"\"\"\n        coeffs_emb = [*c] + [zeros] * degree\n        p, *s = _unnormalise(*coeffs_emb)\n        p_new, s_new = functools.jet(vf, (p,), (s,))\n        return _normalise(p_new, *s_new)\n\n    taylor_coefficients = [u0]\n    degrees = list(itertools.accumulate(map(lambda s: 2**s, range(num_doublings))))\n    for deg in degrees:\n        jet_embedded_deg = tree_util.Partial(jet_embedded, degree=deg)\n        fx, jvp = functools.linearize(jet_embedded_deg, *taylor_coefficients)\n\n        # Compute the next set of coefficients.\n        # TODO: can we fori_loop() this loop?\n        #  the running variable (cs_padded) should have constant size\n        cs = [(fx[deg - 1] / deg)]\n        cs_padded = cs + [zeros] * (deg - 1)\n        for i, fx_i in enumerate(fx[deg : 2 * deg]):\n            # The Jacobian of the embedded jet is block-banded,\n            # i.e., of the form (for j=3)\n            # (A0, 0, 0; A1, A0, 0; A2, A1, A0; *, *, *; *, *, *; *, *, *)\n            # Thus, by attaching zeros to the current set of coefficients\n            # until the input and output shapes match, we compute\n            # the convolution-like sum of matrix-vector products with\n            # a single call to the JVP function.\n            # Bettencourt et al. (2019;\n            # \"Taylor-mode autodiff for higher-order derivatives in JAX\")\n            # explain details.\n            # i = k - deg\n            linear_combination = jvp(*cs_padded)[i]\n            cs_ = cs_padded[: (i + 1)]\n            cs_ += [(fx_i + linear_combination) / (i + deg + 1)]\n            cs_padded = cs_ + [zeros] * (deg - i - 2)\n\n        # Store all new coefficients\n        taylor_coefficients.extend(cs_padded)\n\n    return _unnormalise(*taylor_coefficients)\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_padded_scan","title":"<code>odejet_padded_scan(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Taylor-expand the solution of an IVP with Taylor-mode differentiation.</p> <p>Other than <code>odejet_unroll()</code>, this function implements the loop via a scan, which comes at the price of padding the loop variable with zeros as appropriate. It is expected to compile more quickly than <code>odejet_unroll()</code>, but may execute more slowly.</p> <p>The differences should be small. Consult the benchmarks if performance is critical.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_padded_scan(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Taylor-expand the solution of an IVP with Taylor-mode differentiation.\n\n    Other than `odejet_unroll()`, this function implements the loop via a scan,\n    which comes at the price of padding the loop variable with zeros as appropriate.\n    It is expected to compile more quickly than `odejet_unroll()`, but may\n    execute more slowly.\n\n    The differences should be small.\n    Consult the benchmarks if performance is critical.\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_padded_scan(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    # Number of positional arguments in f\n    num_arguments = len(inits)\n\n    # Initial Taylor series (u_0, u_1, ..., u_k)\n    primals = vf(*inits)\n    taylor_coeffs = [*inits, primals]\n\n    def body(tcoeffs, _):\n        # Pad the Taylor coefficients in zeros, call jet, and return the solution.\n        # This works, because the $i$th output coefficient of jet()\n        # is independent of the $i+j$th input coefficient\n        # (see also the explanation in odejet_doubling_unroll)\n        series = _subsets(tcoeffs[1:], num_arguments)  # for high-order ODEs\n        p, s_new = functools.jet(vf, primals=inits, series=series)\n\n        # The final values in s_new are nonsensical\n        # (well, they are not; but we don't care about them)\n        # so we remove them\n        tcoeffs = [*inits, p, *s_new[:-1]]\n        return tcoeffs, None\n\n    # Pad the initial Taylor series with zeros\n    num_outputs = num_arguments + num\n    zeros = np.zeros_like(primals)\n    taylor_coeffs = _pad_to_length(taylor_coeffs, length=num_outputs, value=zeros)\n\n    # Early exit for num=1.\n    #  Why? because zero-length scan and disable_jit() don't work together.\n    if num == 1:\n        return taylor_coeffs\n\n    # Compute all coefficients with scan().\n    taylor_coeffs, _ = control_flow.scan(\n        body, init=taylor_coeffs, xs=None, length=num - 1\n    )\n    return taylor_coeffs\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_unroll","title":"<code>odejet_unroll(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Taylor-expand the solution of an IVP with Taylor-mode differentiation.</p> <p>Other than <code>odejet_padded_scan()</code>, this function does not depend on zero-padding the coefficients at the price of unrolling a loop of length <code>num-1</code>. It is expected to compile more slowly than <code>odejet_padded_scan()</code>, but execute more quickly.</p> <p>The differences should be small. Consult the benchmarks if performance is critical.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_unroll(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Taylor-expand the solution of an IVP with Taylor-mode differentiation.\n\n    Other than `odejet_padded_scan()`, this function does not depend on zero-padding\n    the coefficients at the price of unrolling a loop of length `num-1`.\n    It is expected to compile more slowly than `odejet_padded_scan()`,\n    but execute more quickly.\n\n    The differences should be small.\n    Consult the benchmarks if performance is critical.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop.\n\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_unroll(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    # Number of positional arguments in f\n    num_arguments = len(inits)\n\n    # Initial Taylor series (u_0, u_1, ..., u_k)\n    primals = vf(*inits)\n    taylor_coeffs = [*inits, primals]\n\n    for _ in range(num - 1):\n        series = _subsets(taylor_coeffs[1:], num_arguments)  # for high-order ODEs\n        p, s_new = functools.jet(vf, primals=inits, series=series)\n        taylor_coeffs = [*inits, p, *s_new]\n    return taylor_coeffs\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.odejet_via_jvp","title":"<code>odejet_via_jvp(vf: Callable, inits: Sequence[Array], /, num: int)</code>","text":"<p>Taylor-expand the solution of an IVP with recursive forward-mode differentiation.</p> <p>Compilation time</p> <p>JIT-compiling this function unrolls a loop.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def odejet_via_jvp(vf: Callable, inits: Sequence[Array], /, num: int):\n    \"\"\"Taylor-expand the solution of an IVP with recursive forward-mode differentiation.\n\n    !!! warning \"Compilation time\"\n        JIT-compiling this function unrolls a loop.\n\n    \"\"\"\n    if not isinstance(inits[0], ArrayLike):\n        _, unravel = tree_util.ravel_pytree(inits[0])\n        inits_flat = [tree_util.ravel_pytree(m)[0] for m in inits]\n\n        def vf_wrapped(*ys, **kwargs):\n            ys = tree_util.tree_map(unravel, ys)\n            return tree_util.ravel_pytree(vf(*ys, **kwargs))[0]\n\n        tcoeffs = odejet_via_jvp(vf_wrapped, inits_flat, num=num)\n        return tree_util.tree_map(unravel, tcoeffs)\n\n    g_n, g_0 = vf, vf\n    taylor_coeffs = [*inits, vf(*inits)]\n    for _ in range(num - 1):\n        g_n = _fwd_recursion_iterate(fun_n=g_n, fun_0=g_0)\n        taylor_coeffs = [*taylor_coeffs, g_n(*inits)]\n    return taylor_coeffs\n</code></pre>"},{"location":"api_docs/taylor/#probdiffeq.taylor.runge_kutta_starter","title":"<code>runge_kutta_starter(dt, *, num: int, prior, ssm, atol=1e-12, rtol=1e-10)</code>","text":"<p>Create an estimator that uses a Runge-Kutta starter.</p> Source code in <code>probdiffeq/taylor.py</code> <pre><code>def runge_kutta_starter(dt, *, num: int, prior, ssm, atol=1e-12, rtol=1e-10):\n    \"\"\"Create an estimator that uses a Runge-Kutta starter.\"\"\"\n\n    def starter(vf, initial_values: tuple, /, t):\n        # TODO: higher-order ODEs\n        # TODO: allow flexible \"solve\" method?\n\n        # Assertions and early exits\n\n        if len(initial_values) &gt; 1:\n            msg = \"Higher-order ODEs are not supported at the moment.\"\n            raise ValueError(msg)\n\n        if num == 0:\n            return [*initial_values]\n\n        if num == 1:\n            return [*initial_values, vf(*initial_values, t=t)]\n\n        # Generate data\n\n        k = num + 1  # important: k &gt; num\n        ts = np.linspace(t, t + dt * (k - 1), num=k, endpoint=True)\n        ys = ode.odeint_and_save_at(\n            vf, initial_values, save_at=ts, atol=atol, rtol=rtol\n        )\n\n        # Initial condition\n        scale = ssm.prototypes.output_scale()\n        rv_t0 = ssm.normal.standard(num + 1, scale)\n        estimator = filter_util.fixedpointsmoother_precon(ssm=ssm)\n        conditional_t0 = ssm.conditional.identity(num + 1)\n        init = (rv_t0, conditional_t0)\n\n        # Discretised prior\n        scale = ssm.prototypes.output_scale()\n        prior_vmap = functools.vmap(prior, in_axes=(0, None))\n        ibm_transitions = prior_vmap(np.diff(ts), scale)\n\n        # Generate an observation-model for the QOI\n        # (1e-7 observation noise for nuggets and for reusing existing code)\n        model_fun = functools.vmap(ssm.conditional.to_derivative, in_axes=(None, 0, 0))\n        std = tree_util.tree_map(lambda s: 1e-7 * np.ones((len(s),)), ys)\n        models = model_fun(0, ys, std)\n\n        zeros = np.zeros_like(models.noise.mean)\n\n        # Run the preconditioned fixedpoint smoother\n        (corrected, conditional), _ = filter_util.estimate_fwd(\n            zeros,\n            init=init,\n            prior_transitions=ibm_transitions,\n            observation_model=models,\n            estimator=estimator,\n        )\n        initial = ssm.conditional.marginalise(corrected, conditional)\n        mean = ssm.stats.mean(initial)\n        return ssm.unravel(mean)\n\n    return starter\n</code></pre>"},{"location":"benchmarks/hires/plot/","title":"Hires","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark all solvers on the HIRES problem.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n</pre> \"\"\"Benchmark all solvers on the HIRES problem.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt import jax   jax.config.update(\"jax_platform_name\", \"cpu\") In\u00a0[2]: Copied! <pre>def load_results():\n    \"\"\"Load the results from a file.\"\"\"\n    return jnp.load(\"./results.npy\", allow_pickle=True)[()]\n\n\ndef load_solution():\n    \"\"\"Load the solution-to-be-plotted from a file.\"\"\"\n    ts = jnp.load(\"./plot_ts.npy\")\n    ys = jnp.load(\"./plot_ys.npy\")\n    return ts, ys\n\n\ndef choose_style(label):\n    \"\"\"Choose a plotting style for a given algorithm.\"\"\"\n    if \"TS\" in label:\n        return {\"color\": \"C0\", \"linestyle\": \"solid\"}\n    if \"SciPy\" in label:\n        return {\"color\": \"C2\", \"linestyle\": \"dashed\"}\n    if \"iffrax\" in label:\n        return {\"color\": \"C3\", \"linestyle\": \"dotted\"}\n    msg = f\"Label {label} unknown.\"\n    raise ValueError(msg)\n\n\ndef plot_results(axis, results):\n    \"\"\"Plot the results.\"\"\"\n    axis.set_title(\"Benchmark\")\n    for label, wp in results.items():\n        style = choose_style(label)\n\n        precision = wp[\"precision\"]\n        work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])\n        axis.loglog(precision, work_mean, label=label, **style)\n\n        range_lower, range_upper = work_mean - work_std, work_mean + work_std\n        axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)\n\n    axis.set_xlabel(\"Precision [relative RMSE]\")\n    axis.set_ylabel(\"Work [wall time, s]\")\n    axis.grid()\n\n    axis.legend(loc=\"upper center\", ncols=3, mode=\"expand\", facecolor=\"ghostwhite\")\n    return axis\n\n\ndef plot_solution(axis, ts, ys, yscale=\"linear\"):\n    \"\"\"Plot the IVP solution.\"\"\"\n    axis.set_title(\"Hires\")\n    kwargs = {\"color\": \"black\", \"alpha\": 0.85}\n\n    axis.plot(ts, ys, linestyle=\"dashed\", marker=\"None\", **kwargs)\n    for y in ys.T:\n        axis.plot(ts[0], y[0], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)\n        axis.plot(ts[-1], y[-1], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)\n\n    axis.set_xlabel(\"Time $t$\")\n    axis.set_ylabel(\"Solution $y$\")\n    axis.set_yscale(yscale)\n    return axis\n</pre> def load_results():     \"\"\"Load the results from a file.\"\"\"     return jnp.load(\"./results.npy\", allow_pickle=True)[()]   def load_solution():     \"\"\"Load the solution-to-be-plotted from a file.\"\"\"     ts = jnp.load(\"./plot_ts.npy\")     ys = jnp.load(\"./plot_ys.npy\")     return ts, ys   def choose_style(label):     \"\"\"Choose a plotting style for a given algorithm.\"\"\"     if \"TS\" in label:         return {\"color\": \"C0\", \"linestyle\": \"solid\"}     if \"SciPy\" in label:         return {\"color\": \"C2\", \"linestyle\": \"dashed\"}     if \"iffrax\" in label:         return {\"color\": \"C3\", \"linestyle\": \"dotted\"}     msg = f\"Label {label} unknown.\"     raise ValueError(msg)   def plot_results(axis, results):     \"\"\"Plot the results.\"\"\"     axis.set_title(\"Benchmark\")     for label, wp in results.items():         style = choose_style(label)          precision = wp[\"precision\"]         work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])         axis.loglog(precision, work_mean, label=label, **style)          range_lower, range_upper = work_mean - work_std, work_mean + work_std         axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)      axis.set_xlabel(\"Precision [relative RMSE]\")     axis.set_ylabel(\"Work [wall time, s]\")     axis.grid()      axis.legend(loc=\"upper center\", ncols=3, mode=\"expand\", facecolor=\"ghostwhite\")     return axis   def plot_solution(axis, ts, ys, yscale=\"linear\"):     \"\"\"Plot the IVP solution.\"\"\"     axis.set_title(\"Hires\")     kwargs = {\"color\": \"black\", \"alpha\": 0.85}      axis.plot(ts, ys, linestyle=\"dashed\", marker=\"None\", **kwargs)     for y in ys.T:         axis.plot(ts[0], y[0], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)         axis.plot(ts[-1], y[-1], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)      axis.set_xlabel(\"Time $t$\")     axis.set_ylabel(\"Solution $y$\")     axis.set_yscale(yscale)     return axis In\u00a0[3]: Copied! <pre>layout = [\n    [\"benchmark\", \"benchmark\", \"solution\"],\n    [\"benchmark\", \"benchmark\", \"solution\"],\n]\nfig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)\n\n\nresults = load_results()\nts, ys = load_solution()\n\n\n_ = plot_results(axes[\"benchmark\"], results)\n_ = plot_solution(axes[\"solution\"], ts, ys)\nplt.show()\n</pre> layout = [     [\"benchmark\", \"benchmark\", \"solution\"],     [\"benchmark\", \"benchmark\", \"solution\"], ] fig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)   results = load_results() ts, ys = load_solution()   _ = plot_results(axes[\"benchmark\"], results) _ = plot_solution(axes[\"solution\"], ts, ys) plt.show()"},{"location":"benchmarks/hires/plot/#hires","title":"Hires\u00b6","text":"<p>The HIRES problem is a common stiff differential equation.</p>"},{"location":"benchmarks/lotkavolterra/plot/","title":"Lotka-Volterra","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark all solvers on the Lotka-Volterra problem.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n</pre> \"\"\"Benchmark all solvers on the Lotka-Volterra problem.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt import jax  jax.config.update(\"jax_platform_name\", \"cpu\") In\u00a0[2]: Copied! <pre>def load_results():\n    \"\"\"Load the results from a file.\"\"\"\n    return jnp.load(\"./results.npy\", allow_pickle=True)[()]\n\n\ndef load_solution():\n    \"\"\"Load the solution-to-be-plotted from a file.\"\"\"\n    ts = jnp.load(\"./plot_ts.npy\")\n    ys = jnp.load(\"./plot_ys.npy\")\n    return ts, ys\n\n\ndef choose_style(label):\n    \"\"\"Choose a plotting style for a given algorithm.\"\"\"\n    if \"ProbDiffEq\" in label:\n        return {\"color\": \"C0\", \"linestyle\": \"solid\"}\n    if \"SciPy\" in label:\n        return {\"color\": \"C2\", \"linestyle\": \"dashed\"}\n    if \"iffrax\" in label:\n        return {\"color\": \"C3\", \"linestyle\": \"dotted\"}\n    msg = f\"Label {label} unknown.\"\n    raise ValueError(msg)\n\n\ndef plot_results(axis, results):\n    \"\"\"Plot the results.\"\"\"\n    axis.set_title(\"Benchmark\")\n    for label, wp in results.items():\n        style = choose_style(label)\n\n        precision = wp[\"precision\"]\n        work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])\n        axis.loglog(precision, work_mean, label=label, **style)\n\n        range_lower, range_upper = work_mean - work_std, work_mean + work_std\n        axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)\n\n    axis.set_xlabel(\"Precision [relative RMSE]\")\n    axis.set_ylabel(\"Work [wall time, s]\")\n    axis.grid()\n    axis.legend(loc=\"upper center\", ncols=3, mode=\"expand\", facecolor=\"ghostwhite\")\n    return axis\n\n\ndef plot_solution(axis, ts, ys, yscale=\"linear\"):\n    \"\"\"Plot the IVP solution.\"\"\"\n    axis.set_title(\"Lotka-Volterra\")\n    kwargs = {\"color\": \"black\", \"alpha\": 0.85}\n\n    axis.plot(\n        ts, ys[:, 0], linestyle=\"solid\", marker=\"None\", label=\"Predators\", **kwargs\n    )\n    axis.plot(ts, ys[:, 1], linestyle=\"dashed\", marker=\"None\", label=\"Prey\", **kwargs)\n    for y in ys.T:\n        axis.plot(ts[0], y[0], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)\n        axis.plot(ts[-1], y[-1], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)\n\n    axis.legend(facecolor=\"ghostwhite\", ncols=2, loc=\"lower center\", mode=\"expand\")\n\n    axis.set_xlabel(\"Time $t$\")\n    axis.set_ylabel(\"Solution $y$\")\n    axis.set_yscale(yscale)\n    return axis\n</pre> def load_results():     \"\"\"Load the results from a file.\"\"\"     return jnp.load(\"./results.npy\", allow_pickle=True)[()]   def load_solution():     \"\"\"Load the solution-to-be-plotted from a file.\"\"\"     ts = jnp.load(\"./plot_ts.npy\")     ys = jnp.load(\"./plot_ys.npy\")     return ts, ys   def choose_style(label):     \"\"\"Choose a plotting style for a given algorithm.\"\"\"     if \"ProbDiffEq\" in label:         return {\"color\": \"C0\", \"linestyle\": \"solid\"}     if \"SciPy\" in label:         return {\"color\": \"C2\", \"linestyle\": \"dashed\"}     if \"iffrax\" in label:         return {\"color\": \"C3\", \"linestyle\": \"dotted\"}     msg = f\"Label {label} unknown.\"     raise ValueError(msg)   def plot_results(axis, results):     \"\"\"Plot the results.\"\"\"     axis.set_title(\"Benchmark\")     for label, wp in results.items():         style = choose_style(label)          precision = wp[\"precision\"]         work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])         axis.loglog(precision, work_mean, label=label, **style)          range_lower, range_upper = work_mean - work_std, work_mean + work_std         axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)      axis.set_xlabel(\"Precision [relative RMSE]\")     axis.set_ylabel(\"Work [wall time, s]\")     axis.grid()     axis.legend(loc=\"upper center\", ncols=3, mode=\"expand\", facecolor=\"ghostwhite\")     return axis   def plot_solution(axis, ts, ys, yscale=\"linear\"):     \"\"\"Plot the IVP solution.\"\"\"     axis.set_title(\"Lotka-Volterra\")     kwargs = {\"color\": \"black\", \"alpha\": 0.85}      axis.plot(         ts, ys[:, 0], linestyle=\"solid\", marker=\"None\", label=\"Predators\", **kwargs     )     axis.plot(ts, ys[:, 1], linestyle=\"dashed\", marker=\"None\", label=\"Prey\", **kwargs)     for y in ys.T:         axis.plot(ts[0], y[0], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)         axis.plot(ts[-1], y[-1], linestyle=\"None\", marker=\".\", markersize=4, **kwargs)      axis.legend(facecolor=\"ghostwhite\", ncols=2, loc=\"lower center\", mode=\"expand\")      axis.set_xlabel(\"Time $t$\")     axis.set_ylabel(\"Solution $y$\")     axis.set_yscale(yscale)     return axis In\u00a0[3]: Copied! <pre>layout = [\n    [\"benchmark\", \"benchmark\", \"solution\"],\n    [\"benchmark\", \"benchmark\", \"solution\"],\n]\nfig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)\n\n\nresults = load_results()\nts, ys = load_solution()\n\n_ = plot_results(axes[\"benchmark\"], results)\n_ = plot_solution(axes[\"solution\"], ts, ys)\n\nplt.show()\n</pre> layout = [     [\"benchmark\", \"benchmark\", \"solution\"],     [\"benchmark\", \"benchmark\", \"solution\"], ] fig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)   results = load_results() ts, ys = load_solution()  _ = plot_results(axes[\"benchmark\"], results) _ = plot_solution(axes[\"solution\"], ts, ys)  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"benchmarks/lotkavolterra/plot/#lotka-volterra","title":"Lotka-Volterra\u00b6","text":"<p>The Lotka-Volterra problem is a common differential equation and relatively easy to solve.</p>"},{"location":"benchmarks/pleiades/plot/","title":"Pleiades","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark all solvers on the Pleiades problem.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n</pre> \"\"\"Benchmark all solvers on the Pleiades problem.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt import jax  jax.config.update(\"jax_platform_name\", \"cpu\") In\u00a0[2]: Copied! <pre>def load_results():\n    \"\"\"Load the results from a file.\"\"\"\n    return jnp.load(\"./results.npy\", allow_pickle=True)[()]\n\n\ndef load_solution():\n    \"\"\"Load the solution-to-be-plotted from a file.\"\"\"\n    ts = jnp.load(\"./plot_ts.npy\")\n    ys = jnp.load(\"./plot_ys.npy\")\n    return ts, ys\n\n\ndef choose_style(label):\n    \"\"\"Choose a plotting style for a given algorithm.\"\"\"\n    if \"probdiffeq\" in label.lower():\n        return {\"color\": \"C0\", \"linestyle\": \"solid\"}\n    if \"numba\" in label.lower():\n        return {\"color\": \"C4\", \"linestyle\": \"dashed\"}\n    if \"scipy\" in label.lower():\n        return {\"color\": \"C2\", \"linestyle\": \"dashed\"}\n    if \"diffrax\" in label.lower():\n        return {\"color\": \"C3\", \"linestyle\": \"dotted\"}\n    msg = f\"Label {label} unknown.\"\n    raise ValueError(msg)\n\n\ndef plot_results(axis, results):\n    \"\"\"Plot the results.\"\"\"\n    axis.set_title(\"Benchmark\") \n    for label, wp in results.items():\n        style = choose_style(label)\n\n        precision = wp[\"precision\"]\n        work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])\n        axis.loglog(precision, work_mean, label=label, **style)\n\n        range_lower, range_upper = work_mean - work_std, work_mean + work_std\n        axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)\n\n    axis.set_xlabel(\"Precision [absolute RMSE]\")\n    axis.set_ylabel(\"Work [wall time, s]\")\n    axis.grid()\n    axis.legend(\n        loc=\"upper center\",\n        ncols=4,\n        fontsize=\"x-small\",\n        mode=\"expand\",\n        facecolor=\"ghostwhite\",\n    )\n    return axis\n\n\ndef plot_solution(axis, ys, yscale=\"linear\"):\n    \"\"\"Plot the IVP solution.\"\"\"\n    axis.set_title(\"Pleiades\")\n    kwargs = {\"color\": \"goldenrod\", \"alpha\": 0.85}\n\n    axis.plot(ys[:, :7], ys[:, 7:14], linestyle=\"solid\", marker=\"None\", **kwargs)\n    axis.plot(\n        ys[0, :7], ys[0, 7:14], linestyle=\"None\", marker=\".\", markersize=4, **kwargs\n    )\n    axis.plot(\n        ys[-1, :7], ys[-1, 7:14], linestyle=\"None\", marker=\"*\", markersize=8, **kwargs\n    )\n\n    axis.set_xlabel(\"Time $t$\")\n    axis.set_ylabel(\"Solution $y$\")\n    axis.set_yscale(yscale)\n    return axis\n</pre> def load_results():     \"\"\"Load the results from a file.\"\"\"     return jnp.load(\"./results.npy\", allow_pickle=True)[()]   def load_solution():     \"\"\"Load the solution-to-be-plotted from a file.\"\"\"     ts = jnp.load(\"./plot_ts.npy\")     ys = jnp.load(\"./plot_ys.npy\")     return ts, ys   def choose_style(label):     \"\"\"Choose a plotting style for a given algorithm.\"\"\"     if \"probdiffeq\" in label.lower():         return {\"color\": \"C0\", \"linestyle\": \"solid\"}     if \"numba\" in label.lower():         return {\"color\": \"C4\", \"linestyle\": \"dashed\"}     if \"scipy\" in label.lower():         return {\"color\": \"C2\", \"linestyle\": \"dashed\"}     if \"diffrax\" in label.lower():         return {\"color\": \"C3\", \"linestyle\": \"dotted\"}     msg = f\"Label {label} unknown.\"     raise ValueError(msg)   def plot_results(axis, results):     \"\"\"Plot the results.\"\"\"     axis.set_title(\"Benchmark\")      for label, wp in results.items():         style = choose_style(label)          precision = wp[\"precision\"]         work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])         axis.loglog(precision, work_mean, label=label, **style)          range_lower, range_upper = work_mean - work_std, work_mean + work_std         axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)      axis.set_xlabel(\"Precision [absolute RMSE]\")     axis.set_ylabel(\"Work [wall time, s]\")     axis.grid()     axis.legend(         loc=\"upper center\",         ncols=4,         fontsize=\"x-small\",         mode=\"expand\",         facecolor=\"ghostwhite\",     )     return axis   def plot_solution(axis, ys, yscale=\"linear\"):     \"\"\"Plot the IVP solution.\"\"\"     axis.set_title(\"Pleiades\")     kwargs = {\"color\": \"goldenrod\", \"alpha\": 0.85}      axis.plot(ys[:, :7], ys[:, 7:14], linestyle=\"solid\", marker=\"None\", **kwargs)     axis.plot(         ys[0, :7], ys[0, 7:14], linestyle=\"None\", marker=\".\", markersize=4, **kwargs     )     axis.plot(         ys[-1, :7], ys[-1, 7:14], linestyle=\"None\", marker=\"*\", markersize=8, **kwargs     )      axis.set_xlabel(\"Time $t$\")     axis.set_ylabel(\"Solution $y$\")     axis.set_yscale(yscale)     return axis In\u00a0[3]: Copied! <pre>layout = [\n    [\"solution\", \"benchmark\", \"benchmark\"],\n    [\"solution\", \"benchmark\", \"benchmark\"],\n]\nfig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)\n\n\nresults = load_results()\n_ts, ys = load_solution()\n\n_ = plot_results(axes[\"benchmark\"], results)\n_ = plot_solution(axes[\"solution\"], ys)\n\nplt.show()\n</pre> layout = [     [\"solution\", \"benchmark\", \"benchmark\"],     [\"solution\", \"benchmark\", \"benchmark\"], ] fig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)   results = load_results() _ts, ys = load_solution()  _ = plot_results(axes[\"benchmark\"], results) _ = plot_solution(axes[\"solution\"], ys)  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"benchmarks/pleiades/plot/#pleiades","title":"Pleiades\u00b6","text":"<p>The Pleiades problem is a common non-stiff differential equation.</p>"},{"location":"benchmarks/taylor_fitzhughnagumo/plot/","title":"FitzHugh-Nagumo","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark all Taylor-series estimators on the Fitzhugh-Nagumo problem.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n</pre> \"\"\"Benchmark all Taylor-series estimators on the Fitzhugh-Nagumo problem.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt import jax  jax.config.update(\"jax_platform_name\", \"cpu\") In\u00a0[2]: Copied! <pre>def load_results():\n    \"\"\"Load the results from a file.\"\"\"\n    return jnp.load(\"./results.npy\", allow_pickle=True)[()]\n\n\ndef choose_style(label):\n    \"\"\"Choose a plotting style for a given algorithm.\"\"\"\n    if \"doubling\" in label.lower():\n        return {\"color\": \"C3\", \"linestyle\": \"dotted\", \"label\": label}\n    if \"unroll\" in label.lower():\n        return {\"color\": \"C2\", \"linestyle\": \"dashdot\", \"label\": label}\n    if \"taylor\" in label.lower():\n        return {\"color\": \"C0\", \"linestyle\": \"solid\", \"label\": label}\n    if \"forward\" in label.lower():\n        return {\"color\": \"C1\", \"linestyle\": \"dashed\", \"label\": label}\n    msg = f\"Label {label} unknown.\"\n    raise ValueError(msg)\n\n\ndef plot_results(axis_compile, axis_perform, results):\n    \"\"\"Plot the results.\"\"\"\n    style_curve = {\"alpha\": 0.85}\n    style_area = {\"alpha\": 0.15}\n    for label, wp in results.items():\n        style = choose_style(label)\n\n        inputs = wp[\"arguments\"]\n        work_compile = wp[\"work_compile\"]\n        work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]\n\n        if \"doubling\" in label:\n            num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))\n            inputs = jnp.arange(1, jnp.amax(inputs) * 1)\n            work_compile = _adaptive_repeat(work_compile, num_repeats)\n            work_mean = _adaptive_repeat(work_mean, num_repeats)\n            work_std = _adaptive_repeat(work_std, num_repeats)\n\n        axis_compile.semilogy(inputs, work_compile, **style, **style_curve)\n\n        range_lower, range_upper = work_mean - work_std, work_mean + work_std\n        axis_perform.semilogy(inputs, work_mean, **style, **style_curve)\n        axis_perform.fill_between(\n            inputs, range_lower, range_upper, **style, **style_area\n        )\n\n    axis_compile.set_ylim((5e-3, 8e1))\n    axis_perform.set_yticks((1e-5, 1e-4))\n    return axis_compile, axis_perform\n\n\ndef _adaptive_repeat(xs, ys):\n    \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"\n    zs = []\n    for x, y in zip(xs, ys):\n        zs.extend([x] * int(y))\n    return jnp.asarray(zs)\n</pre> def load_results():     \"\"\"Load the results from a file.\"\"\"     return jnp.load(\"./results.npy\", allow_pickle=True)[()]   def choose_style(label):     \"\"\"Choose a plotting style for a given algorithm.\"\"\"     if \"doubling\" in label.lower():         return {\"color\": \"C3\", \"linestyle\": \"dotted\", \"label\": label}     if \"unroll\" in label.lower():         return {\"color\": \"C2\", \"linestyle\": \"dashdot\", \"label\": label}     if \"taylor\" in label.lower():         return {\"color\": \"C0\", \"linestyle\": \"solid\", \"label\": label}     if \"forward\" in label.lower():         return {\"color\": \"C1\", \"linestyle\": \"dashed\", \"label\": label}     msg = f\"Label {label} unknown.\"     raise ValueError(msg)   def plot_results(axis_compile, axis_perform, results):     \"\"\"Plot the results.\"\"\"     style_curve = {\"alpha\": 0.85}     style_area = {\"alpha\": 0.15}     for label, wp in results.items():         style = choose_style(label)          inputs = wp[\"arguments\"]         work_compile = wp[\"work_compile\"]         work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]          if \"doubling\" in label:             num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))             inputs = jnp.arange(1, jnp.amax(inputs) * 1)             work_compile = _adaptive_repeat(work_compile, num_repeats)             work_mean = _adaptive_repeat(work_mean, num_repeats)             work_std = _adaptive_repeat(work_std, num_repeats)          axis_compile.semilogy(inputs, work_compile, **style, **style_curve)          range_lower, range_upper = work_mean - work_std, work_mean + work_std         axis_perform.semilogy(inputs, work_mean, **style, **style_curve)         axis_perform.fill_between(             inputs, range_lower, range_upper, **style, **style_area         )      axis_compile.set_ylim((5e-3, 8e1))     axis_perform.set_yticks((1e-5, 1e-4))     return axis_compile, axis_perform   def _adaptive_repeat(xs, ys):     \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"     zs = []     for x, y in zip(xs, ys):         zs.extend([x] * int(y))     return jnp.asarray(zs) In\u00a0[3]: Copied! <pre>fig, (axis_perform, axis_compile) = plt.subplots(\n    ncols=2, figsize=(8, 3), dpi=150, sharex=True\n)\n\nresults = load_results()\n\naxis_compile, axis_perform = plot_results(axis_compile, axis_perform, results)\n\naxis_compile.set_title(\"Compilation time\")\naxis_perform.set_title(\"Evaluation time\")\naxis_compile.legend(loc=\"lower right\")\naxis_compile.set_xlabel(\"Number of Derivatives\")\naxis_perform.set_xlabel(\"Number of Derivatives\")\naxis_perform.set_ylabel(\"Wall time (sec)\")\naxis_perform.grid()\naxis_compile.grid()\n\nplt.show()\n</pre> fig, (axis_perform, axis_compile) = plt.subplots(     ncols=2, figsize=(8, 3), dpi=150, sharex=True )  results = load_results()  axis_compile, axis_perform = plot_results(axis_compile, axis_perform, results)  axis_compile.set_title(\"Compilation time\") axis_perform.set_title(\"Evaluation time\") axis_compile.legend(loc=\"lower right\") axis_compile.set_xlabel(\"Number of Derivatives\") axis_perform.set_xlabel(\"Number of Derivatives\") axis_perform.set_ylabel(\"Wall time (sec)\") axis_perform.grid() axis_compile.grid()  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"benchmarks/taylor_fitzhughnagumo/plot/#fitzhugh-nagumo","title":"FitzHugh-Nagumo\u00b6","text":"<p>The FHN problem is a common non-stiff differential equation.</p>"},{"location":"benchmarks/taylor_node/plot/","title":"Neural ODE","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark all Taylor-series estimators on a Neural ODE.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n</pre> \"\"\"Benchmark all Taylor-series estimators on a Neural ODE.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt import jax  jax.config.update(\"jax_platform_name\", \"cpu\") In\u00a0[2]: Copied! <pre>def load_results():\n    \"\"\"Load the results from a file.\"\"\"\n    return jnp.load(\"./results.npy\", allow_pickle=True)[()]\n\n\ndef choose_style(label):\n    \"\"\"Choose a plotting style for a given algorithm.\"\"\"\n    if \"doubling\" in label.lower():\n        return {\"color\": \"C3\", \"linestyle\": \"dotted\", \"label\": label}\n    if \"unroll\" in label.lower():\n        return {\"color\": \"C2\", \"linestyle\": \"dashdot\", \"label\": label}\n    if \"taylor\" in label.lower():\n        return {\"color\": \"C0\", \"linestyle\": \"solid\"}\n    if \"forward\" in label.lower():\n        return {\"color\": \"C1\", \"linestyle\": \"dashed\", \"label\": label}\n    msg = f\"Label {label} unknown.\"\n    raise ValueError(msg)\n\n\ndef plot_results(axis_compile, axis_perform, results):\n    \"\"\"Plot the results.\"\"\"\n    style_curve = {\"alpha\": 0.85}\n    style_area = {\"alpha\": 0.15}\n    for label, wp in results.items():\n        style = choose_style(label)\n\n        inputs = wp[\"arguments\"]\n        work_compile = wp[\"work_compile\"]\n        work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]\n\n        if \"doubling\" in label:\n            num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))\n            inputs = jnp.arange(1, jnp.amax(inputs) * 1)\n            work_compile = _adaptive_repeat(work_compile, num_repeats)\n            work_mean = _adaptive_repeat(work_mean, num_repeats)\n            work_std = _adaptive_repeat(work_std, num_repeats)\n\n        axis_compile.semilogy(inputs, work_compile, **style, **style_curve)\n\n        range_lower, range_upper = work_mean - work_std, work_mean + work_std\n        axis_perform.semilogy(inputs, work_mean, **style, **style_curve)\n        axis_perform.fill_between(\n            inputs, range_lower, range_upper, **style, **style_area\n        )\n\n    axis_compile.set_ylim((1e-3, 1e2))\n    return axis_compile, axis_perform\n\n\ndef _adaptive_repeat(xs, ys):\n    \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"\n    zs = []\n    for x, y in zip(xs, ys):\n        zs.extend([x] * int(y))\n    return jnp.asarray(zs)\n</pre> def load_results():     \"\"\"Load the results from a file.\"\"\"     return jnp.load(\"./results.npy\", allow_pickle=True)[()]   def choose_style(label):     \"\"\"Choose a plotting style for a given algorithm.\"\"\"     if \"doubling\" in label.lower():         return {\"color\": \"C3\", \"linestyle\": \"dotted\", \"label\": label}     if \"unroll\" in label.lower():         return {\"color\": \"C2\", \"linestyle\": \"dashdot\", \"label\": label}     if \"taylor\" in label.lower():         return {\"color\": \"C0\", \"linestyle\": \"solid\"}     if \"forward\" in label.lower():         return {\"color\": \"C1\", \"linestyle\": \"dashed\", \"label\": label}     msg = f\"Label {label} unknown.\"     raise ValueError(msg)   def plot_results(axis_compile, axis_perform, results):     \"\"\"Plot the results.\"\"\"     style_curve = {\"alpha\": 0.85}     style_area = {\"alpha\": 0.15}     for label, wp in results.items():         style = choose_style(label)          inputs = wp[\"arguments\"]         work_compile = wp[\"work_compile\"]         work_mean, work_std = wp[\"work_mean\"], wp[\"work_std\"]          if \"doubling\" in label:             num_repeats = jnp.diff(jnp.concatenate((jnp.ones((1,)), inputs)))             inputs = jnp.arange(1, jnp.amax(inputs) * 1)             work_compile = _adaptive_repeat(work_compile, num_repeats)             work_mean = _adaptive_repeat(work_mean, num_repeats)             work_std = _adaptive_repeat(work_std, num_repeats)          axis_compile.semilogy(inputs, work_compile, **style, **style_curve)          range_lower, range_upper = work_mean - work_std, work_mean + work_std         axis_perform.semilogy(inputs, work_mean, **style, **style_curve)         axis_perform.fill_between(             inputs, range_lower, range_upper, **style, **style_area         )      axis_compile.set_ylim((1e-3, 1e2))     return axis_compile, axis_perform   def _adaptive_repeat(xs, ys):     \"\"\"Repeat the doubling values correctly to create a comprehensible plot.\"\"\"     zs = []     for x, y in zip(xs, ys):         zs.extend([x] * int(y))     return jnp.asarray(zs) In\u00a0[3]: Copied! <pre>fig, (axis_perform, axis_compile) = plt.subplots(\n    ncols=2, figsize=(8, 3), dpi=150, sharex=True\n)\n\nresults = load_results()\naxis_compile, axis_perform = plot_results(axis_compile, axis_perform, results)\n\naxis_compile.set_title(\"Compilation time\")\naxis_perform.set_title(\"Evaluation time\")\naxis_compile.legend(loc=\"lower right\")\naxis_compile.set_xlabel(\"Number of Derivatives\")\naxis_perform.set_xlabel(\"Number of Derivatives\")\naxis_perform.set_ylabel(\"Wall time (sec)\")\naxis_perform.grid()\naxis_compile.grid()\n\n\nplt.show()\n</pre> fig, (axis_perform, axis_compile) = plt.subplots(     ncols=2, figsize=(8, 3), dpi=150, sharex=True )  results = load_results() axis_compile, axis_perform = plot_results(axis_compile, axis_perform, results)  axis_compile.set_title(\"Compilation time\") axis_perform.set_title(\"Evaluation time\") axis_compile.legend(loc=\"lower right\") axis_compile.set_xlabel(\"Number of Derivatives\") axis_perform.set_xlabel(\"Number of Derivatives\") axis_perform.set_ylabel(\"Wall time (sec)\") axis_perform.grid() axis_compile.grid()   plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"benchmarks/taylor_node/plot/#neural-ode","title":"Neural ODE\u00b6","text":""},{"location":"benchmarks/taylor_pleiades/plot/","title":"Pleiades","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark all Taylor-series estimators on the Pleiades problem.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n</pre> \"\"\"Benchmark all Taylor-series estimators on the Pleiades problem.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt import jax   jax.config.update(\"jax_platform_name\", \"cpu\") In\u00a0[2]: Copied! <pre>def load_results():\n    \"\"\"Load the results from a file.\"\"\"\n    return jnp.load(\"./results.npy\", allow_pickle=True)[()]\n\n\ndef choose_style(label):\n    \"\"\"Choose a plotting style for a given algorithm.\"\"\"\n    if \"doubling\" in label.lower():\n        return {\"color\": \"C3\", \"linestyle\": \"dotted\", \"label\": label}\n    if \"unroll\" in label.lower():\n        return {\"color\": \"C2\", \"linestyle\": \"dashdot\", \"label\": label}\n    if \"taylor\" in label.lower():\n        return {\"color\": \"C0\", \"linestyle\": \"solid\", \"label\": label}\n    if \"forward\" in label.lower():\n        return {\"color\": \"C1\", \"linestyle\": \"dashed\", \"label\": label}\n    msg = f\"Label {label} unknown.\"\n    raise ValueError(msg)\n\n\ndef plot_results(axis_compile, axis_perform, results):\n    \"\"\"Plot the results.\"\"\"\n    for label, wp in results.items():\n        style = choose_style(label)\n\n        inputs = wp[\"arguments\"]\n        work_mean = wp[\"work_compile\"]\n        axis_compile.semilogy(inputs, work_mean, **style)\n\n        work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])\n        range_lower, range_upper = work_mean - work_std, work_mean + work_std\n        axis_perform.semilogy(inputs, work_mean, **style)\n        axis_perform.fill_between(inputs, range_lower, range_upper, alpha=0.3, **style)\n\n    return axis_compile, axis_perform\n</pre> def load_results():     \"\"\"Load the results from a file.\"\"\"     return jnp.load(\"./results.npy\", allow_pickle=True)[()]   def choose_style(label):     \"\"\"Choose a plotting style for a given algorithm.\"\"\"     if \"doubling\" in label.lower():         return {\"color\": \"C3\", \"linestyle\": \"dotted\", \"label\": label}     if \"unroll\" in label.lower():         return {\"color\": \"C2\", \"linestyle\": \"dashdot\", \"label\": label}     if \"taylor\" in label.lower():         return {\"color\": \"C0\", \"linestyle\": \"solid\", \"label\": label}     if \"forward\" in label.lower():         return {\"color\": \"C1\", \"linestyle\": \"dashed\", \"label\": label}     msg = f\"Label {label} unknown.\"     raise ValueError(msg)   def plot_results(axis_compile, axis_perform, results):     \"\"\"Plot the results.\"\"\"     for label, wp in results.items():         style = choose_style(label)          inputs = wp[\"arguments\"]         work_mean = wp[\"work_compile\"]         axis_compile.semilogy(inputs, work_mean, **style)          work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])         range_lower, range_upper = work_mean - work_std, work_mean + work_std         axis_perform.semilogy(inputs, work_mean, **style)         axis_perform.fill_between(inputs, range_lower, range_upper, alpha=0.3, **style)      return axis_compile, axis_perform In\u00a0[3]: Copied! <pre>fig, (axis_perform, axis_compile) = plt.subplots(\n    ncols=2, dpi=150, sharex=True, figsize=(8, 3)\n)\n\nresults = load_results()\n\naxis_compile, axis_perform = plot_results(axis_compile, axis_perform, results)\n\naxis_compile.set_title(\"Compilation time\")\naxis_perform.set_title(\"Evaluation time\")\naxis_compile.legend()\naxis_compile.set_xlabel(\"Number of Derivatives\")\naxis_perform.set_xlabel(\"Number of Derivatives\")\naxis_perform.set_ylabel(\"Wall time (sec)\")\naxis_perform.grid()\naxis_compile.grid()\naxis_perform.set_yticks((1e-5, 1e-4))\n\n\nplt.show()\n</pre> fig, (axis_perform, axis_compile) = plt.subplots(     ncols=2, dpi=150, sharex=True, figsize=(8, 3) )  results = load_results()  axis_compile, axis_perform = plot_results(axis_compile, axis_perform, results)  axis_compile.set_title(\"Compilation time\") axis_perform.set_title(\"Evaluation time\") axis_compile.legend() axis_compile.set_xlabel(\"Number of Derivatives\") axis_perform.set_xlabel(\"Number of Derivatives\") axis_perform.set_ylabel(\"Wall time (sec)\") axis_perform.grid() axis_compile.grid() axis_perform.set_yticks((1e-5, 1e-4))   plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"benchmarks/taylor_pleiades/plot/#pleiades","title":"Pleiades\u00b6","text":"<p>The Pleiades problem is a common non-stiff differential equation.</p>"},{"location":"benchmarks/vanderpol/plot/","title":"Stiff Van-der-Pol","text":"In\u00a0[1]: Copied! <pre>\"\"\"Benchmark all solvers on the Van-der-Pol problem.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport jax\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n</pre> \"\"\"Benchmark all solvers on the Van-der-Pol problem.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt import jax  jax.config.update(\"jax_platform_name\", \"cpu\") In\u00a0[2]: Copied! <pre>def load_results():\n    \"\"\"Load the results from a file.\"\"\"\n    return jnp.load(\"./results.npy\", allow_pickle=True)[()]\n\n\ndef load_solution():\n    \"\"\"Load the solution-to-be-plotted from a file.\"\"\"\n    ts = jnp.load(\"./plot_ts.npy\")\n    ys = jnp.load(\"./plot_ys.npy\")\n    return ts, ys\n\n\ndef choose_style(label):\n    \"\"\"Choose a plotting style for a given algorithm.\"\"\"\n    if \"probdiffeq\" in label.lower():\n        return {\"color\": \"C0\", \"linestyle\": \"solid\"}\n    if \"numba\" in label.lower():\n        return {\"color\": \"C4\", \"linestyle\": \"dashed\"}\n    if \"scipy\" in label.lower():\n        return {\"color\": \"C2\", \"linestyle\": \"dashed\"}\n    if \"diffrax\" in label.lower():\n        return {\"color\": \"C3\", \"linestyle\": \"dotted\"}\n    msg = f\"Label {label} unknown.\"\n    raise ValueError(msg)\n\n\ndef plot_results(axis, results):\n    \"\"\"Plot the results.\"\"\"\n    axis.set_title(\"Benchmark\")\n    for label, wp in results.items():\n        style = choose_style(label)\n\n        precision = wp[\"precision\"]\n        work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])\n        axis.loglog(precision, work_mean, label=label, **style)\n\n        range_lower, range_upper = work_mean - work_std, work_mean + work_std\n        axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)\n\n    axis.set_xlabel(\"Precision [absolute RMSE]\")\n    axis.set_ylabel(\"Work [wall time, s]\")\n    axis.legend(\n        loc=\"upper center\",\n        ncols=3,\n        fontsize=\"x-small\",\n        mode=\"expand\",\n        facecolor=\"ghostwhite\",\n    )\n    axis.grid()\n    return axis\n\n\ndef plot_solution(axis, ts, ys, yscale=\"linear\"):\n    \"\"\"Plot the IVP solution.\"\"\"\n    axis.set_title(\"Van-der-Pol (stiffness: $10^5$)\")\n    kwargs = {\"alpha\": 0.85}\n\n    axis.plot(\n        ts,\n        ys[:, 0],\n        label=\"y\",\n        linestyle=\"solid\",\n        color=\"black\",\n        marker=\"None\",\n        **kwargs,\n    )\n    axis.plot(\n        ts,\n        ys[:, 1],\n        label=r\"$\\dot y$\",\n        linestyle=\"dashed\",\n        color=\"black\",\n        marker=\"None\",\n        **kwargs,\n    )\n\n    axis.legend(facecolor=\"ghostwhite\")\n    axis.set_xlabel(\"Time $t$\")\n    axis.set_ylabel(\"Solution $y$ [clipped]\")\n    axis.set_yscale(yscale)\n    axis.set_ylim((-6, 6))\n    return axis\n</pre> def load_results():     \"\"\"Load the results from a file.\"\"\"     return jnp.load(\"./results.npy\", allow_pickle=True)[()]   def load_solution():     \"\"\"Load the solution-to-be-plotted from a file.\"\"\"     ts = jnp.load(\"./plot_ts.npy\")     ys = jnp.load(\"./plot_ys.npy\")     return ts, ys   def choose_style(label):     \"\"\"Choose a plotting style for a given algorithm.\"\"\"     if \"probdiffeq\" in label.lower():         return {\"color\": \"C0\", \"linestyle\": \"solid\"}     if \"numba\" in label.lower():         return {\"color\": \"C4\", \"linestyle\": \"dashed\"}     if \"scipy\" in label.lower():         return {\"color\": \"C2\", \"linestyle\": \"dashed\"}     if \"diffrax\" in label.lower():         return {\"color\": \"C3\", \"linestyle\": \"dotted\"}     msg = f\"Label {label} unknown.\"     raise ValueError(msg)   def plot_results(axis, results):     \"\"\"Plot the results.\"\"\"     axis.set_title(\"Benchmark\")     for label, wp in results.items():         style = choose_style(label)          precision = wp[\"precision\"]         work_mean, work_std = (wp[\"work_mean\"], wp[\"work_std\"])         axis.loglog(precision, work_mean, label=label, **style)          range_lower, range_upper = work_mean - work_std, work_mean + work_std         axis.fill_between(precision, range_lower, range_upper, alpha=0.3, **style)      axis.set_xlabel(\"Precision [absolute RMSE]\")     axis.set_ylabel(\"Work [wall time, s]\")     axis.legend(         loc=\"upper center\",         ncols=3,         fontsize=\"x-small\",         mode=\"expand\",         facecolor=\"ghostwhite\",     )     axis.grid()     return axis   def plot_solution(axis, ts, ys, yscale=\"linear\"):     \"\"\"Plot the IVP solution.\"\"\"     axis.set_title(\"Van-der-Pol (stiffness: $10^5$)\")     kwargs = {\"alpha\": 0.85}      axis.plot(         ts,         ys[:, 0],         label=\"y\",         linestyle=\"solid\",         color=\"black\",         marker=\"None\",         **kwargs,     )     axis.plot(         ts,         ys[:, 1],         label=r\"$\\dot y$\",         linestyle=\"dashed\",         color=\"black\",         marker=\"None\",         **kwargs,     )      axis.legend(facecolor=\"ghostwhite\")     axis.set_xlabel(\"Time $t$\")     axis.set_ylabel(\"Solution $y$ [clipped]\")     axis.set_yscale(yscale)     axis.set_ylim((-6, 6))     return axis In\u00a0[3]: Copied! <pre>layout = [\n    [\"solution\", \"benchmark\", \"benchmark\"],\n    [\"solution\", \"benchmark\", \"benchmark\"],\n]\nfig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)\n\n\nresults = load_results()\nts, ys = load_solution()\n\n_ = plot_results(axes[\"benchmark\"], results)\n_ = plot_solution(axes[\"solution\"], ts, ys)\n\nplt.show()\n</pre> layout = [     [\"solution\", \"benchmark\", \"benchmark\"],     [\"solution\", \"benchmark\", \"benchmark\"], ] fig, axes = plt.subplot_mosaic(layout, figsize=(8, 3), constrained_layout=True, dpi=300)   results = load_results() ts, ys = load_solution()  _ = plot_results(axes[\"benchmark\"], results) _ = plot_solution(axes[\"solution\"], ts, ys)  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"benchmarks/vanderpol/plot/#stiff-van-der-pol","title":"Stiff Van-der-Pol\u00b6","text":"<p>The van der Pol problem is a common stiff differential equation.</p>"},{"location":"dev_docs/continuous_integration/","title":"Continuous Integration","text":"<p>This guide explains how to install dependencies, run linting and formatting checks, execute tests, and build documentation as part of the continuous integration (CI) process.</p>"},{"location":"dev_docs/continuous_integration/#installation","title":"Installation","text":"<p>After cloning the repository, in the root of the project, and assuming JAX is already installed, do the following: To install all development dependencies, use one or more of the following commands:</p> <pre><code>pip install .[test]  \npip install .[format-and-lint] \npip install .[doc] \n</code></pre> <p>To install everything required for development, you can install all extras at once:</p> <pre><code>pip install .[test,format-and-lint,doc]\n</code></pre>"},{"location":"dev_docs/continuous_integration/#running-checks","title":"Running Checks","text":"<p>The project uses a <code>Makefile</code> to streamline common CI tasks.  You can run the following commands to check code quality and correctness:</p>"},{"location":"dev_docs/continuous_integration/#1-formatting-and-linting","title":"1. Formatting and Linting","text":"<p>To check code formatting and linting rules, run:</p> <pre><code>make format-and-lint\n</code></pre> <p>This will: - Ensure code is properly formatted. - Verify that imports are correctly ordered. - Check for style violations and linting issues. - Enforce documentation conventions.</p>"},{"location":"dev_docs/continuous_integration/#2-running-tests","title":"2. Running Tests","text":"<p>To execute all tests, use:</p> <pre><code>make test\n</code></pre> <p>This will: - Run all tests. - Execute tests in parallel for efficiency.</p>"},{"location":"dev_docs/continuous_integration/#3-running-benchmarks","title":"3. Running Benchmarks","text":"<p>To evaluate the performance of the code, benchmarks can be executed. There are different configurations for benchmarks.</p> <p>To run the full benchmark suite, use:</p> <pre><code>make benchmarks-run\nmake benchmarks-plot-results\n</code></pre> <p>This will: - Execute benchmarking scripts to assess performance. - Plot the results so that the next documentation build displays the results.</p> <p>Benchmarking parameters and configurations can be adjusted in the relevant benchmark scripts, located in the <code>doc/benchmarks/</code> directory.</p> <p>If the goal is not a full benchmark run, but simply a check whether the benchmark scripts execute correctly, use: <pre><code>make benchmarks-run-dry-run\n</code></pre> This is helpful to verify that API changes are reflected in the benchmark code.</p>"},{"location":"dev_docs/continuous_integration/#4-building-documentation","title":"4. Building Documentation","text":"<p>To generate the documentation, use:</p> <pre><code>make doc\n</code></pre> <p>This will: - Sync content in docs/* with the rest of the repo. - Process Jupyter notebooks and Markdown files. - Build the documentation site.</p>"},{"location":"dev_docs/continuous_integration/#5-cleaning-up","title":"5. Cleaning Up","text":"<p>To remove auxiliary files generated during testing or documentation builds, run:</p> <pre><code>make clean\n</code></pre> <p>This removes unnecessary files (eg pytest or mypy caches) to keep the repository clean.</p>"},{"location":"dev_docs/continuous_integration/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>To ensure code quality before committing, the project uses <code>pre-commit</code> hooks. These automatically format, lint, and check files before they are committed to the repository.</p>"},{"location":"dev_docs/continuous_integration/#setting-up-pre-commit","title":"Setting Up Pre-commit","text":"<p>Install <code>pre-commit</code> and set up the hooks by running:</p> <pre><code>pip install pre-commit  # Included in `pip install -e .[format-and-lint]`\npre-commit install\n</code></pre>"},{"location":"dev_docs/continuous_integration/#running-pre-commit-manually","title":"Running Pre-commit Manually","text":"<p>To check all files, not just the staged ones, run:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>To check only the files staged for commit, run:</p> <pre><code>pre-commit run\n</code></pre> <p>This ensures that only properly formatted and linted code is committed.</p>"},{"location":"dev_docs/creating_example_notebook/","title":"Creating an example notebook","text":""},{"location":"dev_docs/creating_example_notebook/#tutorial","title":"Tutorial","text":"<p>To create a new example notebook and include it in the documentation, follow the steps:</p> <ol> <li>Create a jupyter notebook, preferably in <code>docs/examples_*/</code> and fill it with content.    In case you are wondering which subfolder is most appropriate:     if your notebook introduces an external dependency     (for example, an optimisation or sampling library),     do not place it next to the solver-configuration notebooks.</li> <li>Pair the notebook with a <code>py:light</code> version of the notebook via jupytext. This is important for version control, which ignores all examples with <code>*.ipynb</code> or <code>*.md</code> ending.</li> <li>Include the notebook into the docs by mentioning it in the <code>nav</code> section of <code>mkdocs.yml</code></li> <li>Enjoy.</li> </ol>"},{"location":"dev_docs/creating_example_notebook/#benchmark","title":"Benchmark","text":"<ol> <li>Create a new folder in the <code>docs/benchmarks/</code> directory</li> <li>Create the benchmark script. Usually, the execution is in a python script and the plotting in a jupyter notebook.</li> <li>Link the (plotting-)notebook to a markdown file (for better version control). </li> <li>Include the (plotting-)notebook into the docs via <code>mkdocs.yml</code>. Mention the markdown and python script in the same folder under <code>mkdocs.yml -&gt; exclude</code></li> <li>Mention the new benchmark in the makefile (<code>benchmarks-run</code>, <code>benchmarks-dry-run</code>). A dry-run is for checking that the code functions properly. The benchmark run itself should not take less than a minute, otherwise the whole benchmark suite grows out of hand.</li> </ol>"},{"location":"dev_docs/public_api/","title":"Private and public API","text":"<p>All public functions and classes that are in the online documentation  are considered public API. At the moment, this affects the following:</p> <ul> <li><code>ivpsolve.py</code></li> <li><code>adaptive.py</code></li> <li><code>taylor/*</code></li> <li><code>ivpsolvers.py</code></li> <li><code>stats.py</code></li> <li><code>impl.impl.select()</code></li> </ul> <p>Exceptions to this rule are all functions and class that are  marked as <code>warning: highly experimental</code>, e.g., <code>ivpsolve.solve_adaptive_save_at</code>.</p> <p>Everything else (e.g. <code>backend</code>, <code>util</code>, <code>impl</code>) is not public and breaking changes here will not necessarily increase the version.</p>"},{"location":"examples_advanced/equinox_while_loop/","title":"Equinox's while-loops","text":"In\u00a0[1]: Copied! <pre>\"\"\"Use Equinox's while loop to compute gradients of `simulate_terminal_values`.\"\"\"\n\nimport equinox\nimport jax\nimport jax.numpy as jnp\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\nfrom probdiffeq.backend import control_flow\n</pre> \"\"\"Use Equinox's while loop to compute gradients of `simulate_terminal_values`.\"\"\"  import equinox import jax import jax.numpy as jnp  from probdiffeq import ivpsolve, ivpsolvers, taylor from probdiffeq.backend import control_flow  <p>Overwrite the while-loop (via a context manager):</p> In\u00a0[2]: Copied! <pre>def while_loop_func(*a, **kw):\n    \"\"\"Evaluate a bounded while loop.\"\"\"\n    return equinox.internal.while_loop(*a, **kw, kind=\"bounded\", max_steps=100)\n\n\ncontext_compute_gradient = control_flow.context_overwrite_while_loop(while_loop_func)\n</pre> def while_loop_func(*a, **kw):     \"\"\"Evaluate a bounded while loop.\"\"\"     return equinox.internal.while_loop(*a, **kw, kind=\"bounded\", max_steps=100)   context_compute_gradient = control_flow.context_overwrite_while_loop(while_loop_func) <p>The rest is the similar to the \"easy example\" in the quickstart, except for simulating adaptively and computing the value and the gradient (which is impossible without the specialised while-loop implementation).</p> In\u00a0[3]: Copied! <pre>def solution_routine():\n    \"\"\"Construct a parameter-to-solution function and an initial value.\"\"\"\n\n    def vf(y, *, t):  # noqa: ARG001\n        \"\"\"Evaluate the vector field.\"\"\"\n        return 0.5 * y * (1 - y)\n\n    t0, t1 = 0.0, 1.0\n    u0 = jnp.asarray([0.1])\n\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"isotropic\")\n    ts0 = ivpsolvers.correction_ts0(vf, ode_order=1, ssm=ssm)\n\n    strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n\n    def simulate(init_val):\n        \"\"\"Evaluate the parameter-to-solution function.\"\"\"\n        sol = ivpsolve.solve_adaptive_terminal_values(\n            init_val, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n\n        # Any scalar function of the IVP solution would do\n        return jnp.dot(sol.u[0], sol.u[0])\n\n    return simulate, init\n</pre> def solution_routine():     \"\"\"Construct a parameter-to-solution function and an initial value.\"\"\"      def vf(y, *, t):  # noqa: ARG001         \"\"\"Evaluate the vector field.\"\"\"         return 0.5 * y * (1 - y)      t0, t1 = 0.0, 1.0     u0 = jnp.asarray([0.1])      tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"isotropic\")     ts0 = ivpsolvers.correction_ts0(vf, ode_order=1, ssm=ssm)      strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)      def simulate(init_val):         \"\"\"Evaluate the parameter-to-solution function.\"\"\"         sol = ivpsolve.solve_adaptive_terminal_values(             init_val, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm         )          # Any scalar function of the IVP solution would do         return jnp.dot(sol.u[0], sol.u[0])      return simulate, init In\u00a0[4]: Copied! <pre>try:\n    solve, x = solution_routine()\n    solution, gradient = jax.value_and_grad(solve)(x)\nexcept ValueError as err:\n    print(f\"Caught error:\\n\\t {err}\")\n</pre> try:     solve, x = solution_routine()     solution, gradient = jax.value_and_grad(solve)(x) except ValueError as err:     print(f\"Caught error:\\n\\t {err}\") <pre>Caught error:\n\t Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop.\n</pre> In\u00a0[5]: Copied! <pre>with context_compute_gradient:\n    # Construct the solution routine inside the context\n    solve, x = solution_routine()\n\n    # Compute gradients\n    solution, gradient = jax.value_and_grad(solve)(x)\n\n    print(solution)\n    print(gradient)\n</pre> with context_compute_gradient:     # Construct the solution routine inside the context     solve, x = solution_routine()      # Compute gradients     solution, gradient = jax.value_and_grad(solve)(x)      print(solution)     print(gradient) <pre>0.023939388\nNormal(mean=Array([[0.4424412 ],\n       [0.01854868]], dtype=float32), cholesky=Array([[0., 0.],\n       [0., 0.]], dtype=float32))\n</pre>"},{"location":"examples_advanced/equinox_while_loop/#equinoxs-while-loops","title":"Equinox's while-loops\u00b6","text":"<p>Use Equinox's bounded while loop to enable reverse-mode differentiation of adaptive IVP solvers.</p>"},{"location":"examples_advanced/neural_ode/","title":"Diffusion tempering &amp; NODEs","text":"In\u00a0[1]: Copied! <pre>\"\"\"Train a neural ODE with ProbDiffEq and Optax using diffusion tempering.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats\n\n\ndef main(num_data=100, epochs=1_000, print_every=100, hidden=(20,), lr=0.2):\n    \"\"\"Train a neural ODE using diffusion tempering.\"\"\"\n    # Create some data and construct a neural ODE\n    grid = jnp.linspace(0, 1, num=num_data)\n    data = jnp.sin(2.5 * jnp.pi * grid) * jnp.pi * grid\n    stdev = 1e-1\n    output_scale = 1e2\n    vf, u0, (t0, t1), f_args = vf_neural_ode(hidden=hidden, t0=0.0, t1=1)\n\n    # Create a loss (this is where probabilistic numerics enters!)\n    loss = loss_log_marginal_likelihood(vf=vf, t0=t0)\n    loss0, info0 = loss(\n        f_args, u0=u0, grid=grid, data=data, stdev=stdev, output_scale=output_scale\n    )\n\n    # Plot the data and the initial guess\n    plt.title(f\"Initial estimate | Loss: {loss0:.2f}\")\n    plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")\n    plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Estimate\", color=\"C1\")\n    plt.legend()\n    plt.show()\n\n    # Construct an optimiser\n    optim = optax.adam(lr)\n    train_step = train_step_optax(optim, loss=loss)\n\n    # Train the model\n    state = optim.init(f_args)\n    print(\"Loss after...\")\n    for i in range(epochs):\n        (f_args, state), info = train_step(\n            f_args,\n            state,\n            u0=u0,\n            grid=grid,\n            data=data,\n            stdev=stdev,\n            output_scale=output_scale,\n        )\n\n        # Print progressbar\n        if i % print_every == print_every - 1:\n            print(f\"...{(i + 1)} epochs: loss={info['loss']:.3e}\")\n\n        # Diffusion tempering: https://arxiv.org/abs/2402.12231\n        # To all users: Adjust this tempering and\n        # see how it affects parameter estimation.\n        if i % 100 == 0:\n            output_scale /= 10.0\n\n    # Plot the results\n    plt.title(f\"Final estimate | Loss: {info['loss']:.2f}\")\n    plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")\n    plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Initial estimate\", color=\"C1\")\n    plt.plot(grid, info[\"sol\"].u[0], \"-\", label=\"Final estimate\", color=\"C2\")\n    plt.legend()\n    plt.show()\n\n\ndef vf_neural_ode(*, hidden: tuple, t0: float, t1: float):\n    \"\"\"Build a neural ODE.\"\"\"\n    f_args, mlp = model_mlp(hidden=hidden, shape_in=(2,), shape_out=(1,))\n    u0 = jnp.asarray([0.0])\n\n    @jax.jit\n    def vf(y, *, t, p):\n        \"\"\"Evaluate the neural ODE vector field.\"\"\"\n        y_and_t = jnp.concatenate([y, t[None]])\n        return mlp(p, y_and_t)\n\n    return vf, (u0,), (t0, t1), f_args\n\n\ndef model_mlp(\n    *, hidden: tuple, shape_in: tuple = (), shape_out: tuple = (), activation=jnp.tanh\n):\n    \"\"\"Construct an MLP.\"\"\"\n    assert len(shape_in) &lt;= 1\n    assert len(shape_out) &lt;= 1\n\n    shape_prev = shape_in\n    weights = []\n    for h in hidden:\n        W = jnp.empty((h, *shape_prev))\n        b = jnp.empty((h,))\n        shape_prev = (h,)\n        weights.append((W, b))\n\n    W = jnp.empty((*shape_out, *shape_prev))\n    b = jnp.empty(shape_out)\n    weights.append((W, b))\n\n    p_flat, unravel = jax.flatten_util.ravel_pytree(weights)\n\n    def fwd(w, x):\n        for A, b in w[:-1]:\n            x = jnp.dot(A, x) + b\n            x = activation(x)\n\n        A, b = w[-1]\n        return jnp.dot(A, x) + b\n\n    key = jax.random.PRNGKey(1)\n    p_init = jax.random.normal(key, shape=p_flat.shape, dtype=p_flat.dtype)\n    return unravel(p_init), fwd\n\n\ndef loss_log_marginal_likelihood(vf, *, t0):\n    \"\"\"Build a loss function from an ODE problem.\"\"\"\n\n    @jax.jit\n    def loss(\n        p: jax.Array,\n        *,\n        u0: tuple,\n        grid: jax.Array,\n        data: jax.Array,\n        stdev: jax.Array,\n        output_scale: jax.Array,\n    ):\n        \"\"\"Loss function: log-marginal likelihood of the data.\"\"\"\n        # Build a solver\n        tcoeffs = (*u0, vf(*u0, t=t0, p=p))\n        init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n            tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n        )\n        ts0 = ivpsolvers.correction_ts0(lambda *a, **kw: vf(*a, **kw, p=p), ssm=ssm)\n        strategy = ivpsolvers.strategy_smoother(ssm=ssm)\n        solver_ts0 = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n\n        # Solve\n        sol = ivpsolve.solve_fixed_grid(init, grid=grid, solver=solver_ts0, ssm=ssm)\n\n        # Evaluate loss\n        marginal_likelihood = stats.log_marginal_likelihood(\n            data[:, None],\n            standard_deviation=jnp.ones_like(grid) * stdev,\n            posterior=sol.posterior,\n            ssm=sol.ssm,\n        )\n        return -1 * marginal_likelihood, {\"sol\": sol}\n\n    return loss\n\n\ndef train_step_optax(optimizer, loss):\n    \"\"\"Implement a training step using Optax.\"\"\"\n\n    @jax.jit\n    def update(params, opt_state, **loss_kwargs):\n        \"\"\"Update the optimiser state.\"\"\"\n        value_and_grad = jax.value_and_grad(loss, argnums=0, has_aux=True)\n        (value, info), grads = value_and_grad(params, **loss_kwargs)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n\n        info[\"loss\"] = value\n        return (params, opt_state), info\n\n    return update\n\n\nif __name__ == \"__main__\":\n    main()\n</pre> \"\"\"Train a neural ODE with ProbDiffEq and Optax using diffusion tempering.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax  from probdiffeq import ivpsolve, ivpsolvers, stats   def main(num_data=100, epochs=1_000, print_every=100, hidden=(20,), lr=0.2):     \"\"\"Train a neural ODE using diffusion tempering.\"\"\"     # Create some data and construct a neural ODE     grid = jnp.linspace(0, 1, num=num_data)     data = jnp.sin(2.5 * jnp.pi * grid) * jnp.pi * grid     stdev = 1e-1     output_scale = 1e2     vf, u0, (t0, t1), f_args = vf_neural_ode(hidden=hidden, t0=0.0, t1=1)      # Create a loss (this is where probabilistic numerics enters!)     loss = loss_log_marginal_likelihood(vf=vf, t0=t0)     loss0, info0 = loss(         f_args, u0=u0, grid=grid, data=data, stdev=stdev, output_scale=output_scale     )      # Plot the data and the initial guess     plt.title(f\"Initial estimate | Loss: {loss0:.2f}\")     plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")     plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Estimate\", color=\"C1\")     plt.legend()     plt.show()      # Construct an optimiser     optim = optax.adam(lr)     train_step = train_step_optax(optim, loss=loss)      # Train the model     state = optim.init(f_args)     print(\"Loss after...\")     for i in range(epochs):         (f_args, state), info = train_step(             f_args,             state,             u0=u0,             grid=grid,             data=data,             stdev=stdev,             output_scale=output_scale,         )          # Print progressbar         if i % print_every == print_every - 1:             print(f\"...{(i + 1)} epochs: loss={info['loss']:.3e}\")          # Diffusion tempering: https://arxiv.org/abs/2402.12231         # To all users: Adjust this tempering and         # see how it affects parameter estimation.         if i % 100 == 0:             output_scale /= 10.0      # Plot the results     plt.title(f\"Final estimate | Loss: {info['loss']:.2f}\")     plt.plot(grid, data, \"x\", label=\"Data\", color=\"C0\")     plt.plot(grid, info0[\"sol\"].u[0], \"-\", label=\"Initial estimate\", color=\"C1\")     plt.plot(grid, info[\"sol\"].u[0], \"-\", label=\"Final estimate\", color=\"C2\")     plt.legend()     plt.show()   def vf_neural_ode(*, hidden: tuple, t0: float, t1: float):     \"\"\"Build a neural ODE.\"\"\"     f_args, mlp = model_mlp(hidden=hidden, shape_in=(2,), shape_out=(1,))     u0 = jnp.asarray([0.0])      @jax.jit     def vf(y, *, t, p):         \"\"\"Evaluate the neural ODE vector field.\"\"\"         y_and_t = jnp.concatenate([y, t[None]])         return mlp(p, y_and_t)      return vf, (u0,), (t0, t1), f_args   def model_mlp(     *, hidden: tuple, shape_in: tuple = (), shape_out: tuple = (), activation=jnp.tanh ):     \"\"\"Construct an MLP.\"\"\"     assert len(shape_in) &lt;= 1     assert len(shape_out) &lt;= 1      shape_prev = shape_in     weights = []     for h in hidden:         W = jnp.empty((h, *shape_prev))         b = jnp.empty((h,))         shape_prev = (h,)         weights.append((W, b))      W = jnp.empty((*shape_out, *shape_prev))     b = jnp.empty(shape_out)     weights.append((W, b))      p_flat, unravel = jax.flatten_util.ravel_pytree(weights)      def fwd(w, x):         for A, b in w[:-1]:             x = jnp.dot(A, x) + b             x = activation(x)          A, b = w[-1]         return jnp.dot(A, x) + b      key = jax.random.PRNGKey(1)     p_init = jax.random.normal(key, shape=p_flat.shape, dtype=p_flat.dtype)     return unravel(p_init), fwd   def loss_log_marginal_likelihood(vf, *, t0):     \"\"\"Build a loss function from an ODE problem.\"\"\"      @jax.jit     def loss(         p: jax.Array,         *,         u0: tuple,         grid: jax.Array,         data: jax.Array,         stdev: jax.Array,         output_scale: jax.Array,     ):         \"\"\"Loss function: log-marginal likelihood of the data.\"\"\"         # Build a solver         tcoeffs = (*u0, vf(*u0, t=t0, p=p))         init, ibm, ssm = ivpsolvers.prior_wiener_integrated(             tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"         )         ts0 = ivpsolvers.correction_ts0(lambda *a, **kw: vf(*a, **kw, p=p), ssm=ssm)         strategy = ivpsolvers.strategy_smoother(ssm=ssm)         solver_ts0 = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)          # Solve         sol = ivpsolve.solve_fixed_grid(init, grid=grid, solver=solver_ts0, ssm=ssm)          # Evaluate loss         marginal_likelihood = stats.log_marginal_likelihood(             data[:, None],             standard_deviation=jnp.ones_like(grid) * stdev,             posterior=sol.posterior,             ssm=sol.ssm,         )         return -1 * marginal_likelihood, {\"sol\": sol}      return loss   def train_step_optax(optimizer, loss):     \"\"\"Implement a training step using Optax.\"\"\"      @jax.jit     def update(params, opt_state, **loss_kwargs):         \"\"\"Update the optimiser state.\"\"\"         value_and_grad = jax.value_and_grad(loss, argnums=0, has_aux=True)         (value, info), grads = value_and_grad(params, **loss_kwargs)         updates, opt_state = optimizer.update(grads, opt_state)         params = optax.apply_updates(params, updates)          info[\"loss\"] = value         return (params, opt_state), info      return update   if __name__ == \"__main__\":     main() <pre>Loss after...\n</pre> <pre>...100 epochs: loss=2.420e+01\n...200 epochs: loss=1.774e+01\n</pre> <pre>...300 epochs: loss=2.794e+00\n...400 epochs: loss=4.559e+01\n</pre> <pre>...500 epochs: loss=1.050e+01\n...600 epochs: loss=1.653e-01\n</pre> <pre>...700 epochs: loss=-1.223e+00\n...800 epochs: loss=-1.297e+00\n</pre> <pre>...900 epochs: loss=-1.335e+00\n...1000 epochs: loss=-1.248e+00\n</pre>"},{"location":"examples_advanced/neural_ode/#diffusion-tempering-nodes","title":"Diffusion tempering &amp; NODEs\u00b6","text":""},{"location":"examples_advanced/parameter_estimation_blackjax/","title":"Parameter estimation (BlackJAX)","text":"In\u00a0[1]: Copied! <pre>\"\"\"Estimate ODE paramaters with ProbDiffEq and BlackJAX.\"\"\"\n\nimport functools\n\nimport blackjax\nimport jax\nimport jax.experimental.ode\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n</pre> \"\"\"Estimate ODE paramaters with ProbDiffEq and BlackJAX.\"\"\"  import functools  import blackjax import jax import jax.experimental.ode import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor In\u00a0[2]: Copied! <pre># IVP examples in JAX\nif not backend.has_been_selected:\n    backend.select(\"jax\")\n</pre>  # IVP examples in JAX if not backend.has_been_selected:     backend.select(\"jax\")  In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.lotka_volterra()\n\n\n@jax.jit\ndef vf(y, *, t):  # noqa: ARG001\n    \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"\n    return f(y, *f_args)\n\n\ntheta_true = u0 + 0.5 * jnp.flip(u0)\ntheta_guess = u0  # initial guess\n</pre> f, u0, (t0, t1), f_args = ivps.lotka_volterra()   @jax.jit def vf(y, *, t):  # noqa: ARG001     \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"     return f(y, *f_args)   theta_true = u0 + 0.5 * jnp.flip(u0) theta_guess = u0  # initial guess In\u00a0[4]: Copied! <pre>def plot_solution(t, u, *, ax, marker=\".\", **plotting_kwargs):\n    \"\"\"Plot the IVP solution.\"\"\"\n    for d in [0, 1]:\n        ax.plot(t, u[:, d], marker=\"None\", **plotting_kwargs)\n        ax.plot(t[0], u[0, d], marker=marker, **plotting_kwargs)\n        ax.plot(t[-1], u[-1, d], marker=marker, **plotting_kwargs)\n    return ax\n\n\n@jax.jit\ndef solve_fixed(theta, *, ts):\n    \"\"\"Evaluate the parameter-to-solution map, solving on a fixed grid.\"\"\"\n    # Create a probabilistic solver\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)\n    output_scale = 10.0\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n        tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n    )\n    ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_filter(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)\n\n\n@jax.jit\ndef solve_adaptive(theta, *, save_at):\n    \"\"\"Evaluate the parameter-to-solution map, solving on an adaptive grid.\"\"\"\n    # Create a probabilistic solver\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)\n    output_scale = 10.0\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n        tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n    )\n    ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_filter(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n    return ivpsolve.solve_adaptive_save_at(\n        init, save_at=save_at, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm\n    )\n\n\nsave_at = jnp.linspace(t0, t1, num=250, endpoint=True)\nsolve_save_at = functools.partial(solve_adaptive, save_at=save_at)\n</pre> def plot_solution(t, u, *, ax, marker=\".\", **plotting_kwargs):     \"\"\"Plot the IVP solution.\"\"\"     for d in [0, 1]:         ax.plot(t, u[:, d], marker=\"None\", **plotting_kwargs)         ax.plot(t[0], u[0, d], marker=marker, **plotting_kwargs)         ax.plot(t[-1], u[-1, d], marker=marker, **plotting_kwargs)     return ax   @jax.jit def solve_fixed(theta, *, ts):     \"\"\"Evaluate the parameter-to-solution map, solving on a fixed grid.\"\"\"     # Create a probabilistic solver     tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)     output_scale = 10.0     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(         tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"     )     ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)     strategy = ivpsolvers.strategy_filter(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)   @jax.jit def solve_adaptive(theta, *, save_at):     \"\"\"Evaluate the parameter-to-solution map, solving on an adaptive grid.\"\"\"     # Create a probabilistic solver     tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (theta,), num=2)     output_scale = 10.0     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(         tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"     )     ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)     strategy = ivpsolvers.strategy_filter(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)     return ivpsolve.solve_adaptive_save_at(         init, save_at=save_at, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm     )   save_at = jnp.linspace(t0, t1, num=250, endpoint=True) solve_save_at = functools.partial(solve_adaptive, save_at=save_at) In\u00a0[5]: Copied! <pre># Visualise the initial guess and the data\n\nfig, ax = plt.subplots(figsize=(5, 3))\n\ndata_kwargs = {\"alpha\": 0.5, \"color\": \"gray\"}\nax.annotate(\"Data\", (13.0, 30.0), **data_kwargs)\nsol = solve_save_at(theta_true)\nax = plot_solution(sol.t, sol.u[0], ax=ax, **data_kwargs)\n\nguess_kwargs = {\"color\": \"C3\"}\nax.annotate(\"Initial guess\", (7.5, 20.0), **guess_kwargs)\nsol = solve_save_at(theta_guess)\nax = plot_solution(sol.t, sol.u[0], ax=ax, **guess_kwargs)\nplt.show()\n</pre> # Visualise the initial guess and the data  fig, ax = plt.subplots(figsize=(5, 3))  data_kwargs = {\"alpha\": 0.5, \"color\": \"gray\"} ax.annotate(\"Data\", (13.0, 30.0), **data_kwargs) sol = solve_save_at(theta_true) ax = plot_solution(sol.t, sol.u[0], ax=ax, **data_kwargs)  guess_kwargs = {\"color\": \"C3\"} ax.annotate(\"Initial guess\", (7.5, 20.0), **guess_kwargs) sol = solve_save_at(theta_guess) ax = plot_solution(sol.t, sol.u[0], ax=ax, **guess_kwargs) plt.show() In\u00a0[6]: Copied! <pre>mean = theta_guess\ncov = jnp.eye(2) * 30  # fairly uninformed prior\n\n\n@jax.jit\ndef logposterior_fn(theta, *, data, ts, obs_stdev=0.1):\n    \"\"\"Evaluate the logposterior-function of the data.\"\"\"\n    solution = solve_fixed(theta, ts=ts)\n    y_T = jax.tree.map(lambda s: s[-1], solution.posterior)\n    logpdf_data = stats.log_marginal_likelihood_terminal_values(\n        data, standard_deviation=obs_stdev, posterior=y_T, ssm=solution.ssm\n    )\n    logpdf_prior = jax.scipy.stats.multivariate_normal.logpdf(theta, mean=mean, cov=cov)\n    return logpdf_data + logpdf_prior\n\n\n# Fixed steps for reverse-mode differentiability:\n\n\nts = jnp.linspace(t0, t1, endpoint=True, num=100)\ndata = solve_fixed(theta_true, ts=ts).u[0][-1]\n\nlog_M = functools.partial(logposterior_fn, data=data, ts=ts)\n</pre> mean = theta_guess cov = jnp.eye(2) * 30  # fairly uninformed prior   @jax.jit def logposterior_fn(theta, *, data, ts, obs_stdev=0.1):     \"\"\"Evaluate the logposterior-function of the data.\"\"\"     solution = solve_fixed(theta, ts=ts)     y_T = jax.tree.map(lambda s: s[-1], solution.posterior)     logpdf_data = stats.log_marginal_likelihood_terminal_values(         data, standard_deviation=obs_stdev, posterior=y_T, ssm=solution.ssm     )     logpdf_prior = jax.scipy.stats.multivariate_normal.logpdf(theta, mean=mean, cov=cov)     return logpdf_data + logpdf_prior   # Fixed steps for reverse-mode differentiability:   ts = jnp.linspace(t0, t1, endpoint=True, num=100) data = solve_fixed(theta_true, ts=ts).u[0][-1]  log_M = functools.partial(logposterior_fn, data=data, ts=ts) In\u00a0[7]: Copied! <pre>print(jnp.exp(log_M(theta_true)), \"&gt;=\", jnp.exp(log_M(theta_guess)), \"?\")\n</pre> print(jnp.exp(log_M(theta_true)), \"&gt;=\", jnp.exp(log_M(theta_guess)), \"?\") <pre>0.002049896 &gt;= 0.0 ?\n</pre> <p>Set up a sampler.</p> In\u00a0[8]: Copied! <pre>@functools.partial(jax.jit, static_argnames=[\"kernel\", \"num_samples\"])\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    \"\"\"Run BlackJAX' inference loop.\"\"\"\n\n    def one_step(state, rng_key):\n        state, _ = kernel.step(rng_key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n\n    return states\n</pre> @functools.partial(jax.jit, static_argnames=[\"kernel\", \"num_samples\"]) def inference_loop(rng_key, kernel, initial_state, num_samples):     \"\"\"Run BlackJAX' inference loop.\"\"\"      def one_step(state, rng_key):         state, _ = kernel.step(rng_key, state)         return state, state      keys = jax.random.split(rng_key, num_samples)     _, states = jax.lax.scan(one_step, initial_state, keys)      return states <p>Initialise the sampler, warm it up, and run the inference loop.</p> In\u00a0[9]: Copied! <pre>initial_position = theta_guess\nrng_key = jax.random.PRNGKey(0)\n</pre> initial_position = theta_guess rng_key = jax.random.PRNGKey(0) In\u00a0[10]: Copied! <pre># WARMUP\nwarmup = blackjax.window_adaptation(blackjax.nuts, log_M, progress_bar=True)\n\nwarmup_results, _ = warmup.run(rng_key, initial_position, num_steps=200)\n\ninitial_state = warmup_results.state\nstep_size = warmup_results.parameters[\"step_size\"]\ninverse_mass_matrix = warmup_results.parameters[\"inverse_mass_matrix\"]\nnuts_kernel = blackjax.nuts(\n    logdensity_fn=log_M, step_size=step_size, inverse_mass_matrix=inverse_mass_matrix\n)\n</pre> # WARMUP warmup = blackjax.window_adaptation(blackjax.nuts, log_M, progress_bar=True)  warmup_results, _ = warmup.run(rng_key, initial_position, num_steps=200)  initial_state = warmup_results.state step_size = warmup_results.parameters[\"step_size\"] inverse_mass_matrix = warmup_results.parameters[\"inverse_mass_matrix\"] nuts_kernel = blackjax.nuts(     logdensity_fn=log_M, step_size=step_size, inverse_mass_matrix=inverse_mass_matrix ) <pre>Running window adaptation\n</pre>        100.00% [200/200 00:00&lt;?]      In\u00a0[11]: Copied! <pre># INFERENCE LOOP\nrng_key, _ = jax.random.split(rng_key, 2)\nstates = inference_loop(\n    rng_key, kernel=nuts_kernel, initial_state=initial_state, num_samples=150\n)\n</pre> # INFERENCE LOOP rng_key, _ = jax.random.split(rng_key, 2) states = inference_loop(     rng_key, kernel=nuts_kernel, initial_state=initial_state, num_samples=150 ) In\u00a0[12]: Copied! <pre>solution_samples = jax.vmap(solve_save_at)(states.position)\n</pre> solution_samples = jax.vmap(solve_save_at)(states.position) In\u00a0[13]: Copied! <pre># Visualise the initial guess and the data\n\nfig, ax = plt.subplots()\n\nsample_kwargs = {\"color\": \"C0\"}\nax.annotate(\"Samples\", (2.75, 31.0), **sample_kwargs)\nfor ts, us in zip(solution_samples.t, solution_samples.u[0]):\n    ax = plot_solution(ts, us, ax=ax, linewidth=0.1, alpha=0.75, **sample_kwargs)\n\ndata_kwargs = {\"color\": \"gray\"}\nax.annotate(\"Data\", (18.25, 40.0), **data_kwargs)\nsol = solve_save_at(theta_true)\nax = plot_solution(sol.t, sol.u[0], ax=ax, linewidth=4, alpha=0.5, **data_kwargs)\n\nguess_kwargs = {\"color\": \"gray\"}\nax.annotate(\"Initial guess\", (6.0, 12.0), **guess_kwargs)\nsol = solve_save_at(theta_guess)\nax = plot_solution(\n    sol.t, sol.u[0], ax=ax, linestyle=\"dashed\", alpha=0.75, **guess_kwargs\n)\nplt.show()\n</pre> # Visualise the initial guess and the data  fig, ax = plt.subplots()  sample_kwargs = {\"color\": \"C0\"} ax.annotate(\"Samples\", (2.75, 31.0), **sample_kwargs) for ts, us in zip(solution_samples.t, solution_samples.u[0]):     ax = plot_solution(ts, us, ax=ax, linewidth=0.1, alpha=0.75, **sample_kwargs)  data_kwargs = {\"color\": \"gray\"} ax.annotate(\"Data\", (18.25, 40.0), **data_kwargs) sol = solve_save_at(theta_true) ax = plot_solution(sol.t, sol.u[0], ax=ax, linewidth=4, alpha=0.5, **data_kwargs)  guess_kwargs = {\"color\": \"gray\"} ax.annotate(\"Initial guess\", (6.0, 12.0), **guess_kwargs) sol = solve_save_at(theta_guess) ax = plot_solution(     sol.t, sol.u[0], ax=ax, linestyle=\"dashed\", alpha=0.75, **guess_kwargs ) plt.show() <p>The samples cover a perhaps surpringly large range of potential initial conditions, but lead to the \"correct\" data.</p> <p>In parameter space, this is what it looks like:</p> In\u00a0[14]: Copied! <pre>plt.title(\"Posterior samples (parameter space)\")\nplt.plot(states.position[:, 0], states.position[:, 1], \"o\", alpha=0.5, markersize=4)\nplt.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8)\nplt.plot(theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8)\nplt.legend()\nplt.show()\n</pre> plt.title(\"Posterior samples (parameter space)\") plt.plot(states.position[:, 0], states.position[:, 1], \"o\", alpha=0.5, markersize=4) plt.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8) plt.plot(theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8) plt.legend() plt.show() <p>Let's add the value of $M$ to the plot to see whether the sampler covers the entire region of interest.</p> In\u00a0[15]: Copied! <pre>xlim = 14, jnp.amax(states.position[:, 0]) + 0.5\nylim = 14, jnp.amax(states.position[:, 1]) + 0.5\n\nxs = jnp.linspace(*xlim, endpoint=True, num=300)\nys = jnp.linspace(*ylim, endpoint=True, num=300)\nXs, Ys = jnp.meshgrid(xs, ys)\n\nThetas = jnp.stack((Xs, Ys))\nlog_M_vmapped_x = jax.vmap(log_M, in_axes=-1, out_axes=-1)\nlog_M_vmapped = jax.vmap(log_M_vmapped_x, in_axes=-1, out_axes=-1)\nZs = log_M_vmapped(Thetas)\n</pre> xlim = 14, jnp.amax(states.position[:, 0]) + 0.5 ylim = 14, jnp.amax(states.position[:, 1]) + 0.5  xs = jnp.linspace(*xlim, endpoint=True, num=300) ys = jnp.linspace(*ylim, endpoint=True, num=300) Xs, Ys = jnp.meshgrid(xs, ys)  Thetas = jnp.stack((Xs, Ys)) log_M_vmapped_x = jax.vmap(log_M, in_axes=-1, out_axes=-1) log_M_vmapped = jax.vmap(log_M_vmapped_x, in_axes=-1, out_axes=-1) Zs = log_M_vmapped(Thetas) In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 3))\n\nax_samples, ax_heatmap = ax\n\nfig.suptitle(\"Posterior samples (parameter space)\")\nax_samples.plot(\n    states.position[:, 0], states.position[:, 1], \".\", alpha=0.5, markersize=4\n)\nax_samples.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8)\nax_samples.plot(\n    theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8\n)\nax_samples.legend()\nim = ax_heatmap.contourf(Xs, Ys, jnp.exp(Zs), cmap=\"cividis\", alpha=0.8)\nplt.colorbar(im)\nplt.show()\n</pre> fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 3))  ax_samples, ax_heatmap = ax  fig.suptitle(\"Posterior samples (parameter space)\") ax_samples.plot(     states.position[:, 0], states.position[:, 1], \".\", alpha=0.5, markersize=4 ) ax_samples.plot(theta_true[0], theta_true[1], \"P\", label=\"Truth\", markersize=8) ax_samples.plot(     theta_guess[0], theta_guess[1], \"P\", label=\"Initial guess\", markersize=8 ) ax_samples.legend() im = ax_heatmap.contourf(Xs, Ys, jnp.exp(Zs), cmap=\"cividis\", alpha=0.8) plt.colorbar(im) plt.show() <p>Looks great!</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#parameter-estimation-blackjax","title":"Parameter estimation (BlackJAX)\u00b6","text":"<p>This tutorial explains how to estimate unknown parameters of initial value problems (IVPs) using Markov Chain Monte Carlo (MCMC) methods as provided by BlackJAX.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#tldr","title":"TL;DR\u00b6","text":"<p>Compute log-posterior of IVP parameters given observations of the IVP solution with ProbDiffEq. Sample from this posterior using BlackJAX. Evaluating the log-likelihood of the data is described in this paper. Based on this log-likelihood, sampling from the log-posterior is as done in this paper.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#technical-setup","title":"Technical setup\u00b6","text":"<p>Let $f$ be a known vector field. In this example, we use the Lotka-Volterra model. Consider an ordinary differential equation</p> <p>$$ \\dot y(t) = f(y(t)), \\quad 0 \\leq t \\leq T $$</p> <p>subject to an unknown initial condition $y(0) = \\theta$. Recall from the previous tutorials that the probabilistic IVP solution is an approximation of the posterior distribution</p> <p>$$ p\\left(y ~|~ [\\dot y(t_n) = f(y(t_n))]_{n=0}^N, y(0) = \\theta\\right) $$</p> <p>for a Gaussian prior over $y$ and a pre-determined or adaptively selected grid $t_0, ..., t_N$.</p> <p>We don't know the initial condition of the IVP, but assume that we have noisy observations of the IVP soution $y$ at the terminal time $T$ of the integration problem,</p> <p>$$ p(\\text{data}~|~ y(T)) = N(y(T), \\sigma^2 I) $$</p> <p>for some $\\sigma &gt; 0$. We can use these observations to reconstruct $\\theta$, for example by sampling from $p(\\text{data}~|~\\theta)$ (which is a function of $\\theta$).</p> <p>Now, one way of evaluating $p(\\text{data} ~|~ \\theta)$ is to use any numerical solver, for example a Runge-Kutta method, to approximate $y(T)$ from $\\theta$ and evaluate $N(y(T), \\sigma^2 I)$. But this ignores a few crucial concepts (e.g., the numerical error of the approximation; we refer to the references linked above). Instead, we can use a probabilistic solver instead of \"any\" numerical solver and build a more comprehensive model:</p> <p>We can combine probabilistic IVP solvers with MCMC methods to estimate $\\theta$ from $\\text{data}$ in a way that quantifies numerical approximation errors (and other model mismatches). To do so, we approximate the distribution of the IVP solution given the parameter $p(y(T) \\mid \\theta)$ and evaluate the marginal distribution of $N(y(T), \\sigma^2I)$ given the probabilistic IVP solution. More formally, we use ProbDiffEq to evaluate the density of the unnormalised posterior</p> <p>$$ M(\\theta) := p(\\theta \\mid \\text{data})\\propto p(\\text{data} \\mid \\theta)p(\\theta) $$</p> <p>where \"$\\propto$\" means \"proportional to\" and the likelihood stems from the IVP solution</p> <p>$$ p(\\text{data} \\mid \\theta) = \\int p(\\text{data} \\mid y(T)) p(y(T) \\mid [\\dot y(t_n) = f(y(t_n))]_{n=0}^N, y(0) = \\theta), \\theta) d y(T) $$ Loosely speaking, this distribution averages $N(y(T), \\sigma^2I)$ over all IVP solutions $y(T)$ that are realistic given the differential equation, grid $t_0, ..., t_N$, and prior distribution $p(y)$. This is useful, because if the approximation error is large, $M(\\theta)$ \"knows this\". If the approximation error is ridiculously small, $M(\\theta)$ \"knows this\" too and  we recover the procedure described for non-probabilistic solvers above. Interestingly, non-probabilistic solvers cannot do this averaging because they do not yield a statistical description of estimated IVP solutions. Non-probabilistic solvers would also fail if the observations were noise-free (i.e. $\\sigma = 0$), but the present example notebook remains stable. (Try it yourself!)</p> <p>To sample $\\theta$ according to $M$ (respectively $\\log M$), we evaluate $M(\\theta)$ with ProbDiffEq, compute its gradient with JAX, and use this gradient to sample $\\theta$ with BlackJAX:</p> <ol> <li><p>ProbDiffEq: Compute the probabilistic IVP solution by approximating $p(y(T) ~|~ [\\dot y(t_n) = f(y(t_n))]_n, y(0) = \\theta)$</p> </li> <li><p>ProbDiffEq: Evaluate  $M(\\theta)$ by marginalising over the IVP solution computed in step 1.</p> </li> <li><p>JAX: Compute the gradient $\\nabla_\\theta M(\\theta)$</p> </li> <li><p>BlackJAX: Sample from $\\log M(\\theta)$ using, for example, the No-U-Turn-Sampler (which requires $\\nabla_\\theta M(\\theta))$.</p> </li> </ol> <p>Here is how:</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#problem-setting","title":"Problem setting\u00b6","text":"<p>First, we set up an IVP and create some artificial data by simulating the system with \"incorrect\" initial conditions.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#log-posterior-densities-via-probdiffeq","title":"Log-posterior densities via ProbDiffEq\u00b6","text":"<p>Set up a log-posterior density function that we can plug into BlackJAX. Choose a Gaussian prior centered at the initial guess with a large variance.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#sampling-with-blackjax","title":"Sampling with BlackJAX\u00b6","text":"<p>From here on, BlackJAX takes over:</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#visualisation","title":"Visualisation\u00b6","text":"<p>Now that we have samples of $\\theta$, let's plot the corresponding solutions:</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#conclusion","title":"Conclusion\u00b6","text":"<p>In conclusion, a log-posterior density function can be provided by ProbDiffEq such that any of BlackJAX' samplers yield parameter estimates of IVPs.</p>"},{"location":"examples_advanced/parameter_estimation_blackjax/#whats-next","title":"What's next\u00b6","text":"<p>Try to get a feeling for how the sampler reacts to changing observation noises, solver parameters, and so on. We could extend the sampling problem from $\\theta \\mapsto \\log M(\\theta)$ to some $(\\theta, \\sigma) \\mapsto \\log \\tilde M(\\theta, \\sigma)$, i.e., treat the observation noise as unknown and run Hamiltonian Monte Carlo in a higher-dimensional parameter space. We could also add a more suitable prior distribution $p(\\theta)$ to regularise the problem.</p> <p>A final side note: We could also replace the sampler with an optimisation algorithm and use this procedure to solve boundary value problems (albeit this may not be very efficient; use this algorithm instead).</p>"},{"location":"examples_advanced/parameter_estimation_optax/","title":"Parameter estimation (Optax)","text":"In\u00a0[1]: Copied! <pre>\"\"\"Estimate ODE parameters with ProbDiffEq and Optax.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats\n</pre> \"\"\"Estimate ODE parameters with ProbDiffEq and Optax.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, stats In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax  <p>Create a problem and some fake-data:</p> In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.lotka_volterra()\nf_args = jnp.asarray(f_args)\n\n\n@jax.jit\ndef vf(y, t, *, p):  # noqa: ARG001\n    \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"\n    return f(y, *p)\n\n\ndef solve(p):\n    \"\"\"Evaluate the parameter-to-solution map.\"\"\"\n    tcoeffs = (u0, vf(u0, t0, p=p))\n    output_scale = 10.0\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n        tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"\n    )\n    ts0 = ivpsolvers.correction_ts0(lambda y, t: vf(y, t, p=p), ssm=ssm)\n    strategy = ivpsolvers.strategy_smoother(ssm=ssm)\n    solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)\n    return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)\n\n\nparameter_true = f_args + 0.05\nparameter_guess = f_args\n\n\nts = jnp.linspace(t0, t1, endpoint=True, num=100)\nsolution_true = solve(parameter_true)\ndata = solution_true.u[0]\nplt.plot(ts, data, \"P-\")\nplt.show()\n</pre> f, u0, (t0, t1), f_args = ivps.lotka_volterra() f_args = jnp.asarray(f_args)   @jax.jit def vf(y, t, *, p):  # noqa: ARG001     \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"     return f(y, *p)   def solve(p):     \"\"\"Evaluate the parameter-to-solution map.\"\"\"     tcoeffs = (u0, vf(u0, t0, p=p))     output_scale = 10.0     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(         tcoeffs, output_scale=output_scale, ssm_fact=\"isotropic\"     )     ts0 = ivpsolvers.correction_ts0(lambda y, t: vf(y, t, p=p), ssm=ssm)     strategy = ivpsolvers.strategy_smoother(ssm=ssm)     solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts0, ssm=ssm)     return ivpsolve.solve_fixed_grid(init, grid=ts, solver=solver, ssm=ssm)   parameter_true = f_args + 0.05 parameter_guess = f_args   ts = jnp.linspace(t0, t1, endpoint=True, num=100) solution_true = solve(parameter_true) data = solution_true.u[0] plt.plot(ts, data, \"P-\") plt.show() <p>We make an initial guess, but it does not lead to a good data fit:</p> In\u00a0[4]: Copied! <pre>solution_guess = solve(parameter_guess)\nplt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125)\nplt.plot(ts, solution_guess.u[0])\nplt.show()\n</pre> solution_guess = solve(parameter_guess) plt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125) plt.plot(ts, solution_guess.u[0]) plt.show() <p>Use the probdiffeq functionality to compute a parameter-to-data fit function.</p> <p>This incorporates the likelihood of the data under the distribution induced by the probabilistic ODE solution (which was generated with the current parameter guess).</p> In\u00a0[5]: Copied! <pre>@jax.jit\ndef parameter_to_data_fit(parameters_, /, standard_deviation=1e-1):\n    \"\"\"Evaluate the data fit as a function of the parameters.\"\"\"\n    sol_ = solve(parameters_)\n    return -1.0 * stats.log_marginal_likelihood(\n        data,\n        standard_deviation=jnp.ones_like(sol_.t) * standard_deviation,\n        posterior=sol_.posterior,\n        ssm=sol_.ssm,\n    )\n\n\nsensitivities = jax.jit(jax.grad(parameter_to_data_fit))\n</pre> @jax.jit def parameter_to_data_fit(parameters_, /, standard_deviation=1e-1):     \"\"\"Evaluate the data fit as a function of the parameters.\"\"\"     sol_ = solve(parameters_)     return -1.0 * stats.log_marginal_likelihood(         data,         standard_deviation=jnp.ones_like(sol_.t) * standard_deviation,         posterior=sol_.posterior,         ssm=sol_.ssm,     )   sensitivities = jax.jit(jax.grad(parameter_to_data_fit)) <p>We can differentiate the function forward- and reverse-mode (the latter is possible because we use fixed steps)</p> In\u00a0[6]: Copied! <pre>parameter_to_data_fit(parameter_guess)\nsensitivities(parameter_guess)\n</pre> parameter_to_data_fit(parameter_guess) sensitivities(parameter_guess) Out[6]: <pre>Array([44.875042, 68.57584 , 51.92206 , 24.460234], dtype=float32)</pre> <p>Now, enter optax: build an optimizer, and optimise the parameter-to-model-fit function. The following is more or less taken from the optax-documentation.</p> In\u00a0[7]: Copied! <pre>def build_update_fn(*, optimizer, loss_fn):\n    \"\"\"Build a function for executing a single step in the optimization.\"\"\"\n\n    @jax.jit\n    def update(params, opt_state):\n        \"\"\"Update the optimiser state.\"\"\"\n        _loss, grads = jax.value_and_grad(loss_fn)(params)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state\n\n    return update\n\n\noptim = optax.adam(learning_rate=1e-2)\nupdate_fn = build_update_fn(optimizer=optim, loss_fn=parameter_to_data_fit)\n</pre> def build_update_fn(*, optimizer, loss_fn):     \"\"\"Build a function for executing a single step in the optimization.\"\"\"      @jax.jit     def update(params, opt_state):         \"\"\"Update the optimiser state.\"\"\"         _loss, grads = jax.value_and_grad(loss_fn)(params)         updates, opt_state = optimizer.update(grads, opt_state)         params = optax.apply_updates(params, updates)         return params, opt_state      return update   optim = optax.adam(learning_rate=1e-2) update_fn = build_update_fn(optimizer=optim, loss_fn=parameter_to_data_fit) In\u00a0[8]: Copied! <pre>p = parameter_guess\nstate = optim.init(p)\n\nchunk_size = 10\nfor i in range(chunk_size):\n    for _ in range(chunk_size):\n        p, state = update_fn(p, state)\n\n    print(f\"After {(i + 1) * chunk_size} iterations:\", p)\n</pre> p = parameter_guess state = optim.init(p)  chunk_size = 10 for i in range(chunk_size):     for _ in range(chunk_size):         p, state = update_fn(p, state)      print(f\"After {(i + 1) * chunk_size} iterations:\", p) <pre>After 10 iterations: [0.42702383 0.04230705 0.42326728 0.05160698]\nAfter 20 iterations: [0.45761833 0.07951874 0.4569888  0.04045167]\nAfter 30 iterations: [0.47949782 0.07957107 0.47714967 0.05495605]\nAfter 40 iterations: [0.4985081  0.07780007 0.4943     0.07007392]\nAfter 50 iterations: [0.51403695 0.08094375 0.50932753 0.08073058]\nAfter 60 iterations: [0.5262151  0.08694342 0.52200544 0.08696883]\nAfter 70 iterations: [0.5352229  0.09245805 0.531671   0.090909  ]\nAfter 80 iterations: [0.54162693 0.09584862 0.538437   0.0939946 ]\nAfter 90 iterations: [0.545845   0.0975972  0.5427565  0.09657406]\nAfter 100 iterations: [0.5485102  0.09855746 0.5454584  0.09833122]\n</pre> <p>The solution looks much better:</p> In\u00a0[9]: Copied! <pre>solution_better = solve(p)\nplt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125)\nplt.plot(ts, solution_better.u[0])\nplt.show()\n</pre> solution_better = solve(p) plt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125) plt.plot(ts, solution_better.u[0]) plt.show()"},{"location":"examples_advanced/parameter_estimation_optax/#parameter-estimation-optax","title":"Parameter estimation (Optax)\u00b6","text":"<p>We create some data, compute the marginal likelihood of this data under the ODE posterior (which is something you cannot do with non-probabilistic solvers!), and optimize the parameters with <code>optax</code>.</p> <p>Link to paper: https://arxiv.org/abs/2202.01287</p>"},{"location":"examples_advanced/solve_pde/","title":"Solve a PDE","text":"In\u00a0[1]: Copied! <pre>\"\"\"Solve a PDE.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\njax.config.update(\"jax_enable_x64\", True)\n\n\ndef main():\n    \"\"\"Simulate a PDE.\"\"\"\n    key = jax.random.PRNGKey(1)\n    f, (u0,), (t0, t1) = fhn_2d(key, num=40, t1=10.0)\n\n    @jax.jit\n    def vf(y, *, t):  # noqa: ARG001\n        \"\"\"Evaluate the dynamics of the PDE.\"\"\"\n        return f(y)\n\n    print(\"Problem dimension:\", u0.size)\n\n    # Set up a state-space model\n    tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)\n    init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\")\n\n    # Build a solver\n    ts = ivpsolvers.correction_ts1(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\n    solver = ivpsolvers.solver_dynamic(\n        ssm=ssm, strategy=strategy, prior=ibm, correction=ts\n    )\n    adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n\n    # Solve the ODE\n    save_at = jnp.linspace(t0, t1, num=5, endpoint=True)\n    simulate = simulator(save_at=save_at, adaptive_solver=adaptive_solver, ssm=ssm)\n    (u, u_std) = simulate(init)\n\n    fig, axes = plt.subplots(\n        nrows=2, ncols=len(u), figsize=(2 * len(u), 3), tight_layout=True\n    )\n    for t_i, u_i, std_i, ax_i in zip(save_at, u, u_std, axes.T):\n        ax_i[0].set_title(f\"t = {t_i:.1f}\")\n        img = ax_i[0].imshow(u_i[0], cmap=\"copper\", vmin=-1, vmax=1)\n        plt.colorbar(img)\n\n        uncertainty = jnp.log10(jnp.abs(std_i[0]) + 1e-10)\n        img = ax_i[1].imshow(uncertainty, cmap=\"bone\", vmin=-7, vmax=-3)\n        plt.colorbar(img)\n\n        ax_i[0].set_xticks(())\n        ax_i[1].set_xticks(())\n        ax_i[0].set_yticks(())\n        ax_i[1].set_yticks(())\n\n    axes[0][0].set_ylabel(\"PDE solution\")\n    axes[1][0].set_ylabel(\"log(stdev)\")\n    plt.show()\n\n\ndef simulator(save_at, adaptive_solver, ssm):\n    \"\"\"Simulate a PDE.\"\"\"\n\n    @jax.jit\n    def solve(init):\n        solution = ivpsolve.solve_adaptive_save_at(\n            init, save_at=save_at, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n        )\n        return (solution.u[0], solution.u_std[0])\n\n    return solve\n\n\ndef fhn_2d(prng_key, *, num, t1, t0=0.0, a=2.8e-4, b=5e-3, k=-0.005, tau=1.0):\n    \"\"\"Construct the FitzHugh-Nagumo PDE.\n\n    Source: https://github.com/pnkraemer/tornadox/blob/main/tornadox/ivp.py\n\n    But simplified since Probdiffeq can handle matrix-valued ODEs.\n    Here, we also set tau = 1.0 to make the example quick to execute.\n    \"\"\"\n    y0 = jax.random.uniform(prng_key, shape=(2, num, num))\n\n    @jax.jit\n    def fhn_2d(x):\n        u, v = x\n        du = _laplace_2d(u, dx=1.0 / num)\n        dv = _laplace_2d(v, dx=1.0 / num)\n        u_new = a * du + u - u**3 - v + k\n        v_new = (b * dv + u - v) / tau\n        return jnp.stack((u_new, v_new))\n\n    return fhn_2d, (y0,), (t0, t1)\n\n\ndef _laplace_2d(grid, dx):\n    \"\"\"2D Laplace operator on a vectorized 2d grid.\"\"\"\n    # Set the boundary values to the nearest interior node\n    # This enforces Neumann conditions.\n    padded_grid = jnp.pad(grid, pad_width=1, mode=\"edge\")\n\n    # Laplacian via convolve2d()\n    kernel = jnp.array([[0.0, 1.0, 0.0], [1.0, -4.0, 1.0], [0.0, 1.0, 0.0]])\n    kernel /= dx**2\n    grid = jax.scipy.signal.convolve2d(padded_grid, kernel, mode=\"same\")\n    return grid[1:-1, 1:-1]\n\n\nif __name__ == \"__main__\":\n    main()\n</pre> \"\"\"Solve a PDE.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt  from probdiffeq import ivpsolve, ivpsolvers, taylor  jax.config.update(\"jax_enable_x64\", True)   def main():     \"\"\"Simulate a PDE.\"\"\"     key = jax.random.PRNGKey(1)     f, (u0,), (t0, t1) = fhn_2d(key, num=40, t1=10.0)      @jax.jit     def vf(y, *, t):  # noqa: ARG001         \"\"\"Evaluate the dynamics of the PDE.\"\"\"         return f(y)      print(\"Problem dimension:\", u0.size)      # Set up a state-space model     tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)     init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\")      # Build a solver     ts = ivpsolvers.correction_ts1(vf, ssm=ssm)     strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)     solver = ivpsolvers.solver_dynamic(         ssm=ssm, strategy=strategy, prior=ibm, correction=ts     )     adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)      # Solve the ODE     save_at = jnp.linspace(t0, t1, num=5, endpoint=True)     simulate = simulator(save_at=save_at, adaptive_solver=adaptive_solver, ssm=ssm)     (u, u_std) = simulate(init)      fig, axes = plt.subplots(         nrows=2, ncols=len(u), figsize=(2 * len(u), 3), tight_layout=True     )     for t_i, u_i, std_i, ax_i in zip(save_at, u, u_std, axes.T):         ax_i[0].set_title(f\"t = {t_i:.1f}\")         img = ax_i[0].imshow(u_i[0], cmap=\"copper\", vmin=-1, vmax=1)         plt.colorbar(img)          uncertainty = jnp.log10(jnp.abs(std_i[0]) + 1e-10)         img = ax_i[1].imshow(uncertainty, cmap=\"bone\", vmin=-7, vmax=-3)         plt.colorbar(img)          ax_i[0].set_xticks(())         ax_i[1].set_xticks(())         ax_i[0].set_yticks(())         ax_i[1].set_yticks(())      axes[0][0].set_ylabel(\"PDE solution\")     axes[1][0].set_ylabel(\"log(stdev)\")     plt.show()   def simulator(save_at, adaptive_solver, ssm):     \"\"\"Simulate a PDE.\"\"\"      @jax.jit     def solve(init):         solution = ivpsolve.solve_adaptive_save_at(             init, save_at=save_at, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm         )         return (solution.u[0], solution.u_std[0])      return solve   def fhn_2d(prng_key, *, num, t1, t0=0.0, a=2.8e-4, b=5e-3, k=-0.005, tau=1.0):     \"\"\"Construct the FitzHugh-Nagumo PDE.      Source: https://github.com/pnkraemer/tornadox/blob/main/tornadox/ivp.py      But simplified since Probdiffeq can handle matrix-valued ODEs.     Here, we also set tau = 1.0 to make the example quick to execute.     \"\"\"     y0 = jax.random.uniform(prng_key, shape=(2, num, num))      @jax.jit     def fhn_2d(x):         u, v = x         du = _laplace_2d(u, dx=1.0 / num)         dv = _laplace_2d(v, dx=1.0 / num)         u_new = a * du + u - u**3 - v + k         v_new = (b * dv + u - v) / tau         return jnp.stack((u_new, v_new))      return fhn_2d, (y0,), (t0, t1)   def _laplace_2d(grid, dx):     \"\"\"2D Laplace operator on a vectorized 2d grid.\"\"\"     # Set the boundary values to the nearest interior node     # This enforces Neumann conditions.     padded_grid = jnp.pad(grid, pad_width=1, mode=\"edge\")      # Laplacian via convolve2d()     kernel = jnp.array([[0.0, 1.0, 0.0], [1.0, -4.0, 1.0], [0.0, 1.0, 0.0]])     kernel /= dx**2     grid = jax.scipy.signal.convolve2d(padded_grid, kernel, mode=\"same\")     return grid[1:-1, 1:-1]   if __name__ == \"__main__\":     main() <pre>Problem dimension: 3200\n</pre>"},{"location":"examples_advanced/solve_pde/#solve-a-pde","title":"Solve a PDE\u00b6","text":"<p>This tutorial replicates Figure 1 from https://arxiv.org/abs/2110.11812, but uses some advanced features in Probdiffeq, namely, solving matrix-valued problems and adaptive simulation with fixedpoint smoothing.</p>"},{"location":"examples_basic/conditioning_on_zero_residual/","title":"How probabilistic solvers work","text":"In\u00a0[1]: Copied! <pre>\"\"\"Demonstrate how probabilistic solvers work via conditioning on constraints.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n</pre> \"\"\"Demonstrate how probabilistic solvers work via conditioning on constraints.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax In\u00a0[3]: Copied! <pre># Create an ODE problem\n\n\n@jax.jit\ndef vector_field(y, t):  # noqa: ARG001\n    \"\"\"Evaluate the logistic ODE vector field.\"\"\"\n    return 10.0 * y * (2.0 - y)\n\n\nt0, t1 = 0.0, 0.5\nu0 = jnp.asarray([0.1])\n</pre> # Create an ODE problem   @jax.jit def vector_field(y, t):  # noqa: ARG001     \"\"\"Evaluate the logistic ODE vector field.\"\"\"     return 10.0 * y * (2.0 - y)   t0, t1 = 0.0, 0.5 u0 = jnp.asarray([0.1]) In\u00a0[4]: Copied! <pre># Assemble the discretised prior (with and without the correct Taylor coefficients)\n\nNUM_DERIVATIVES = 2\ntcoeffs_like = [u0] * (NUM_DERIVATIVES + 1)\nts = jnp.linspace(t0, t1, num=500, endpoint=True)\ninit_raw, transitions, ssm = ivpsolvers.prior_wiener_integrated_discrete(\n    ts, tcoeffs_like, output_scale=100.0, ssm_fact=\"dense\"\n)\n\nmarkov_seq_prior = stats.MarkovSeq(init_raw, transitions)\n\n\ntcoeffs = taylor.odejet_padded_scan(\n    lambda y: vector_field(y, t=t0), (u0,), num=NUM_DERIVATIVES\n)\ninit_tcoeffs = ssm.normal.from_tcoeffs(tcoeffs)\nmarkov_seq_tcoeffs = stats.MarkovSeq(init_tcoeffs, transitions)\n</pre> # Assemble the discretised prior (with and without the correct Taylor coefficients)  NUM_DERIVATIVES = 2 tcoeffs_like = [u0] * (NUM_DERIVATIVES + 1) ts = jnp.linspace(t0, t1, num=500, endpoint=True) init_raw, transitions, ssm = ivpsolvers.prior_wiener_integrated_discrete(     ts, tcoeffs_like, output_scale=100.0, ssm_fact=\"dense\" )  markov_seq_prior = stats.MarkovSeq(init_raw, transitions)   tcoeffs = taylor.odejet_padded_scan(     lambda y: vector_field(y, t=t0), (u0,), num=NUM_DERIVATIVES ) init_tcoeffs = ssm.normal.from_tcoeffs(tcoeffs) markov_seq_tcoeffs = stats.MarkovSeq(init_tcoeffs, transitions) In\u00a0[5]: Copied! <pre># Compute the posterior\n\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"dense\"\n)\nts1 = ivpsolvers.correction_ts1(vector_field, ssm=ssm)\nstrategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\nsolver = ivpsolvers.solver(strategy, prior=ibm, correction=ts1, ssm=ssm)\nadaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-2, ssm=ssm)\n\ndt0 = ivpsolve.dt0(lambda y: vector_field(y, t=t0), (u0,))\nsol = ivpsolve.solve_adaptive_save_at(\n    init, save_at=ts, dt0=1.0, adaptive_solver=adaptive_solver, ssm=ssm\n)\nmarkov_seq_posterior = stats.markov_select_terminal(sol.posterior)\n</pre> # Compute the posterior  init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"dense\" ) ts1 = ivpsolvers.correction_ts1(vector_field, ssm=ssm) strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm) solver = ivpsolvers.solver(strategy, prior=ibm, correction=ts1, ssm=ssm) adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-2, ssm=ssm)  dt0 = ivpsolve.dt0(lambda y: vector_field(y, t=t0), (u0,)) sol = ivpsolve.solve_adaptive_save_at(     init, save_at=ts, dt0=1.0, adaptive_solver=adaptive_solver, ssm=ssm ) markov_seq_posterior = stats.markov_select_terminal(sol.posterior) In\u00a0[6]: Copied! <pre># Compute marginals\n\nmargs_prior = stats.markov_marginals(markov_seq_prior, reverse=False, ssm=ssm)\nmargs_tcoeffs = stats.markov_marginals(markov_seq_tcoeffs, reverse=False, ssm=ssm)\nmargs_posterior = stats.markov_marginals(markov_seq_posterior, reverse=True, ssm=ssm)\n</pre> # Compute marginals  margs_prior = stats.markov_marginals(markov_seq_prior, reverse=False, ssm=ssm) margs_tcoeffs = stats.markov_marginals(markov_seq_tcoeffs, reverse=False, ssm=ssm) margs_posterior = stats.markov_marginals(markov_seq_posterior, reverse=True, ssm=ssm) In\u00a0[7]: Copied! <pre># Compute samples\n\nnum_samples = 5\nkey = jax.random.PRNGKey(seed=1)\nsamples_prior, _ = stats.markov_sample(\n    key, markov_seq_prior, shape=(num_samples,), reverse=False, ssm=ssm\n)\nsamples_tcoeffs, _ = stats.markov_sample(\n    key, markov_seq_tcoeffs, shape=(num_samples,), reverse=False, ssm=ssm\n)\nsamples_posterior, _ = stats.markov_sample(\n    key, markov_seq_posterior, shape=(num_samples,), reverse=True, ssm=ssm\n)\n</pre> # Compute samples  num_samples = 5 key = jax.random.PRNGKey(seed=1) samples_prior, _ = stats.markov_sample(     key, markov_seq_prior, shape=(num_samples,), reverse=False, ssm=ssm ) samples_tcoeffs, _ = stats.markov_sample(     key, markov_seq_tcoeffs, shape=(num_samples,), reverse=False, ssm=ssm ) samples_posterior, _ = stats.markov_sample(     key, markov_seq_posterior, shape=(num_samples,), reverse=True, ssm=ssm ) In\u00a0[8]: Copied! <pre># Plot the results\n\nfig, (axes_state, axes_residual, axes_log_abs) = plt.subplots(\n    nrows=3, ncols=3, sharex=True, sharey=\"row\", constrained_layout=True, figsize=(8, 5)\n)\naxes_state[0].set_title(\"Prior\")\naxes_state[1].set_title(\"w/ Initial condition\")\naxes_state[2].set_title(\"Posterior\")\n\nsample_style = {\"marker\": \"None\", \"alpha\": 0.99, \"linewidth\": 0.75}\nmean_style = {\n    \"marker\": \"None\",\n    \"color\": \"black\",\n    \"linestyle\": \"dashed\",\n    \"linewidth\": 0.99,\n}\n\n\ndef residual(x, t):\n    \"\"\"Evaluate the ODE residual.\"\"\"\n    return x[1] - jax.vmap(jax.vmap(vector_field), in_axes=(0, None))(x[0], t)\n\n\nresidual_prior = residual(samples_prior, ts[:-1])\nresidual_tcoeffs = residual(samples_tcoeffs, ts[:-1])\nresidual_posterior = residual(samples_posterior, ts[:-1])\n\n\nfor i in range(num_samples):\n    # Plot all state-samples\n    axes_state[0].plot(ts[1:], samples_prior[0][i, ..., 0], **sample_style, color=\"C0\")\n    axes_state[1].plot(\n        ts[1:], samples_tcoeffs[0][i, ..., 0], **sample_style, color=\"C1\"\n    )\n    axes_state[2].plot(\n        ts[:-1], samples_posterior[0][i, ..., 0], **sample_style, color=\"C2\"\n    )\n\n    # Plot all residual-samples\n    axes_residual[0].plot(ts[:-1], residual_prior[i, ...], **sample_style, color=\"C0\")\n    axes_residual[1].plot(ts[:-1], residual_tcoeffs[i, ...], **sample_style, color=\"C1\")\n    axes_residual[2].plot(\n        ts[:-1], residual_posterior[i, ...], **sample_style, color=\"C2\"\n    )\n\n    # Plot all log-residual samples\n    axes_log_abs[0].plot(\n        ts[:-1], jnp.log10(jnp.abs(residual_prior))[i, ...], **sample_style, color=\"C0\"\n    )\n    axes_log_abs[1].plot(\n        ts[:-1],\n        jnp.log10(jnp.abs(residual_tcoeffs))[i, ...],\n        **sample_style,\n        color=\"C1\",\n    )\n    axes_log_abs[2].plot(\n        ts[:-1],\n        jnp.log10(jnp.abs(residual_posterior))[i, ...],\n        **sample_style,\n        color=\"C2\",\n    )\n#\n\n\ndef residual_mean(x, t):\n    \"\"\"Evaluate the ODE residual.\"\"\"\n    return x[1] - jax.vmap(vector_field)(x[0], t)\n\n\n# # Plot state means\naxes_state[0].plot(ts[1:], ssm.stats.qoi(margs_prior)[0], **mean_style)\naxes_state[1].plot(ts[1:], ssm.stats.qoi(margs_tcoeffs)[0], **mean_style)\naxes_state[2].plot(ts[:-1], ssm.stats.qoi(margs_posterior)[0], **mean_style)\n\n# # Plot residual means\naxes_residual[0].plot(\n    ts[:-1], residual_mean(ssm.stats.qoi(margs_prior), ts[:-1]), **mean_style\n)\naxes_residual[1].plot(\n    ts[:-1], residual_mean(ssm.stats.qoi(margs_tcoeffs), ts[:-1]), **mean_style\n)\naxes_residual[2].plot(\n    ts[:-1], residual_mean(ssm.stats.qoi(margs_posterior), ts[:-1]), **mean_style\n)\n\n\n# Set the x- and y-ticks/limits\naxes_state[0].set_xticks((t0, (t0 + t1) / 2, t1))\naxes_state[0].set_xlim((t0, t1))\n\naxes_state[0].set_ylim((-1, 3))\naxes_state[0].set_yticks((-1, 1, 3))\n\naxes_residual[0].set_ylim((-10.0, 20))\naxes_residual[0].set_yticks((-10.0, 5, 20))\n\naxes_log_abs[0].set_ylim((-6, 4))\naxes_log_abs[0].set_yticks((-6, -1, 4))\n\n# Label the x- and y-axes\naxes_state[0].set_ylabel(\"Solution\")\naxes_residual[0].set_ylabel(\"Residual\")\naxes_log_abs[0].set_ylabel(r\"Log-residual\")\naxes_log_abs[0].set_xlabel(\"Time $t$\")\naxes_log_abs[1].set_xlabel(\"Time $t$\")\naxes_log_abs[2].set_xlabel(\"Time $t$\")\n\n# Show the result\nfig.align_ylabels()\nplt.show()\n</pre> # Plot the results  fig, (axes_state, axes_residual, axes_log_abs) = plt.subplots(     nrows=3, ncols=3, sharex=True, sharey=\"row\", constrained_layout=True, figsize=(8, 5) ) axes_state[0].set_title(\"Prior\") axes_state[1].set_title(\"w/ Initial condition\") axes_state[2].set_title(\"Posterior\")  sample_style = {\"marker\": \"None\", \"alpha\": 0.99, \"linewidth\": 0.75} mean_style = {     \"marker\": \"None\",     \"color\": \"black\",     \"linestyle\": \"dashed\",     \"linewidth\": 0.99, }   def residual(x, t):     \"\"\"Evaluate the ODE residual.\"\"\"     return x[1] - jax.vmap(jax.vmap(vector_field), in_axes=(0, None))(x[0], t)   residual_prior = residual(samples_prior, ts[:-1]) residual_tcoeffs = residual(samples_tcoeffs, ts[:-1]) residual_posterior = residual(samples_posterior, ts[:-1])   for i in range(num_samples):     # Plot all state-samples     axes_state[0].plot(ts[1:], samples_prior[0][i, ..., 0], **sample_style, color=\"C0\")     axes_state[1].plot(         ts[1:], samples_tcoeffs[0][i, ..., 0], **sample_style, color=\"C1\"     )     axes_state[2].plot(         ts[:-1], samples_posterior[0][i, ..., 0], **sample_style, color=\"C2\"     )      # Plot all residual-samples     axes_residual[0].plot(ts[:-1], residual_prior[i, ...], **sample_style, color=\"C0\")     axes_residual[1].plot(ts[:-1], residual_tcoeffs[i, ...], **sample_style, color=\"C1\")     axes_residual[2].plot(         ts[:-1], residual_posterior[i, ...], **sample_style, color=\"C2\"     )      # Plot all log-residual samples     axes_log_abs[0].plot(         ts[:-1], jnp.log10(jnp.abs(residual_prior))[i, ...], **sample_style, color=\"C0\"     )     axes_log_abs[1].plot(         ts[:-1],         jnp.log10(jnp.abs(residual_tcoeffs))[i, ...],         **sample_style,         color=\"C1\",     )     axes_log_abs[2].plot(         ts[:-1],         jnp.log10(jnp.abs(residual_posterior))[i, ...],         **sample_style,         color=\"C2\",     ) #   def residual_mean(x, t):     \"\"\"Evaluate the ODE residual.\"\"\"     return x[1] - jax.vmap(vector_field)(x[0], t)   # # Plot state means axes_state[0].plot(ts[1:], ssm.stats.qoi(margs_prior)[0], **mean_style) axes_state[1].plot(ts[1:], ssm.stats.qoi(margs_tcoeffs)[0], **mean_style) axes_state[2].plot(ts[:-1], ssm.stats.qoi(margs_posterior)[0], **mean_style)  # # Plot residual means axes_residual[0].plot(     ts[:-1], residual_mean(ssm.stats.qoi(margs_prior), ts[:-1]), **mean_style ) axes_residual[1].plot(     ts[:-1], residual_mean(ssm.stats.qoi(margs_tcoeffs), ts[:-1]), **mean_style ) axes_residual[2].plot(     ts[:-1], residual_mean(ssm.stats.qoi(margs_posterior), ts[:-1]), **mean_style )   # Set the x- and y-ticks/limits axes_state[0].set_xticks((t0, (t0 + t1) / 2, t1)) axes_state[0].set_xlim((t0, t1))  axes_state[0].set_ylim((-1, 3)) axes_state[0].set_yticks((-1, 1, 3))  axes_residual[0].set_ylim((-10.0, 20)) axes_residual[0].set_yticks((-10.0, 5, 20))  axes_log_abs[0].set_ylim((-6, 4)) axes_log_abs[0].set_yticks((-6, -1, 4))  # Label the x- and y-axes axes_state[0].set_ylabel(\"Solution\") axes_residual[0].set_ylabel(\"Residual\") axes_log_abs[0].set_ylabel(r\"Log-residual\") axes_log_abs[0].set_xlabel(\"Time $t$\") axes_log_abs[1].set_xlabel(\"Time $t$\") axes_log_abs[2].set_xlabel(\"Time $t$\")  # Show the result fig.align_ylabels() plt.show()"},{"location":"examples_basic/conditioning_on_zero_residual/#how-probabilistic-solvers-work","title":"How probabilistic solvers work\u00b6","text":"<p>Probabilistic solvers condition a prior distribution on satisfying a zero-ODE-residual on a specified grid.</p>"},{"location":"examples_basic/dynamic_output_scales/","title":"Solver types","text":"In\u00a0[1]: Copied! <pre>\"\"\"Display the behaviour of the solvers when the scale of the ODE varies.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers\n</pre> \"\"\"Display the behaviour of the solvers when the scale of the ODE varies.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.affine_independent(initial_values=(1.0,), a=2.0)\n\n\n@jax.jit\ndef vf(*ys, t):  # noqa: ARG001\n    \"\"\"Evaluate the affine vector field.\"\"\"\n    return f(*ys, *f_args)\n</pre> f, u0, (t0, t1), f_args = ivps.affine_independent(initial_values=(1.0,), a=2.0)   @jax.jit def vf(*ys, t):  # noqa: ARG001     \"\"\"Evaluate the affine vector field.\"\"\"     return f(*ys, *f_args) In\u00a0[4]: Copied! <pre>num_derivatives = 1\n\ntcoeffs = (u0, vf(u0, t=t0))\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"dense\"\n)\nts1 = ivpsolvers.correction_ts1(vf, ssm=ssm)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\ndynamic = ivpsolvers.solver_dynamic(strategy, prior=ibm, correction=ts1, ssm=ssm)\nmle = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts1, ssm=ssm)\n</pre> num_derivatives = 1  tcoeffs = (u0, vf(u0, t=t0)) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"dense\" ) ts1 = ivpsolvers.correction_ts1(vf, ssm=ssm) strategy = ivpsolvers.strategy_filter(ssm=ssm) dynamic = ivpsolvers.solver_dynamic(strategy, prior=ibm, correction=ts1, ssm=ssm) mle = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts1, ssm=ssm) In\u00a0[5]: Copied! <pre>t0, t1 = 0.0, 3.0\nnum_pts = 200\n\nts = jnp.linspace(t0, t1, num=num_pts, endpoint=True)\n\n\nsolution_dynamic = ivpsolve.solve_fixed_grid(init, grid=ts, solver=dynamic, ssm=ssm)\nsolution_mle = ivpsolve.solve_fixed_grid(init, grid=ts, solver=mle, ssm=ssm)\n</pre> t0, t1 = 0.0, 3.0 num_pts = 200  ts = jnp.linspace(t0, t1, num=num_pts, endpoint=True)   solution_dynamic = ivpsolve.solve_fixed_grid(init, grid=ts, solver=dynamic, ssm=ssm) solution_mle = ivpsolve.solve_fixed_grid(init, grid=ts, solver=mle, ssm=ssm) <p>Plot the solution.</p> In\u00a0[6]: Copied! <pre>fig, (axes_linear, axes_log) = plt.subplots(ncols=2, nrows=2, sharex=True, sharey=\"row\")\n\n\nu_dynamic = solution_dynamic.u[0]\nu_mle = solution_mle.u[0]\nscale_dynamic = solution_dynamic.output_scale\nscale_mle = jnp.ones_like(solution_mle.output_scale) * solution_mle.output_scale[-1]\n\nstyle_target = {\n    \"marker\": \"None\",\n    \"label\": \"Target\",\n    \"color\": \"black\",\n    \"linewidth\": 0.5,\n    \"alpha\": 1,\n    \"linestyle\": \"dashed\",\n}\nstyle_approx = {\n    \"marker\": \"None\",\n    \"label\": \"Posterior mean\",\n    \"color\": \"C0\",\n    \"linewidth\": 1.5,\n    \"alpha\": 0.75,\n}\nstyle_scale = {\n    \"marker\": \"None\",\n    \"color\": \"C3\",\n    \"linestyle\": \"solid\",\n    \"label\": \"Output scale\",\n    \"linewidth\": 1.5,\n    \"alpha\": 0.75,\n}\n\naxes_linear[0].set_title(\"Time-varying model\")\naxes_linear[0].plot(ts, jnp.exp(ts * 2), **style_target)\naxes_linear[0].plot(ts, u_dynamic, **style_approx)\naxes_linear[0].plot(ts[1:], scale_dynamic, **style_scale)\naxes_linear[0].legend()\n\naxes_linear[1].set_title(\"Constant model\")\naxes_linear[1].plot(ts, jnp.exp(ts * 2), **style_target)\naxes_linear[1].plot(ts, u_mle, **style_approx)\naxes_linear[1].plot(ts[1:], scale_mle, **style_scale)\naxes_linear[1].legend()\n\naxes_linear[0].set_ylabel(\"Linear scale\")\n\naxes_linear[0].set_xlim((t0, t1))\n\n\naxes_log[0].semilogy(ts, jnp.exp(ts * 2), **style_target)\naxes_log[0].semilogy(ts, u_dynamic, **style_approx)\naxes_log[0].semilogy(ts[1:], scale_dynamic, **style_scale)\naxes_log[0].legend()\n\naxes_log[1].semilogy(ts, jnp.exp(ts * 2), **style_target)\naxes_log[1].semilogy(ts, u_mle, **style_approx)\naxes_log[1].semilogy(ts[1:], scale_mle, **style_scale)\naxes_log[1].legend()\n\naxes_log[0].set_ylabel(\"Logarithmic scale\")\naxes_log[0].set_xlabel(\"Time t\")\naxes_log[1].set_xlabel(\"Time t\")\n\naxes_log[0].set_xlim((t0, t1))\n\nfig.align_ylabels()\nplt.show()\n</pre> fig, (axes_linear, axes_log) = plt.subplots(ncols=2, nrows=2, sharex=True, sharey=\"row\")   u_dynamic = solution_dynamic.u[0] u_mle = solution_mle.u[0] scale_dynamic = solution_dynamic.output_scale scale_mle = jnp.ones_like(solution_mle.output_scale) * solution_mle.output_scale[-1]  style_target = {     \"marker\": \"None\",     \"label\": \"Target\",     \"color\": \"black\",     \"linewidth\": 0.5,     \"alpha\": 1,     \"linestyle\": \"dashed\", } style_approx = {     \"marker\": \"None\",     \"label\": \"Posterior mean\",     \"color\": \"C0\",     \"linewidth\": 1.5,     \"alpha\": 0.75, } style_scale = {     \"marker\": \"None\",     \"color\": \"C3\",     \"linestyle\": \"solid\",     \"label\": \"Output scale\",     \"linewidth\": 1.5,     \"alpha\": 0.75, }  axes_linear[0].set_title(\"Time-varying model\") axes_linear[0].plot(ts, jnp.exp(ts * 2), **style_target) axes_linear[0].plot(ts, u_dynamic, **style_approx) axes_linear[0].plot(ts[1:], scale_dynamic, **style_scale) axes_linear[0].legend()  axes_linear[1].set_title(\"Constant model\") axes_linear[1].plot(ts, jnp.exp(ts * 2), **style_target) axes_linear[1].plot(ts, u_mle, **style_approx) axes_linear[1].plot(ts[1:], scale_mle, **style_scale) axes_linear[1].legend()  axes_linear[0].set_ylabel(\"Linear scale\")  axes_linear[0].set_xlim((t0, t1))   axes_log[0].semilogy(ts, jnp.exp(ts * 2), **style_target) axes_log[0].semilogy(ts, u_dynamic, **style_approx) axes_log[0].semilogy(ts[1:], scale_dynamic, **style_scale) axes_log[0].legend()  axes_log[1].semilogy(ts, jnp.exp(ts * 2), **style_target) axes_log[1].semilogy(ts, u_mle, **style_approx) axes_log[1].semilogy(ts[1:], scale_mle, **style_scale) axes_log[1].legend()  axes_log[0].set_ylabel(\"Logarithmic scale\") axes_log[0].set_xlabel(\"Time t\") axes_log[1].set_xlabel(\"Time t\")  axes_log[0].set_xlim((t0, t1))  fig.align_ylabels() plt.show() <p>The dynamic solver adapts the output-scale so that both the solution and the output-scale grow exponentially. The ODE-solution fits the truth well.</p> <p>The solver_mle does not have this tool, and the ODE solution is not able to follow the exponential: it drifts back to the origin. (This is expected, we are basically trying to fit an exponential with a piecewise polynomial.)</p>"},{"location":"examples_basic/dynamic_output_scales/#solver-types","title":"Solver types\u00b6","text":"<p>You can choose between a <code>adaptive.solver_calibrationfree()</code> (which does not calibrate the output-scale), a <code>adaptive.solver_mle()</code> (which calibrates a global output scale via quasi-maximum-likelihood-estimation), and a <code>adaptive.solver_dynamic()</code>, which calibrates a time-varying, piecewise constant output-scale via \"local' quasi-maximum-likelihood estimation, similar to how ODE solver estimate local errors.</p> <p>But are these good for? In short: choose a <code>solver_dynamic</code> if your ODE output-scale varies quite strongly, and choose an <code>solver_mle</code> otherwise.</p> <p>For example, consider the numerical solution of a linear ODE with fixed steps:</p>"},{"location":"examples_basic/posterior_uncertainties/","title":"Posterior uncertainties","text":"In\u00a0[1]: Copied! <pre>\"\"\"Display the marginal uncertainties of filters and smoothers.\"\"\"\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n\n# Set up the ODE\n\n\ndef vf(y, *, t):  # noqa: ARG001\n    \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"\n    y0, y1 = y[0], y[1]\n\n    y0_new = 0.5 * y0 - 0.05 * y0 * y1\n    y1_new = -0.5 * y1 + 0.05 * y0 * y1\n    return jnp.asarray([y0_new, y1_new])\n\n\nt0 = 0.0\nt1 = 2.0\nu0 = jnp.asarray([20.0, 20.0])\n\n\n# Set up a solver\n# To all users: Try replacing the fixedpoint-smoother with a filter!\ntcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=3)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\")\nts = ivpsolvers.correction_ts1(vf, ssm=ssm)\nstrategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\nsolver = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts, ssm=ssm)\nadaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-1, ssm=ssm)\n\n# Solve the ODE\nts = jnp.linspace(t0, t1, endpoint=True, num=50)\nsol = ivpsolve.solve_adaptive_save_at(\n    init, save_at=ts, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n)\n\n# Calibrate\nmarginals = stats.calibrate(sol.marginals, output_scale=sol.output_scale, ssm=ssm)\nstd = ssm.stats.standard_deviation(marginals)\nu_std = ssm.stats.qoi_from_sample(std)\n\n# Plot the solution\nfig, axes = plt.subplots(\n    nrows=3,\n    ncols=len(tcoeffs),\n    sharex=\"col\",\n    tight_layout=True,\n    figsize=(len(u_std) * 2, 5),\n)\nfor i, (u_i, std_i, ax_i) in enumerate(zip(sol.u, u_std, axes.T)):\n    # Set up titles and axis descriptions\n    if i == 0:\n        ax_i[0].set_title(\"State\")\n        ax_i[0].set_ylabel(\"Prey\")\n        ax_i[1].set_ylabel(\"Predators\")\n        ax_i[2].set_ylabel(\"Std.-dev.\")\n    elif i == 1:\n        ax_i[0].set_title(f\"{i}st deriv.\")\n    elif i == 2:\n        ax_i[0].set_title(f\"{i}nd deriv.\")\n    elif i == 3:\n        ax_i[0].set_title(f\"{i}rd deriv.\")\n    else:\n        ax_i[0].set_title(f\"{i}th deriv.\")\n\n    ax_i[-1].set_xlabel(\"Time\")\n\n    for m, std, ax in zip(u_i.T, std_i.T, ax_i):\n        # Plot the mean\n        ax.plot(sol.t, m)\n\n        # Plot the standard deviation\n        lower, upper = m - 1.96 * std, m + 1.96 * std\n        ax.fill_between(sol.t, lower, upper, alpha=0.3)\n        ax.set_xlim((jnp.amin(ts), jnp.amax(ts)))\n\n    ax_i[2].semilogy(sol.t, std_i[:, 0], label=\"Prey\")\n    ax_i[2].semilogy(sol.t, std_i[:, 1], label=\"Predators\")\n    ax_i[2].legend(fontsize=\"x-small\")\n\nfig.align_ylabels()\nplt.show()\n</pre> \"\"\"Display the marginal uncertainties of filters and smoothers.\"\"\"  import jax.numpy as jnp import matplotlib.pyplot as plt  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor  # Set up the ODE   def vf(y, *, t):  # noqa: ARG001     \"\"\"Evaluate the Lotka-Volterra vector field.\"\"\"     y0, y1 = y[0], y[1]      y0_new = 0.5 * y0 - 0.05 * y0 * y1     y1_new = -0.5 * y1 + 0.05 * y0 * y1     return jnp.asarray([y0_new, y1_new])   t0 = 0.0 t1 = 2.0 u0 = jnp.asarray([20.0, 20.0])   # Set up a solver # To all users: Try replacing the fixedpoint-smoother with a filter! tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=3) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"blockdiag\") ts = ivpsolvers.correction_ts1(vf, ssm=ssm) strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm) solver = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts, ssm=ssm) adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-1, rtol=1e-1, ssm=ssm)  # Solve the ODE ts = jnp.linspace(t0, t1, endpoint=True, num=50) sol = ivpsolve.solve_adaptive_save_at(     init, save_at=ts, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm )  # Calibrate marginals = stats.calibrate(sol.marginals, output_scale=sol.output_scale, ssm=ssm) std = ssm.stats.standard_deviation(marginals) u_std = ssm.stats.qoi_from_sample(std)  # Plot the solution fig, axes = plt.subplots(     nrows=3,     ncols=len(tcoeffs),     sharex=\"col\",     tight_layout=True,     figsize=(len(u_std) * 2, 5), ) for i, (u_i, std_i, ax_i) in enumerate(zip(sol.u, u_std, axes.T)):     # Set up titles and axis descriptions     if i == 0:         ax_i[0].set_title(\"State\")         ax_i[0].set_ylabel(\"Prey\")         ax_i[1].set_ylabel(\"Predators\")         ax_i[2].set_ylabel(\"Std.-dev.\")     elif i == 1:         ax_i[0].set_title(f\"{i}st deriv.\")     elif i == 2:         ax_i[0].set_title(f\"{i}nd deriv.\")     elif i == 3:         ax_i[0].set_title(f\"{i}rd deriv.\")     else:         ax_i[0].set_title(f\"{i}th deriv.\")      ax_i[-1].set_xlabel(\"Time\")      for m, std, ax in zip(u_i.T, std_i.T, ax_i):         # Plot the mean         ax.plot(sol.t, m)          # Plot the standard deviation         lower, upper = m - 1.96 * std, m + 1.96 * std         ax.fill_between(sol.t, lower, upper, alpha=0.3)         ax.set_xlim((jnp.amin(ts), jnp.amax(ts)))      ax_i[2].semilogy(sol.t, std_i[:, 0], label=\"Prey\")     ax_i[2].semilogy(sol.t, std_i[:, 1], label=\"Predators\")     ax_i[2].legend(fontsize=\"x-small\")  fig.align_ylabels() plt.show()"},{"location":"examples_basic/posterior_uncertainties/#posterior-uncertainties","title":"Posterior uncertainties\u00b6","text":""},{"location":"examples_basic/second_order_problems/","title":"Second-order systems","text":"In\u00a0[1]: Copied! <pre>\"\"\"Demonstrate how to solve second-order IVPs without transforming them first.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n</pre> \"\"\"Demonstrate how to solve second-order IVPs without transforming them first.\"\"\"  import jax import jax.numpy as jnp import matplotlib.pyplot as plt from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, taylor In\u00a0[2]: Copied! <pre>if not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax  <p>Quick refresher: first-order ODEs</p> In\u00a0[3]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.three_body_restricted_first_order()\n\n\n@jax.jit\ndef vf_1(y, t):  # noqa: ARG001\n    \"\"\"Evaluate the three-body problem as a first-order IVP.\"\"\"\n    return f(y, *f_args)\n\n\ntcoeffs = taylor.odejet_padded_scan(lambda y: vf_1(y, t=t0), (u0,), num=4)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\"\n)\nts0 = ivpsolvers.correction_ts0(vf_1, ssm=ssm)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\nsolver_1st = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm)\nadaptive_solver_1st = ivpsolvers.adaptive(solver_1st, atol=1e-5, rtol=1e-5, ssm=ssm)\n</pre> f, u0, (t0, t1), f_args = ivps.three_body_restricted_first_order()   @jax.jit def vf_1(y, t):  # noqa: ARG001     \"\"\"Evaluate the three-body problem as a first-order IVP.\"\"\"     return f(y, *f_args)   tcoeffs = taylor.odejet_padded_scan(lambda y: vf_1(y, t=t0), (u0,), num=4) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\" ) ts0 = ivpsolvers.correction_ts0(vf_1, ssm=ssm) strategy = ivpsolvers.strategy_filter(ssm=ssm) solver_1st = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm) adaptive_solver_1st = ivpsolvers.adaptive(solver_1st, atol=1e-5, rtol=1e-5, ssm=ssm) In\u00a0[4]: Copied! <pre>solution = ivpsolve.solve_adaptive_save_every_step(\n    init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_1st, ssm=ssm\n)\n</pre> solution = ivpsolve.solve_adaptive_save_every_step(     init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_1st, ssm=ssm ) In\u00a0[5]: Copied! <pre>norm = jnp.linalg.norm((solution.u[0][-1] - u0) / jnp.abs(1.0 + u0))\nplt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\")\nplt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\")\nplt.show()\n</pre> norm = jnp.linalg.norm((solution.u[0][-1] - u0) / jnp.abs(1.0 + u0)) plt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\") plt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\") plt.show() <p>The default configuration assumes that the ODE to be solved is of first order. Now, the same game with a second-order ODE</p> In\u00a0[6]: Copied! <pre>f, (u0, du0), (t0, t1), f_args = ivps.three_body_restricted()\n\n\n@jax.jit\ndef vf_2(y, dy, t):  # noqa: ARG001\n    \"\"\"Evaluate the three-body problem as a second-order IVP.\"\"\"\n    return f(y, dy, *f_args)\n\n\n# One derivative more than above because we don't transform to first order\ntcoeffs = taylor.odejet_padded_scan(lambda *ys: vf_2(*ys, t=t0), (u0, du0), num=3)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(\n    tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\"\n)\nts0 = ivpsolvers.correction_ts0(vf_2, ode_order=2, ssm=ssm)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\nsolver_2nd = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm)\nadaptive_solver_2nd = ivpsolvers.adaptive(solver_2nd, atol=1e-5, rtol=1e-5, ssm=ssm)\n</pre> f, (u0, du0), (t0, t1), f_args = ivps.three_body_restricted()   @jax.jit def vf_2(y, dy, t):  # noqa: ARG001     \"\"\"Evaluate the three-body problem as a second-order IVP.\"\"\"     return f(y, dy, *f_args)   # One derivative more than above because we don't transform to first order tcoeffs = taylor.odejet_padded_scan(lambda *ys: vf_2(*ys, t=t0), (u0, du0), num=3) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(     tcoeffs, output_scale=1.0, ssm_fact=\"isotropic\" ) ts0 = ivpsolvers.correction_ts0(vf_2, ode_order=2, ssm=ssm) strategy = ivpsolvers.strategy_filter(ssm=ssm) solver_2nd = ivpsolvers.solver_mle(strategy, prior=ibm, correction=ts0, ssm=ssm) adaptive_solver_2nd = ivpsolvers.adaptive(solver_2nd, atol=1e-5, rtol=1e-5, ssm=ssm)  In\u00a0[7]: Copied! <pre>solution = ivpsolve.solve_adaptive_save_every_step(\n    init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_2nd, ssm=ssm\n)\n</pre> solution = ivpsolve.solve_adaptive_save_every_step(     init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver_2nd, ssm=ssm ) In\u00a0[8]: Copied! <pre>norm = jnp.linalg.norm((solution.u[0][-1, ...] - u0) / jnp.abs(1.0 + u0))\nplt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\")\nplt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\")\nplt.show()\n</pre> norm = jnp.linalg.norm((solution.u[0][-1, ...] - u0) / jnp.abs(1.0 + u0)) plt.title(f\"shape={solution.u[0].shape}, error={norm:.3f}\") plt.plot(solution.u[0][:, 0], solution.u[0][:, 1], marker=\".\") plt.show() <p>The results are indistinguishable from the plot. While the runtimes of both solvers are similar, the error of the second-order solver is much lower.</p> <p>See the benchmarks for more quantitative versions of this statement.</p>"},{"location":"examples_basic/second_order_problems/#second-order-systems","title":"Second-order systems\u00b6","text":""},{"location":"examples_basic/taylor_coefficients/","title":"Taylor coefficients","text":"In\u00a0[1]: Copied! <pre>\"\"\"Demonstrate how central Taylor coefficient estimation is to Probdiffeq.\"\"\"\n\nimport collections\n\nimport jax\nimport jax.numpy as jnp\nfrom diffeqzoo import backend, ivps\n\nfrom probdiffeq import ivpsolve, ivpsolvers, stats, taylor\n\nif not backend.has_been_selected:\n    backend.select(\"jax\")  # ivp examples in jax\n</pre> \"\"\"Demonstrate how central Taylor coefficient estimation is to Probdiffeq.\"\"\"  import collections  import jax import jax.numpy as jnp from diffeqzoo import backend, ivps  from probdiffeq import ivpsolve, ivpsolvers, stats, taylor  if not backend.has_been_selected:     backend.select(\"jax\")  # ivp examples in jax  <p>We start by defining an ODE.</p> In\u00a0[2]: Copied! <pre>f, u0, (t0, t1), f_args = ivps.logistic()\n\n\ndef vf(*y, t):  # noqa: ARG001\n    \"\"\"Evaluate the vector field.\"\"\"\n    return f(*y, *f_args)\n</pre> f, u0, (t0, t1), f_args = ivps.logistic()   def vf(*y, t):  # noqa: ARG001     \"\"\"Evaluate the vector field.\"\"\"     return f(*y, *f_args) <p>Here is a wrapper arounds Probdiffeq's solution routine.</p> In\u00a0[3]: Copied! <pre>def solve(tc):\n    \"\"\"Solve the ODE.\"\"\"\n    init, prior, ssm = ivpsolvers.prior_wiener_integrated(tc, ssm_fact=\"dense\")\n    ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)\n    strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)\n    solver = ivpsolvers.solver_mle(strategy, prior=prior, correction=ts0, ssm=ssm)\n    ts = jnp.linspace(t0, t1, endpoint=True, num=10)\n    adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-2, rtol=1e-2, ssm=ssm)\n    return ivpsolve.solve_adaptive_save_at(\n        init, save_at=ts, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm\n    )\n</pre> def solve(tc):     \"\"\"Solve the ODE.\"\"\"     init, prior, ssm = ivpsolvers.prior_wiener_integrated(tc, ssm_fact=\"dense\")     ts0 = ivpsolvers.correction_ts0(vf, ssm=ssm)     strategy = ivpsolvers.strategy_fixedpoint(ssm=ssm)     solver = ivpsolvers.solver_mle(strategy, prior=prior, correction=ts0, ssm=ssm)     ts = jnp.linspace(t0, t1, endpoint=True, num=10)     adaptive_solver = ivpsolvers.adaptive(solver, atol=1e-2, rtol=1e-2, ssm=ssm)     return ivpsolve.solve_adaptive_save_at(         init, save_at=ts, adaptive_solver=adaptive_solver, dt0=0.1, ssm=ssm     ) <p>It's time to solve some ODEs:</p> In\u00a0[4]: Copied! <pre>tcoeffs = taylor.odejet_padded_scan(lambda *y: vf(*y, t=t0), [u0], num=2)\nsolution = solve(tcoeffs)\nprint(jax.tree.map(jnp.shape, solution))\n</pre> tcoeffs = taylor.odejet_padded_scan(lambda *y: vf(*y, t=t0), [u0], num=2) solution = solve(tcoeffs) print(jax.tree.map(jnp.shape, solution))  <pre>IVPSolution(t=(10,), u=[(10,), (10,), (10,)], u_std=[(10,), (10,), (10,)], output_scale=(9,), marginals=Normal(mean=(10, 3), cholesky=(10, 3, 3)), posterior=MarkovSeq(init=Normal(mean=(10, 3), cholesky=(10, 3, 3)), conditional=LatentCond(A=(9, 3, 3), noise=Normal(mean=(9, 3), cholesky=(9, 3, 3)), to_latent=(9, 3), to_observed=(9, 3))), num_steps=(9,), ssm=FactImpl(name='dense', prototypes=&lt;probdiffeq.impl._prototypes.DensePrototype object at 0x7f1310fd9580&gt;, normal=&lt;probdiffeq.impl._normal.DenseNormal object at 0x7f1310ff1fa0&gt;, stats=&lt;probdiffeq.impl._stats.DenseStats object at 0x7f1315f01a30&gt;, linearise=&lt;probdiffeq.impl._linearise.DenseLinearisation object at 0x7f1310ff3aa0&gt;, conditional=&lt;probdiffeq.impl._conditional.DenseConditional object at 0x7f1310219c40&gt;, num_derivatives=2, unravel=&lt;jax._src.util.HashablePartial object at 0x7f1310f73fb0&gt;))\n</pre> <p>The type of solution.u matches that of the initial condition.</p> In\u00a0[5]: Copied! <pre>print(jax.tree.map(jnp.shape, tcoeffs))\nprint(jax.tree.map(jnp.shape, solution.u))\n</pre>  print(jax.tree.map(jnp.shape, tcoeffs)) print(jax.tree.map(jnp.shape, solution.u)) <pre>[(), (), ()]\n[(10,), (10,), (10,)]\n</pre> <p>Anything that behaves like a list work. For example, we can use lists or tuples, but also named tuples.</p> In\u00a0[6]: Copied! <pre>Taylor = collections.namedtuple(\"Taylor\", [\"state\", \"velocity\", \"acceleration\"])\ntcoeffs = Taylor(*tcoeffs)\nsolution = solve(tcoeffs)\n\nprint(jax.tree.map(jnp.shape, tcoeffs))\nprint(jax.tree.map(jnp.shape, solution))\nprint(jax.tree.map(jnp.shape, solution.u))\n</pre>  Taylor = collections.namedtuple(\"Taylor\", [\"state\", \"velocity\", \"acceleration\"]) tcoeffs = Taylor(*tcoeffs) solution = solve(tcoeffs)  print(jax.tree.map(jnp.shape, tcoeffs)) print(jax.tree.map(jnp.shape, solution)) print(jax.tree.map(jnp.shape, solution.u)) <pre>Taylor(state=(), velocity=(), acceleration=())\nIVPSolution(t=(10,), u=Taylor(state=(10,), velocity=(10,), acceleration=(10,)), u_std=Taylor(state=(10,), velocity=(10,), acceleration=(10,)), output_scale=(9,), marginals=Normal(mean=(10, 3), cholesky=(10, 3, 3)), posterior=MarkovSeq(init=Normal(mean=(10, 3), cholesky=(10, 3, 3)), conditional=LatentCond(A=(9, 3, 3), noise=Normal(mean=(9, 3), cholesky=(9, 3, 3)), to_latent=(9, 3), to_observed=(9, 3))), num_steps=(9,), ssm=FactImpl(name='dense', prototypes=&lt;probdiffeq.impl._prototypes.DensePrototype object at 0x7f13100fe960&gt;, normal=&lt;probdiffeq.impl._normal.DenseNormal object at 0x7f12f8af0c80&gt;, stats=&lt;probdiffeq.impl._stats.DenseStats object at 0x7f13101ef860&gt;, linearise=&lt;probdiffeq.impl._linearise.DenseLinearisation object at 0x7f13100fc6b0&gt;, conditional=&lt;probdiffeq.impl._conditional.DenseConditional object at 0x7f12f8b70740&gt;, num_derivatives=2, unravel=&lt;jax._src.util.HashablePartial object at 0x7f12f8bf6690&gt;))\nTaylor(state=(10,), velocity=(10,), acceleration=(10,))\n</pre> <p>The same applies to statistical quantities that we can extract from the solution. For example, the standard deviation or samples from the solution object:</p> In\u00a0[7]: Copied! <pre>key = jax.random.PRNGKey(seed=15)\nposterior = stats.markov_select_terminal(solution.posterior)\nsamples, samples_init = stats.markov_sample(\n    key, posterior, reverse=True, ssm=solution.ssm\n)\n\nprint(jax.tree.map(jnp.shape, solution.u))\nprint(jax.tree.map(jnp.shape, solution.u_std))\nprint(jax.tree.map(jnp.shape, samples))\nprint(jax.tree.map(jnp.shape, samples_init))\n</pre>  key = jax.random.PRNGKey(seed=15) posterior = stats.markov_select_terminal(solution.posterior) samples, samples_init = stats.markov_sample(     key, posterior, reverse=True, ssm=solution.ssm )  print(jax.tree.map(jnp.shape, solution.u)) print(jax.tree.map(jnp.shape, solution.u_std)) print(jax.tree.map(jnp.shape, samples)) print(jax.tree.map(jnp.shape, samples_init))  <pre>Taylor(state=(10,), velocity=(10,), acceleration=(10,))\nTaylor(state=(10,), velocity=(10,), acceleration=(10,))\nTaylor(state=(9,), velocity=(9,), acceleration=(9,))\nTaylor(state=(), velocity=(), acceleration=())\n</pre>"},{"location":"examples_basic/taylor_coefficients/#taylor-coefficients","title":"Taylor coefficients\u00b6","text":"<p>To build a probabilistic solver, we need to build a specific state-space model. To build this specific state-space model, we interact with Taylor coefficients. Here are some examples how Taylor coefficients play a role in Probdiffeq's solution routines.</p>"},{"location":"examples_quickstart/quickstart/","title":"Quickstart","text":"In\u00a0[1]: Copied! <pre>\"\"\"Solve the logistic equation.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\n\nfrom probdiffeq import ivpsolve, ivpsolvers, taylor\n\n# Define a differential equation\n\n\n@jax.jit\ndef vf(y, *, t):  # noqa: ARG001\n    \"\"\"Evaluate the dynamics of the logistic ODE.\"\"\"\n    return 2 * y * (1 - y)\n\n\nu0 = jnp.asarray([0.1])\nt0, t1 = 0.0, 5.0\n\n\n# Set up a state-space model\ntcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1)\ninit, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")\n\n\n# Build a solver\nts = ivpsolvers.correction_ts1(vf, ssm=ssm, ode_order=1)\nstrategy = ivpsolvers.strategy_filter(ssm=ssm)\nsolver = ivpsolvers.solver_mle(ssm=ssm, strategy=strategy, prior=ibm, correction=ts)\nadaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)\n\n\n# Solve the ODE\n# To all users: Try different solution routines.\nsolution = ivpsolve.solve_adaptive_save_every_step(\n    init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm\n)\n\n# Look at the solution\nprint(f\"\\ninitial = {jax.tree.map(jnp.shape, init)}\")\nprint(f\"\\nsolution = {jax.tree.map(jnp.shape, solution)}\")\n</pre> \"\"\"Solve the logistic equation.\"\"\"  import jax import jax.numpy as jnp  from probdiffeq import ivpsolve, ivpsolvers, taylor  # Define a differential equation   @jax.jit def vf(y, *, t):  # noqa: ARG001     \"\"\"Evaluate the dynamics of the logistic ODE.\"\"\"     return 2 * y * (1 - y)   u0 = jnp.asarray([0.1]) t0, t1 = 0.0, 5.0   # Set up a state-space model tcoeffs = taylor.odejet_padded_scan(lambda y: vf(y, t=t0), (u0,), num=1) init, ibm, ssm = ivpsolvers.prior_wiener_integrated(tcoeffs, ssm_fact=\"dense\")   # Build a solver ts = ivpsolvers.correction_ts1(vf, ssm=ssm, ode_order=1) strategy = ivpsolvers.strategy_filter(ssm=ssm) solver = ivpsolvers.solver_mle(ssm=ssm, strategy=strategy, prior=ibm, correction=ts) adaptive_solver = ivpsolvers.adaptive(solver, ssm=ssm)   # Solve the ODE # To all users: Try different solution routines. solution = ivpsolve.solve_adaptive_save_every_step(     init, t0=t0, t1=t1, dt0=0.1, adaptive_solver=adaptive_solver, ssm=ssm )  # Look at the solution print(f\"\\ninitial = {jax.tree.map(jnp.shape, init)}\") print(f\"\\nsolution = {jax.tree.map(jnp.shape, solution)}\") <pre>\ninitial = Normal(mean=(2,), cholesky=(2, 2))\n\nsolution = IVPSolution(t=(38,), u=[(38, 1), (38, 1)], u_std=[(38, 1), (38, 1)], output_scale=(37,), marginals=Normal(mean=(38, 2), cholesky=(38, 2, 2)), posterior=Normal(mean=(38, 2), cholesky=(38, 2, 2)), num_steps=(37,), ssm=FactImpl(name='dense', prototypes=&lt;probdiffeq.impl._prototypes.DensePrototype object at 0x7f1010ec19d0&gt;, normal=&lt;probdiffeq.impl._normal.DenseNormal object at 0x7f1010ec1a00&gt;, stats=&lt;probdiffeq.impl._stats.DenseStats object at 0x7f1010ec1880&gt;, linearise=&lt;probdiffeq.impl._linearise.DenseLinearisation object at 0x7f1010ec19a0&gt;, conditional=&lt;probdiffeq.impl._conditional.DenseConditional object at 0x7f1010ec1970&gt;, num_derivatives=1, unravel=&lt;jax._src.util.HashablePartial object at 0x7f1015bc0260&gt;))\n</pre>"},{"location":"examples_quickstart/quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>Let's have a look at an easy example.</p>"}]}