{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0664997",
   "metadata": {},
   "source": [
    "# Parameter estimation with Optax\n",
    "\n",
    "**Time-series data and optimization with ``optax``**\n",
    "\n",
    "We create some fake-observational data, compute the marginal likelihood of this fake data _under the ODE posterior_ (which is something you cannot do with non-probabilistic solvers!), and optimize the parameters with `optax`.\n",
    "\n",
    "\n",
    "Tronarp, Bosch, and Hennig call this \"physics-enhanced regression\" ([link to paper](https://arxiv.org/abs/2202.01287))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39709be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from diffeqzoo import backend, ivps\n",
    "from jax.config import config\n",
    "\n",
    "from probdiffeq import ivpsolve, ivpsolvers, solution\n",
    "from probdiffeq.doc_util import notebook\n",
    "from probdiffeq.implementations import recipes\n",
    "from probdiffeq.strategies import smoothers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(notebook.plot_config())\n",
    "\n",
    "if not backend.has_been_selected:\n",
    "    backend.select(\"jax\")  # ivp examples in jax\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8277f",
   "metadata": {},
   "source": [
    "Create a problem and some fake-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, u0, (t0, t1), f_args = ivps.lotka_volterra()\n",
    "f_args = jnp.asarray(f_args)\n",
    "\n",
    "parameter_true = f_args + 0.05\n",
    "parameter_guess = f_args\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def vf(y, t, p):\n",
    "    return f(y, *p)\n",
    "\n",
    "\n",
    "ts = jnp.linspace(t0, t1, endpoint=True, num=100)\n",
    "\n",
    "strategy = smoothers.Smoother(\n",
    "    recipes.IsoTS0.from_params(num_derivatives=1),\n",
    ")\n",
    "solver = ivpsolvers.CalibrationFreeSolver(strategy, output_scale_sqrtm=10.0)\n",
    "\n",
    "\n",
    "solution_true = ivpsolve.solve_fixed_grid(\n",
    "    vf, initial_values=(u0,), grid=ts, solver=solver, parameters=parameter_true\n",
    ")\n",
    "data = solution_true.u\n",
    "plt.plot(ts, data, \"P-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43029048",
   "metadata": {},
   "source": [
    "We make an initial guess, but it does not lead to a good data fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cabed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_guess = ivpsolve.solve_fixed_grid(\n",
    "    vf, initial_values=(u0,), grid=ts, solver=solver, parameters=parameter_guess\n",
    ")\n",
    "plt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125)\n",
    "plt.plot(ts, solution_guess.u)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470cfb67",
   "metadata": {},
   "source": [
    "Use the probdiffeq functionality to compute a parameter-to-data fit function.\n",
    "\n",
    "This incorporates the likelihood of the data under the distribution induced by the probabilistic ODE solution (which was generated with the current parameter guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_log_likelihood(parameters_, u0_, ts_, solver_, vf_, data_, obs_stdev=1e-1):\n",
    "    sol_ = ivpsolve.solve_fixed_grid(\n",
    "        vf_, initial_values=(u0_,), grid=ts_, solver=solver_, parameters=parameters_\n",
    "    )\n",
    "\n",
    "    observation_std = jnp.ones_like(ts_) * obs_stdev\n",
    "    return -1.0 * solution.log_marginal_likelihood(\n",
    "        observation_std=observation_std, u=data_, solution=sol_\n",
    "    )\n",
    "\n",
    "\n",
    "parameter_to_data_fit = jax.jit(\n",
    "    functools.partial(\n",
    "        param_to_log_likelihood, solver_=solver, ts_=ts, vf_=vf, u0_=u0, data_=data\n",
    "    )\n",
    ")\n",
    "\n",
    "sensitivities = jax.jit(jax.grad(parameter_to_data_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0131f69",
   "metadata": {},
   "source": [
    "We can differentiate the function forward- and reverse-mode (the latter is possible because we use fixed steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5281a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "parameter_to_data_fit(parameter_guess)\n",
    "sensitivities(parameter_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad0744",
   "metadata": {},
   "source": [
    "Now, enter optax: build an optimizer, and optimise the parameter-to-model-fit function. The following is more or less taken from the [optax-documentation](https://optax.readthedocs.io/en/latest/optax-101.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e567bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_update_fn(*, optimizer, loss_fn):\n",
    "    \"\"\"Build a function for executing a single step in the optimization.\"\"\"\n",
    "\n",
    "    @jax.jit\n",
    "    def update(params, opt_state):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "\n",
    "    return update\n",
    "\n",
    "\n",
    "optim = optax.adam(learning_rate=1e-2)\n",
    "update_fn = build_update_fn(optimizer=optim, loss_fn=parameter_to_data_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b66256",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "p = parameter_guess\n",
    "state = optim.init(p)\n",
    "\n",
    "chunk_size = 10\n",
    "for i in range(chunk_size):\n",
    "    for _ in range(chunk_size):\n",
    "        p, state = update_fn(p, state)\n",
    "\n",
    "    print(f\"After {(i+1)*chunk_size} iterations:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8b4a8",
   "metadata": {},
   "source": [
    "The solution looks much better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_wrong = ivpsolve.solve_fixed_grid(\n",
    "    vf, initial_values=(u0,), grid=ts, solver=solver, parameters=p\n",
    ")\n",
    "plt.plot(ts, data, color=\"k\", linestyle=\"solid\", linewidth=6, alpha=0.125)\n",
    "plt.plot(ts, solution_wrong.u)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
